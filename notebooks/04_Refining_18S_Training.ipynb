{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8477e711-f561-415a-a736-dffd202354b8",
   "metadata": {},
   "source": [
    "# 18S Model Training and Evaluation\n",
    "\n",
    "**Objective:** To build, train, and evaluate a deep learning classifier for the 18S rRNA gene (Eukaryotes) using the pre-processed data.\n",
    "\n",
    "**Methodology:**\n",
    "1. Load the 18S-specific training/testing data and encoders from disk.\n",
    "2. Define the neural network architecture.\n",
    "3. Train the model on the training data using the GPU.\n",
    "4. Save, reload, and evaluate the final model's accuracy on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbd40b8-8e82-4b8c-865d-4e96c71c939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Setup ---\n",
      "TensorFlow Version: 2.10.1\n",
      "GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set up project path\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# --- Verification Step: Check for GPU ---\n",
    "print(\"--- TensorFlow Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a70ef60-91ba-4250-b5f5-4bf149d984dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading 18S Data ---\n",
      "Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "Shape of X_train: (6427, 14058)\n",
      "Shape of y_train: (6427,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1607, 14058)\n",
      "Shape of y_test:  (1607,)\n",
      "Number of classes (genera): 616\n"
     ]
    }
   ],
   "source": [
    "# --- Define 18S-specific file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_18s.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_18s.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_18s.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_18s.npy\"\n",
    "\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"18s_genus_label_encoder.pkl\"\n",
    "\n",
    "# --- Load the data and encoders ---\n",
    "print(\"--- Loading 18S Data ---\")\n",
    "X_train = load_npz(X_TRAIN_PATH)\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_train = np.load(Y_TRAIN_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "print(\"Data loading complete.\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40cca7d-9520-4e9c-be72-3f37c1ca92da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining 18S Model Architecture ---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              28792832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 616)               631400    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,522,408\n",
      "Trainable params: 31,522,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--- Preparing Data and Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas-v3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 2048), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Loss: 2.4823 | Acc: 50.31% [██████████··········] | Val_Loss: 1.5871 | Val_Acc: 66.25% [█████████████·······]Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 19/50 | Loss: 2.4734 | Acc: 50.67% [██████████··········] | Val_Loss: 1.6282 | Val_Acc: 68.58% [█████████████·······]Epoch 19: early stopping\n",
      "\n",
      "\n",
      "--- Training complete. ---\n",
      "\n",
      "Saving trained model to: C:\\Users\\jampa\\Music\\atlas-v3\\models\\18s_genus_classifier.keras\n",
      "✅ Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- PART 1: Define and Compile Model ---\n",
    "print(\"--- Defining 18S Model Architecture ---\")\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2048, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- PART 2: Prepare Data and Train ---\n",
    "print(\"\\n--- Preparing Data and Starting Training ---\")\n",
    "\n",
    "# Pre-flight check for singletons in the training set\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "if np.min(counts) < 2:\n",
    "    print(\"WARNING: Singletons found in y_train. Cleaning...\")\n",
    "    non_singleton_indices = np.where(~np.isin(y_train, unique[counts < 2]))[0]\n",
    "    X_train = X_train[non_singleton_indices]\n",
    "    y_train = y_train[non_singleton_indices]\n",
    "\n",
    "# Create validation set\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Define custom callback\n",
    "class TrainingProgressCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = logs.get('accuracy', 0); val_acc = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0); val_loss = logs.get('val_loss', 0)\n",
    "        acc_bar = '█' * int(acc * 20) + '·' * (20 - int(acc * 20))\n",
    "        val_acc_bar = '█' * int(val_acc * 20) + '·' * (20 - int(val_acc * 20))\n",
    "        print(f\"\\rEpoch {epoch+1:02d}/50 | Loss: {loss:.4f} | Acc: {acc:.2%} [{acc_bar}] | Val_Loss: {val_loss:.4f} | Val_Acc: {val_acc:.2%} [{val_acc_bar}]\", end='')\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Start training\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping, TrainingProgressCallback()]\n",
    ")\n",
    "print(\"\\n\\n--- Training complete. ---\")\n",
    "\n",
    "\n",
    "# --- PART 3: Save the Model ---\n",
    "MODEL_PATH = MODELS_DIR / \"18s_genus_classifier.keras\"\n",
    "print(f\"\\nSaving trained model to: {MODEL_PATH}\")\n",
    "model.save(MODEL_PATH)\n",
    "print(\"✅ Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3e89ec-8ed7-4945-b0f8-bb8d31b37aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Evaluation ---\n",
      "Clearing TensorFlow session...\n",
      "Memory cleared successfully.\n",
      "\n",
      "Loading model from: C:\\Users\\jampa\\Music\\atlas-v3\\models\\18s_genus_classifier.keras\n",
      "Model loaded successfully.\n",
      "\n",
      "Loading test data...\n",
      "Test data loaded successfully.\n",
      "\n",
      "Evaluating model on the test set...\n",
      "51/51 [==============================] - 2s 19ms/step - loss: 1.9340 - accuracy: 0.6409\n",
      "\n",
      "--- Final 18S Model Evaluation ---\n",
      "Test Set Loss:     1.9340\n",
      "Test Set Accuracy: 64.09%\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: FINAL MODEL EVALUATION\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To perform a definitive, unbiased evaluation of the trained 18S model on\n",
    "#   the unseen test set. This is a memory-safe operation.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "\n",
    "# --- Define all necessary file paths ---\n",
    "project_root = Path.cwd().parent\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"18s_genus_classifier.keras\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_18s.npz\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_18s.npy\"\n",
    "\n",
    "# --- 1. Clean up memory ---\n",
    "print(\"--- Starting Final Evaluation ---\")\n",
    "print(\"Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "print(\"Memory cleared successfully.\")\n",
    "\n",
    "# --- 2. Load model and test data ---\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "loaded_model = load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "print(f\"\\nLoading test data...\")\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "print(\"Test data loaded successfully.\")\n",
    "\n",
    "# --- 3. Evaluate the model ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\n--- Final 18S Model Evaluation ---\")\n",
    "print(f\"Test Set Loss:     {loss:.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy:.2%}\")\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d720866-c67d-4663-8af1-c05ddef3596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Training History Plots ---\n",
      "\n",
      "Could not generate plots because the 'history' object was not found in memory.\n",
      "This is expected if the kernel was restarted after training.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: VISUALIZE TRAINING HISTORY (Optional)\n",
    "# =============================================================================\n",
    "#\n",
    "# NOTE: This cell will only work if the kernel has NOT been restarted since\n",
    "# the training cell was run, as it depends on the 'history' object in memory.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    print(\"--- Generating Training History Plots ---\")\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle('18S Model Training History', fontsize=18)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1.plot(history_df.index + 1, history_df['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history_df.index + 1, history_df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "    ax1.set_title('Model Accuracy Over Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--')\n",
    "\n",
    "    # Plot Loss\n",
    "    ax2.plot(history_df.index + 1, history_df['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history_df.index + 1, history_df['val_loss'], label='Validation Loss', marker='o')\n",
    "    ax2.set_title('Model Loss Over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nCould not generate plots because the 'history' object was not found in memory.\")\n",
    "    print(\"This is expected if the kernel was restarted after training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b80f0-d19a-41ab-8003-b624bf0313e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
