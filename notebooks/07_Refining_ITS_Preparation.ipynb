{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e9c723-7971-4464-a13a-595cee3f60e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n",
      "\n",
      "Source UNITE database archive found.\n",
      "  - Location: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\sh_general_release_19.02.2025.tgz\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                           ATLAS v3: ITS Data Preparation (Fungi)\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       Refine the data preparation pipeline for the ITS (Internal Transcribed\n",
    "#       Spacer) region, the primary barcode for fungi identification, using\n",
    "#       the UNITE database.\n",
    "#\n",
    "#   METHODOLOGY:\n",
    "#\n",
    "#       1.  Create a Development Sample: Extract a small, manageable sample\n",
    "#           from the full UNITE database (`.tgz` archive) to enable rapid,\n",
    "#           interactive development.\n",
    "#       2.  Develop a Custom Parser: Create a new taxonomy parser specifically\n",
    "#           designed for the UNITE database's `k__Fungi;p__Ascomycota;...`\n",
    "#           header format.\n",
    "#       3.  Clean and Process: Apply the full data cleaning and feature\n",
    "#           engineering workflow (k-mer counting, vectorizing, splitting).\n",
    "#       4.  Save Artifacts: Save the final, model-ready artifacts for the ITS\n",
    "#           pipeline.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import tarfile # Required for reading from .tgz archives\n",
    "import io      # Required for stream handling\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "# This ensures that the notebook can find the project's root directory\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    project_root = Path.cwd().parent\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "# --- Define Core Directories ---\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Define ITS Specific File Paths ---\n",
    "\n",
    "# --- NOTE: Update this filename if you download a different UNITE release ---\n",
    "FULL_UNITE_PATH = RAW_DATA_DIR / \"sh_general_release_19.02.2025.tgz\"\n",
    "\n",
    "# Path to the small sample file we will create for development\n",
    "SAMPLE_UNITE_PATH = RAW_DATA_DIR / \"UNITE_sample_10k.fasta\"\n",
    "\n",
    "# --- Verification Step ---\n",
    "if FULL_UNITE_PATH.exists():\n",
    "    print(f\"\\nSource UNITE database archive found.\")\n",
    "    print(f\"  - Location: {FULL_UNITE_PATH}\")\n",
    "else:\n",
    "    print(f\"\\n[ERROR] The source UNITE database archive was not found.\")\n",
    "    print(f\"  - Expected: {FULL_UNITE_PATH}\")\n",
    "    print(\"        Please ensure the file is downloaded and correctly named in the 'data/raw' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74bc3d6c-607b-46df-ad82-248cdadeba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sample of 10000 sequences from the UNITE archive...\n",
      "This may take a moment...\n",
      "  - Found FASTA file in archive: sh_general_release_dynamic_19.02.2025.fasta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9047316464fa49aeafa5783f3b727f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Sampling records:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Created sample file with 10000 sequences.\n",
      "  - Location: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\UNITE_sample_10k.fasta\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#       STEP 1: CREATE A DEVELOPMENT SAMPLE FROM THE UNITE ARCHIVE\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To extract the first 10,000 sequences from the main FASTA file\n",
    "#       contained within the UNITE `.tgz` archive.\n",
    "#\n",
    "#   RATIONALE:\n",
    "#\n",
    "#       Working with a smaller sample file (`UNITE_sample_10k.fasta`) allows\n",
    "#       for rapid, interactive development and debugging of the subsequent\n",
    "#       parsing and cleaning steps. This script is designed to be run only\n",
    "#       once; if the sample file is found, this step will be skipped.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Configuration ---\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "# --- Main Logic ---\n",
    "# This check prevents us from re-running this process unnecessarily.\n",
    "if not SAMPLE_UNITE_PATH.exists():\n",
    "    print(f\"Creating a sample of {SAMPLE_SIZE} sequences from the UNITE archive...\")\n",
    "    print(\"This may take a moment...\")\n",
    "\n",
    "    sample_records = []\n",
    "    try:\n",
    "        # Open the .tgz archive for reading\n",
    "        with tarfile.open(FULL_UNITE_PATH, \"r:gz\") as tar:\n",
    "            # Find the main FASTA file within the archive\n",
    "            fasta_member = None\n",
    "            for member in tar.getmembers():\n",
    "                if member.name.endswith('.fasta'):\n",
    "                    fasta_member = member\n",
    "                    break\n",
    "\n",
    "            if fasta_member:\n",
    "                print(f\"  - Found FASTA file in archive: {fasta_member.name}\")\n",
    "                # Extract the file content into an in-memory text stream\n",
    "                fasta_file = tar.extractfile(fasta_member)\n",
    "                fasta_stream = io.TextIOWrapper(fasta_file, encoding=\"utf-8\")\n",
    "\n",
    "                # Parse the stream and collect the sample records with a progress bar\n",
    "                records_iterator = SeqIO.parse(fasta_stream, \"fasta\")\n",
    "                for i, record in tqdm(enumerate(records_iterator), total=SAMPLE_SIZE, desc=\"  - Sampling records\"):\n",
    "                    if i >= SAMPLE_SIZE:\n",
    "                        break\n",
    "                    sample_records.append(record)\n",
    "\n",
    "                # Write the collected records to our new sample file\n",
    "                with open(SAMPLE_UNITE_PATH, \"w\") as handle_out:\n",
    "                    SeqIO.write(sample_records, handle_out, \"fasta\")\n",
    "\n",
    "                print(f\"\\n[SUCCESS] Created sample file with {len(sample_records)} sequences.\")\n",
    "                print(f\"  - Location: {SAMPLE_UNITE_PATH}\")\n",
    "            else:\n",
    "                print(\"\\n[ERROR] No .fasta file was found inside the .tgz archive.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] An error occurred while processing the archive: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"UNITE sample file already exists. No action needed.\")\n",
    "    print(f\"  - Location: {SAMPLE_UNITE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5191d0ef-b491-4748-b2c8-73c4772982a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying corrected UNITE taxonomy parser (v2) to sample data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deefa9ed35b40e695dfa2e91fb09864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Parsing headers:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Re-parsing complete. Created a DataFrame with 10000 rows.\n",
      "\n",
      "--- ASCII PREVIEW: First 5 Rows (Corrected) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fungi</td>\n",
       "      <td>Ascomycota</td>\n",
       "      <td>Dothideomycetes</td>\n",
       "      <td>Abrothallales</td>\n",
       "      <td>Abrothallaceae</td>\n",
       "      <td>Abrothallus</td>\n",
       "      <td>Abrothallus_subhalei</td>\n",
       "      <td>Abrothallus_subhalei|MT153946|SH1227328.10FU|r...</td>\n",
       "      <td>CAACCCTTGCTTACCTACCACGTTGCTTCGGCGGGCCCGGGGCAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fungi</td>\n",
       "      <td>Mucoromycota</td>\n",
       "      <td>Mucoromycetes</td>\n",
       "      <td>Mucorales</td>\n",
       "      <td>Mucoraceae</td>\n",
       "      <td>Mucor</td>\n",
       "      <td>Mucor_inaequisporus</td>\n",
       "      <td>Mucor_inaequisporus|JN206177|SH1227742.10FU|re...</td>\n",
       "      <td>ATCATTAAATAATTTGATAATTAYACAATTATCTAATTTACTGTGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fungi</td>\n",
       "      <td>Ascomycota</td>\n",
       "      <td>Saccharomycetes</td>\n",
       "      <td>Saccharomycetales</td>\n",
       "      <td>Saccharomycetales_fam_Incertae_sedis</td>\n",
       "      <td>Candida</td>\n",
       "      <td>Candida_vrieseae</td>\n",
       "      <td>Candida_vrieseae|KY102517|SH1232203.10FU|refs|...</td>\n",
       "      <td>CAGTTAGTTTATGTTCTCTCTGCCTGCGCTTAGTTGCGCGGCGAGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fungi</td>\n",
       "      <td>Ascomycota</td>\n",
       "      <td>Eurotiomycetes</td>\n",
       "      <td>Chaetothyriales</td>\n",
       "      <td>Herpotrichiellaceae</td>\n",
       "      <td>Exophiala</td>\n",
       "      <td>Exophiala_lecanii-corni</td>\n",
       "      <td>Exophiala_lecanii-corni|AY857528|SH1233462.10F...</td>\n",
       "      <td>ATCATTAACGAGTTAGGGTCTTTTATAGGCTCGACCTCCCAACCCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fungi</td>\n",
       "      <td>Ascomycota</td>\n",
       "      <td>Dothideomycetes</td>\n",
       "      <td>Capnodiales</td>\n",
       "      <td>Johansoniaceae</td>\n",
       "      <td>Johansonia</td>\n",
       "      <td>Johansonia_chapadensis</td>\n",
       "      <td>Johansonia_chapadensis|HQ423449|SH1236832.10FU...</td>\n",
       "      <td>CCGAGTGAGGGTCCTCGTGGCCCAACCTCCAACCCCCTGTGAGACC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kingdom        phylum            class              order  \\\n",
       "0   Fungi    Ascomycota  Dothideomycetes      Abrothallales   \n",
       "1   Fungi  Mucoromycota    Mucoromycetes          Mucorales   \n",
       "2   Fungi    Ascomycota  Saccharomycetes  Saccharomycetales   \n",
       "3   Fungi    Ascomycota   Eurotiomycetes    Chaetothyriales   \n",
       "4   Fungi    Ascomycota  Dothideomycetes        Capnodiales   \n",
       "\n",
       "                                 family        genus                  species  \\\n",
       "0                        Abrothallaceae  Abrothallus     Abrothallus_subhalei   \n",
       "1                            Mucoraceae        Mucor      Mucor_inaequisporus   \n",
       "2  Saccharomycetales_fam_Incertae_sedis      Candida         Candida_vrieseae   \n",
       "3                   Herpotrichiellaceae    Exophiala  Exophiala_lecanii-corni   \n",
       "4                        Johansoniaceae   Johansonia   Johansonia_chapadensis   \n",
       "\n",
       "                                                  id  \\\n",
       "0  Abrothallus_subhalei|MT153946|SH1227328.10FU|r...   \n",
       "1  Mucor_inaequisporus|JN206177|SH1227742.10FU|re...   \n",
       "2  Candida_vrieseae|KY102517|SH1232203.10FU|refs|...   \n",
       "3  Exophiala_lecanii-corni|AY857528|SH1233462.10F...   \n",
       "4  Johansonia_chapadensis|HQ423449|SH1236832.10FU...   \n",
       "\n",
       "                                            sequence  \n",
       "0  CAACCCTTGCTTACCTACCACGTTGCTTCGGCGGGCCCGGGGCAAG...  \n",
       "1  ATCATTAAATAATTTGATAATTAYACAATTATCTAATTTACTGTGA...  \n",
       "2  CAGTTAGTTTATGTTCTCTCTGCCTGCGCTTAGTTGCGCGGCGAGG...  \n",
       "3  ATCATTAACGAGTTAGGGTCTTTTATAGGCTCGACCTCCCAACCCT...  \n",
       "4  CCGAGTGAGGGTCCTCGTGGCCCAACCTCCAACCCCCTGTGAGACC...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning Initiated ---\n",
      "  - Step 1: Removed 0 rows with missing 'genus' labels.\n",
      "  - Step 2: Removed 1630 rows for rare genera (less than 3 members).\n",
      "-----------------------------------\n",
      "[SUCCESS] Cleaning complete.\n",
      "          Final DataFrame has 8370 sequences ready for feature engineering.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#           STEP 2 (REVISED): CORRECT AND RE-RUN THE UNITE PARSER\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   DIAGNOSIS:\n",
    "#\n",
    "#       The previous step produced a DataFrame with all 'None' values for the\n",
    "#       taxonomic ranks. This indicates the parser logic was incorrect. It was\n",
    "#       attempting to read the second element of the header, but the actual\n",
    "#       taxonomy string is the LAST element.\n",
    "#\n",
    "#   ACTION:\n",
    "#\n",
    "#       We will define a corrected `v2` of the parser that correctly targets\n",
    "#       the last element of the pipe-separated string and re-process the\n",
    "#       sample data to create a valid DataFrame.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- A fresh list to hold our structured data ---\n",
    "parsed_data = []\n",
    "\n",
    "# --- Define the CORRECTED (v2) UNITE-specific parsing function ---\n",
    "def parse_unite_taxonomy_v2(description):\n",
    "    \"\"\"\n",
    "    Parses the UNITE database header format, correctly targeting the LAST\n",
    "    pipe-separated element for the taxonomy string.\n",
    "    \"\"\"\n",
    "    parsed_ranks = {\n",
    "        'kingdom': None, 'phylum': None, 'class': None, 'order': None,\n",
    "        'family': None, 'genus': None, 'species': None\n",
    "    }\n",
    "    try:\n",
    "        # --- FIX: Target the last element, not the second ---\n",
    "        taxonomy_str = description.split('|')[-1]\n",
    "        \n",
    "        ranks = taxonomy_str.split(';')\n",
    "        for rank_str in ranks:\n",
    "            parts = rank_str.split('__')\n",
    "            if len(parts) == 2:\n",
    "                prefix, name = parts\n",
    "                if not name: continue\n",
    "                if   prefix == 'k': parsed_ranks['kingdom'] = name\n",
    "                elif prefix == 'p': parsed_ranks['phylum'] = name\n",
    "                elif prefix == 'c': parsed_ranks['class'] = name\n",
    "                elif prefix == 'o': parsed_ranks['order'] = name\n",
    "                elif prefix == 'f': parsed_ranks['family'] = name\n",
    "                elif prefix == 'g': parsed_ranks['genus'] = name\n",
    "                elif prefix == 's': parsed_ranks['species'] = name\n",
    "    except IndexError:\n",
    "        pass\n",
    "    return parsed_ranks\n",
    "\n",
    "# --- Apply the CORRECTED parser to our sample data ---\n",
    "print(\"Applying corrected UNITE taxonomy parser (v2) to sample data...\")\n",
    "with open(SAMPLE_UNITE_PATH, \"r\") as handle:\n",
    "    for record in tqdm(SeqIO.parse(handle, \"fasta\"), total=SAMPLE_SIZE, desc=\"  - Parsing headers\"):\n",
    "        taxonomy_dict = parse_unite_taxonomy_v2(record.description)\n",
    "        taxonomy_dict['id'] = record.id\n",
    "        taxonomy_dict['sequence'] = str(record.seq)\n",
    "        parsed_data.append(taxonomy_dict)\n",
    "\n",
    "# --- Create and Verify the DataFrame ---\n",
    "df = pd.DataFrame(parsed_data)\n",
    "print(f\"\\n[SUCCESS] Re-parsing complete. Created a DataFrame with {len(df)} rows.\")\n",
    "print(\"\\n--- ASCII PREVIEW: First 5 Rows (Corrected) ---\")\n",
    "display(df.head())\n",
    "\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                 STEP 3: CLEAN AND FILTER THE DATAFRAME\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To prepare the parsed data for feature engineering by removing unusable\n",
    "#       rows. We will perform our standard two-step cleaning process.\n",
    "#\n",
    "#   WORKFLOW:\n",
    "#\n",
    "#       1.  Remove Missing Targets: Drop any rows that do not have a 'genus'\n",
    "#           label, as they cannot be used for supervised training.\n",
    "#       2.  Remove Rare Classes: Filter out any genera represented by fewer\n",
    "#           than 3 sequences to ensure data quality and stable model training.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Configuration for this phase ---\n",
    "TARGET_RANK = 'genus'\n",
    "MIN_CLASS_MEMBERS = 3\n",
    "\n",
    "# --- 1. Remove Missing Targets ---\n",
    "print(f\"\\n--- Data Cleaning Initiated ---\")\n",
    "initial_rows = len(df)\n",
    "df_cleaned = df.dropna(subset=[TARGET_RANK]).copy()\n",
    "rows_after_dropna = len(df_cleaned)\n",
    "print(f\"  - Step 1: Removed {initial_rows - rows_after_dropna} rows with missing '{TARGET_RANK}' labels.\")\n",
    "\n",
    "# --- 2. Remove Rare Classes ---\n",
    "class_counts = df_cleaned[TARGET_RANK].value_counts()\n",
    "classes_to_keep = class_counts[class_counts >= MIN_CLASS_MEMBERS].index\n",
    "df_filtered = df_cleaned[df_cleaned[TARGET_RANK].isin(classes_to_keep)].copy()\n",
    "rows_after_filter = len(df_filtered)\n",
    "print(f\"  - Step 2: Removed {rows_after_dropna - rows_after_filter} rows for rare genera (less than {MIN_CLASS_MEMBERS} members).\")\n",
    "\n",
    "# --- Final Verification ---\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"[SUCCESS] Cleaning complete.\")\n",
    "print(f\"          Final DataFrame has {len(df_filtered)} sequences ready for feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0e6b72-6148-40fb-9e5f-e06a015da1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Engineering 7-mer features ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc646bfe87e14fee8d91207e449ea1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Calculating k-mers:   0%|          | 0/8370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "--- Step 5: Vectorizing data ---\n",
      "  - Feature matrix shape: (8370, 18837)\n",
      "  - Label vector shape:   (8370,)\n",
      "-----------------------------------------\n",
      "--- Step 6: Splitting data into training/testing sets ---\n",
      "  - Training set shape: (6696, 18837)\n",
      "  - Testing set shape:  (1674, 18837)\n",
      "-----------------------------------------\n",
      "--- Step 7: Saving all ITS artifacts to disk ---\n",
      "  - All artifacts saved successfully.\n",
      "-----------------------------------------\n",
      "\n",
      "[SUCCESS] ITS DATA PREPARATION COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#       STEPS 4-7: FEATURE ENGINEERING, VECTORIZING, AND SAVING\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To execute the final data transformation and persistence steps,\n",
    "#       converting our cleaned DataFrame into model-ready numerical artifacts.\n",
    "#\n",
    "#   WORKFLOW:\n",
    "#\n",
    "#       4.  Engineer Features: Calculate k-mer counts for each sequence.\n",
    "#           A k-mer size of 7 is chosen for the variable ITS region to\n",
    "#           balance specificity and generality.\n",
    "#       5.  Vectorize Data: Convert k-mer counts and text labels into\n",
    "#           numerical matrices (X and y).\n",
    "#       6.  Split Data: Partition the dataset into training (80%) and\n",
    "#           testing (20%) sets for model development and evaluation.\n",
    "#       7.  Save Artifacts: Save all processed data and encoders to disk\n",
    "#           with unique 'its' filenames.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Imports for this final phase ---\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import save_npz\n",
    "import pickle\n",
    "\n",
    "# --- Configuration for this phase ---\n",
    "KMER_SIZE = 7 # Using a k-mer size of 7 for the variable ITS region\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Step 4: Feature Engineering (K-mer Counting) ---\n",
    "print(f\"--- Step 4: Engineering {KMER_SIZE}-mer features ---\")\n",
    "def get_kmer_counts(sequence, k):\n",
    "    \"\"\"A reusable function to calculate k-mer counts for a sequence.\"\"\"\n",
    "    counts = Counter()\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        if \"N\" not in kmer.upper():\n",
    "            counts[kmer] += 1\n",
    "    return dict(counts)\n",
    "\n",
    "df_filtered['kmer_counts'] = list(tqdm((get_kmer_counts(seq, KMER_SIZE) for seq in df_filtered['sequence']), total=len(df_filtered), desc=\"  - Calculating k-mers\"))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Step 5: Vectorize Features and Labels ---\n",
    "print(\"--- Step 5: Vectorizing data ---\")\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X = vectorizer.fit_transform(df_filtered['kmer_counts'])\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_RANK])\n",
    "print(f\"  - Feature matrix shape: {X.shape}\")\n",
    "print(f\"  - Label vector shape:   {y.shape}\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Step 6: Split Data ---\n",
    "print(\"--- Step 6: Splitting data into training/testing sets ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "print(f\"  - Training set shape: {X_train.shape}\")\n",
    "print(f\"  - Testing set shape:  {X_test.shape}\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Step 7: Save All Processed Artifacts ---\n",
    "print(\"--- Step 7: Saving all ITS artifacts to disk ---\")\n",
    "# Save data matrices with unique 'its' names\n",
    "save_npz(PROCESSED_DATA_DIR / \"X_train_its.npz\", X_train)\n",
    "save_npz(PROCESSED_DATA_DIR / \"X_test_its.npz\", X_test)\n",
    "np.save(PROCESSED_DATA_DIR / \"y_train_its.npy\", y_train)\n",
    "np.save(PROCESSED_DATA_DIR / \"y_test_its.npy\", y_test)\n",
    "\n",
    "# Save encoders with unique 'its' names\n",
    "with open(MODELS_DIR / \"its_genus_vectorizer.pkl\", 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(MODELS_DIR / \"its_genus_label_encoder.pkl\", 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"  - All artifacts saved successfully.\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"\\n[SUCCESS] ITS DATA PREPARATION COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af7f31-8e5d-40cb-8357-705d68c199b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
