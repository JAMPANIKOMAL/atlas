{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7882a96e-398e-432d-94a6-e78a9864ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n",
      "\n",
      "Environment is set up. Ready to begin Explorer pipeline development.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#             ATLAS v3: \"Explorer\" Unsupervised Pipeline Development\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To develop the \"Explorer\" pipeline, the unsupervised learning component\n",
    "#       of ATLAS. This pipeline is responsible for processing sequences that\n",
    "#       were NOT classified by the \"Filter\" models, discovering novel taxonomic\n",
    "#       groups, and providing a \"best guess\" annotation for them.\n",
    "#\n",
    "#   METHODOLOGY:\n",
    "#\n",
    "#       1.  Simulate Input: Create a sample FASTA file of \"unclassified\"\n",
    "#           sequences for development purposes.\n",
    "#       2.  Sequence Vectorization: Implement the Doc2Vec algorithm to convert\n",
    "#           raw DNA sequences into meaningful numerical vectors (embeddings).\n",
    "#           This involves creating a \"corpus\" of k-mers and training a model.\n",
    "#       3.  Clustering: Apply the HDBSCAN algorithm to the sequence vectors\n",
    "#           to group them into clusters of related organisms. HDBSCAN is\n",
    "#           chosen for its ability to handle noise and find clusters of\n",
    "#           varying shapes.\n",
    "#       4.  Interpretation: For each discovered cluster, select a representative\n",
    "#           sequence and (conceptually) outline how a BLAST search would be\n",
    "#           used to provide a taxonomic hypothesis.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# Gensim for Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# HDBSCAN for clustering\n",
    "import hdbscan\n",
    "\n",
    "# Scikit-learn for helper functions\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "# --- Define Directories ---\n",
    "# We will use the existing directory structure\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nEnvironment is set up. Ready to begin Explorer pipeline development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c40883-fc4c-4cf0-97ce-d5404399e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating input data for the Explorer pipeline...\n",
      "  - Source: SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "  - Destination: unclassified_sample_for_explorer.fasta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96661ff9bb14807be5f80a0582c97cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Sampling records:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Created simulated input file with 5000 sequences.\n",
      "\n",
      "Loading simulated sequences into memory...\n",
      "  - Successfully loaded 5000 sequences.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                  STEP 1 (REVISED): SIMULATE THE INPUT DATA\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To create a sample FASTA file that represents the \"unclassified\"\n",
    "#       sequences that would be the output of the \"Filter\" pipelines.\n",
    "#\n",
    "#   RATIONALE (UPDATED):\n",
    "#\n",
    "#       Based on a more rigorous approach, we will source our \"unclassified\"\n",
    "#       sequences from a database that was NOT used to train our most recent\n",
    "#       (ITS) model. Using the full SILVA database provides a diverse set of\n",
    "#       16S and 18S sequences that are novel from the perspective of the ITS\n",
    "#       classifier. This avoids data leakage and creates a more realistic\n",
    "#       development environment for the Explorer pipeline.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Configuration ---\n",
    "SIMULATED_INPUT_PATH = RAW_DATA_DIR / \"unclassified_sample_for_explorer.fasta\"\n",
    "# --- FIX: Use the full SILVA database as the source ---\n",
    "SOURCE_FILE_PATH = RAW_DATA_DIR / \"SILVA_138.1_SSURef_NR99_tax_silva.fasta\"\n",
    "NUM_SEQUENCES_TO_SIMULATE = 5000\n",
    "\n",
    "# --- Main Logic ---\n",
    "# This check prevents us from re-creating the file on every run\n",
    "if not SIMULATED_INPUT_PATH.exists():\n",
    "    print(f\"Simulating input data for the Explorer pipeline...\")\n",
    "    print(f\"  - Source: {SOURCE_FILE_PATH.name}\")\n",
    "    print(f\"  - Destination: {SIMULATED_INPUT_PATH.name}\")\n",
    "    \n",
    "    simulated_records = []\n",
    "    try:\n",
    "        with open(SOURCE_FILE_PATH, \"r\") as handle_in:\n",
    "            # Use tqdm to show progress as reading the large file can take a moment\n",
    "            records_iterator = SeqIO.parse(handle_in, \"fasta\")\n",
    "            for i, record in tqdm(enumerate(records_iterator), total=NUM_SEQUENCES_TO_SIMULATE, desc=\"  - Sampling records\"):\n",
    "                if i >= NUM_SEQUENCES_TO_SIMULATE:\n",
    "                    break\n",
    "                simulated_records.append(record)\n",
    "        \n",
    "        # Write the collected records to the new file\n",
    "        with open(SIMULATED_INPUT_PATH, \"w\") as handle_out:\n",
    "            SeqIO.write(simulated_records, handle_out, \"fasta\")\n",
    "            \n",
    "        print(f\"\\n[SUCCESS] Created simulated input file with {len(simulated_records)} sequences.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n[ERROR] Source file not found: {SOURCE_FILE_PATH}\")\n",
    "        print(\"        Please ensure the full SILVA FASTA file exists in `data/raw`.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] An error occurred: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Simulated input file already exists. No action needed.\")\n",
    "    print(f\"  - Location: {SIMULATED_INPUT_PATH}\")\n",
    "\n",
    "# --- Load the sequences into memory for the next steps ---\n",
    "print(\"\\nLoading simulated sequences into memory...\")\n",
    "try:\n",
    "    unclassified_sequences = list(SeqIO.parse(SIMULATED_INPUT_PATH, \"fasta\"))\n",
    "    print(f\"  - Successfully loaded {len(unclassified_sequences)} sequences.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Could not load sequences. Please check for the file at {SIMULATED_INPUT_PATH}\")\n",
    "    unclassified_sequences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d717d-35cd-48d7-8425-853b8d8b0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2.1: Preparing the Doc2Vec Corpus ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b939f332b7754e67befcde6f66967d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Processing sequences:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Corpus prepared with 5000 documents.\n",
      "\n",
      "--- Step 2.2: Training the Doc2Vec Model ---\n",
      "  - Vocabulary built with 5793 unique k-mers.\n",
      "  - Starting training (this may take a minute)...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                   STEP 2: SEQUENCE VECTORIZATION (DOC2VEC)\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To convert our list of 5,000 \"unclassified\" DNA sequences into\n",
    "#       high-quality numerical vectors using the Doc2Vec algorithm.\n",
    "#\n",
    "#   RATIONALE:\n",
    "#\n",
    "#       Unlike simple k-mer counting, Doc2Vec learns the contextual\n",
    "#       relationships between k-mers within a sequence. This produces a\n",
    "#       dense vector embedding for each sequence where similar sequences are\n",
    "#       mapped to nearby points in vector space, making them ideal for\n",
    "#       clustering algorithms.\n",
    "#\n",
    "#   WORKFLOW:\n",
    "#\n",
    "#       1.  Prepare a \"corpus\" by converting each DNA sequence into a list of\n",
    "#           its constituent k-mers (our \"words\").\n",
    "#       2.  Tag each document (sequence) with its unique ID.\n",
    "#       3.  Define and train a Gensim Doc2Vec model on this corpus.\n",
    "#       4.  Save the trained model for future use.\n",
    "#       5.  Extract the final 100-dimensional vector for each sequence.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Configuration for this phase ---\n",
    "KMER_SIZE = 6          # A smaller k-mer is good for finding general patterns\n",
    "VECTOR_SIZE = 100      # The dimensionality of our final sequence vectors\n",
    "DOC2VEC_MODEL_PATH = MODELS_DIR / \"explorer_doc2vec.model\"\n",
    "\n",
    "# --- 1. Prepare the Corpus for Doc2Vec ---\n",
    "print(\"--- Step 2.1: Preparing the Doc2Vec Corpus ---\")\n",
    "\n",
    "def sequence_to_kmers(sequence_str, k):\n",
    "    \"\"\"Converts a DNA sequence string into a list of k-mers.\"\"\"\n",
    "    return [sequence_str[i:i+k] for i in range(len(sequence_str) - k + 1)]\n",
    "\n",
    "# Create a list of TaggedDocument objects, which is the required input for Gensim\n",
    "# Each document's \"words\" are its k-mers, and its \"tag\" is its sequence ID.\n",
    "corpus = [\n",
    "    TaggedDocument(\n",
    "        words=sequence_to_kmers(str(seq.seq), KMER_SIZE),\n",
    "        tags=[seq.id]\n",
    "    )\n",
    "    for seq in tqdm(unclassified_sequences, desc=\"  - Processing sequences\")\n",
    "]\n",
    "\n",
    "print(f\"  - Corpus prepared with {len(corpus)} documents.\")\n",
    "\n",
    "\n",
    "# --- 2. Define and Train the Doc2Vec Model ---\n",
    "print(\"\\n--- Step 2.2: Training the Doc2Vec Model ---\")\n",
    "# Instantiate the model with key parameters\n",
    "# `dm=1` specifies the 'distributed memory' (PV-DM) algorithm\n",
    "# `min_count=3` ignores rare k-mers to reduce noise\n",
    "# `window=8` looks at a context of 8 k-mers on either side\n",
    "# `epochs=40` is a good number of training iterations for this dataset size\n",
    "doc2vec_model = Doc2Vec(\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    dm=1,\n",
    "    min_count=3,\n",
    "    window=8,\n",
    "    epochs=40,\n",
    "    workers=4 # Use 4 CPU cores for training\n",
    ")\n",
    "\n",
    "# Build the vocabulary from our corpus\n",
    "doc2vec_model.build_vocab(corpus)\n",
    "print(f\"  - Vocabulary built with {len(doc2vec_model.wv.key_to_index)} unique k-mers.\")\n",
    "\n",
    "# Train the model\n",
    "print(\"  - Starting training (this may take a minute)...\")\n",
    "doc2vec_model.train(\n",
    "    corpus,\n",
    "    total_examples=doc2vec_model.corpus_count,\n",
    "    epochs=doc2vec_model.epochs\n",
    ")\n",
    "print(\"  - Training complete.\")\n",
    "\n",
    "\n",
    "# --- 3. Save the Trained Model ---\n",
    "print(f\"\\n--- Step 2.3: Saving the Model ---\")\n",
    "try:\n",
    "    doc2vec_model.save(str(DOC2VEC_MODEL_PATH))\n",
    "    print(f\"  - Model saved successfully to: {DOC2VEC_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not save the model: {e}\")\n",
    "\n",
    "\n",
    "# --- 4. Extract the Final Vectors ---\n",
    "print(\"\\n--- Step 2.4: Extracting Sequence Vectors ---\")\n",
    "# The `model.dv` object holds the final vector for each document tag (sequence ID)\n",
    "sequence_vectors = np.array([doc2vec_model.dv[seq.id] for seq in unclassified_sequences])\n",
    "\n",
    "# It's good practice to normalize the vectors for clustering\n",
    "sequence_vectors = normalize(sequence_vectors)\n",
    "print(\"  - Vectors extracted and normalized.\")\n",
    "\n",
    "# --- Final Verification ---\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(\"    VECTORIZATION COMPLETE\")\n",
    "print(\"=\"*45)\n",
    "print(f\"  - Final shape of our vector matrix: {sequence_vectors.shape}\")\n",
    "print(f\"  - This corresponds to {sequence_vectors.shape[0]} sequences, each with {sequence_vectors.shape[1]} features.\")\n",
    "print(\"=\"*45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a52d6f-adfc-47d6-9888-241f39f9356e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
