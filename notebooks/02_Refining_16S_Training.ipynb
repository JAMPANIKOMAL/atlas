{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d4d919-9086-4bd0-9533-87ddf6232260",
   "metadata": {},
   "source": [
    "# 16S Model Training and Evaluation\n",
    "\n",
    "**Objective:** To build, train, and evaluate a deep learning classifier for the 16S rRNA gene using the pre-processed data.\n",
    "\n",
    "**Methodology:**\n",
    "1. Load the training/testing data and encoders from disk.\n",
    "2. Define the neural network architecture using TensorFlow/Keras.\n",
    "3. Train the model on the training data, using the GPU if available.\n",
    "4. Evaluate the final model's accuracy on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c866ac0-2727-4016-bb94-936338adb111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Setup ---\n",
      "TensorFlow Version: 2.10.1\n",
      "✅ GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set up project path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# --- Verification Step: Check for GPU ---\n",
    "# This will tell us if TensorFlow can see your GPU.\n",
    "print(\"--- TensorFlow Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"✅ GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92b03-1dbf-4385-bf3b-44a02129c5bf",
   "metadata": {},
   "source": [
    "### Step 2: Load Pre-processed Data and Encoders\n",
    "\n",
    "We will now load all the artifacts that were saved by our data preparation notebook. This includes the training data, testing data, and the crucial `vectorizer` and `label_encoder` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893ba25a-a7c8-4eb5-8eb6-5a8b9546301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from disk...\n",
      "✅ Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "Shape of X_train: (4449, 12850)\n",
      "Shape of y_train: (4449,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1113, 12850)\n",
      "Shape of y_test:  (1113,)\n",
      "Number of classes (genera): 345\n"
     ]
    }
   ],
   "source": [
    "# --- Define file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_16s.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_16s.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_16s.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_16s.npy\"\n",
    "\n",
    "VECTORIZER_PATH = MODELS_DIR / \"16s_genus_vectorizer.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"16s_genus_label_encoder.pkl\"\n",
    "\n",
    "\n",
    "# --- Load the data and encoders ---\n",
    "print(\"Loading data from disk...\")\n",
    "X_train = load_npz(X_TRAIN_PATH)\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_train = np.load(Y_TRAIN_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Note: We don't need to load the vectorizer right now, but we will need it for a final script.\n",
    "# The label_encoder is important because it tells us the number of classes.\n",
    "print(\"✅ Data loading complete.\")\n",
    "\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34222ab-f697-4762-b3b3-bea2714fa00d",
   "metadata": {},
   "source": [
    "### Step 3: Define the Neural Network Architecture\n",
    "\n",
    "We will now define our deep learning model using the Keras `Sequential` API. The architecture will consist of a series of layers:\n",
    "\n",
    "-   An **Input Layer** that accepts our k-mer feature vectors.\n",
    "-   Two hidden **Dense** layers with ReLU activation, which act as the primary learning components of the network.\n",
    "-   **Dropout** layers placed after each Dense layer to prevent the model from overfitting to the training data.\n",
    "-   An **Output Layer** with a `softmax` activation function, which will output the probability for each of the possible genera.\n",
    "\n",
    "The model will then be compiled with an `adam` optimizer and a `sparse_categorical_crossentropy` loss function, which are standard and effective choices for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d13394-a57d-4129-97f9-9dfc1f07f4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Defined and Compiled.\n",
      "Here is a summary:\n",
      "\n",
      "+-----------------------------------------------------------------+\n",
      "| Input Layer:         (None, 12850)                     |\n",
      "| Dense Layer (ReLU):    (None, 2048)                               |\n",
      "| Dropout (0.5):         (None, 2048)                               |\n",
      "| Dense Layer (ReLU):    (None, 1024)                               |\n",
      "| Dropout (0.5):         (None, 1024)                               |\n",
      "| Output Layer (Softmax):(None, 345)                                |\n",
      "+-----------------------------------------------------------------+\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              26318848  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 345)               353625    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,770,649\n",
      "Trainable params: 28,770,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# --- Get model parameters from our loaded data ---\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1] # The number of unique k-mer features\n",
    "\n",
    "# --- Define the Sequential model ---\n",
    "model = Sequential([\n",
    "    # Input layer and first hidden layer\n",
    "    Dense(2048, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# --- Compile the model ---\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- Print a summary of the model's architecture ---\n",
    "print(\"Model Architecture Defined and Compiled.\")\n",
    "print(\"Here is a summary:\")\n",
    "\n",
    "# ASCII Art representation\n",
    "print(\"\\n+-----------------------------------------------------------------+\")\n",
    "print(f\"| Input Layer:         (None, {input_shape})                     |\")\n",
    "print(\"| Dense Layer (ReLU):    (None, 2048)                               |\")\n",
    "print(\"| Dropout (0.5):         (None, 2048)                               |\")\n",
    "print(\"| Dense Layer (ReLU):    (None, 1024)                               |\")\n",
    "print(\"| Dropout (0.5):         (None, 1024)                               |\")\n",
    "print(f\"| Output Layer (Softmax):(None, {num_classes})                                |\")\n",
    "print(\"+-----------------------------------------------------------------+\\n\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcc6eb-d21d-40f0-b38d-7f98861679b1",
   "metadata": {},
   "source": [
    "### Step 4: Train the Neural Network\n",
    "\n",
    "We will now begin the training process by calling the `model.fit()` method. This function will feed the training data (`X_train`, `y_train`) to the model for a specified number of cycles, or **epochs**.\n",
    "\n",
    "During training, it will:\n",
    "-   Show the progress for each epoch.\n",
    "-   Calculate the `loss` and `accuracy` on the training data.\n",
    "-   After each epoch, it will evaluate the model on the **validation data** (a small portion of the training set held aside) to monitor for overfitting.\n",
    "-   We will use an `EarlyStopping` callback, which automatically stops the training process if the validation accuracy does not improve for a set number of epochs. This saves time and prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb44afb1-13d7-40c0-91a1-0aa6408a523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pre-flight Check for Stratification ---\n",
      "Training set is clean. No singletons found.\n",
      "-------------------------------------------\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas-v3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 2048), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Loss: 3.4091 | Acc: 43.06% [████████············] | Val_Loss: 1.8653 | Val_Acc: 63.60% [████████████········]\n",
      "Epoch 02/50 | Loss: 2.1825 | Acc: 58.74% [███████████·········] | Val_Loss: 1.2338 | Val_Acc: 74.83% [██████████████······]\n",
      "Epoch 03/50 | Loss: 1.8218 | Acc: 63.89% [████████████········] | Val_Loss: 1.0215 | Val_Acc: 77.75% [███████████████·····]\n",
      "Epoch 04/50 | Loss: 1.5980 | Acc: 66.88% [█████████████·······] | Val_Loss: 0.7356 | Val_Acc: 84.27% [████████████████····]\n",
      "Epoch 05/50 | Loss: 1.4675 | Acc: 69.31% [█████████████·······] | Val_Loss: 0.6119 | Val_Acc: 86.74% [█████████████████···]\n",
      "Epoch 06/50 | Loss: 1.4046 | Acc: 70.70% [██████████████······] | Val_Loss: 0.5698 | Val_Acc: 87.19% [█████████████████···]\n",
      "Epoch 07/50 | Loss: 1.4341 | Acc: 69.48% [█████████████·······] | Val_Loss: 0.5089 | Val_Acc: 90.34% [██████████████████··]\n",
      "Epoch 08/50 | Loss: 1.3033 | Acc: 72.23% [██████████████······] | Val_Loss: 0.4667 | Val_Acc: 90.79% [██████████████████··]\n",
      "Epoch 09/50 | Loss: 1.2526 | Acc: 73.08% [██████████████······] | Val_Loss: 0.4559 | Val_Acc: 89.21% [█████████████████···]\n",
      "Epoch 10/50 | Loss: 1.2542 | Acc: 73.53% [██████████████······] | Val_Loss: 0.3261 | Val_Acc: 94.83% [██████████████████··]\n",
      "Epoch 11/50 | Loss: 1.1710 | Acc: 74.85% [██████████████······] | Val_Loss: 0.3140 | Val_Acc: 92.58% [██████████████████··]\n",
      "Epoch 12/50 | Loss: 1.1881 | Acc: 74.78% [██████████████······] | Val_Loss: 0.3869 | Val_Acc: 91.69% [██████████████████··]\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 13/50 | Loss: 1.1282 | Acc: 75.95% [███████████████·····] | Val_Loss: 0.3896 | Val_Acc: 91.01% [██████████████████··]\n",
      "Epoch 13: early stopping\n",
      "\n",
      "--- Training complete. ---\n",
      "\n",
      "Saving trained model to: C:\\Users\\jampa\\Music\\atlas-v3\\models\\16s_genus_classifier.keras\n",
      "✅ Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "# --- FIX: Add the missing import statement ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- PART 1: FIX THE STRATIFICATION VALUE-ERROR ---\n",
    "# We must ensure our y_train set doesn't have any singleton classes before splitting it.\n",
    "\n",
    "print(\"--- Pre-flight Check for Stratification ---\")\n",
    "# Count the occurrences of each class in the training labels\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Find which classes have fewer than 2 members (the new singletons)\n",
    "singletons = unique_classes[class_counts < 2]\n",
    "\n",
    "if len(singletons) > 0:\n",
    "    print(f\"Found {len(singletons)} singleton class(es) in the training set. Removing them...\")\n",
    "    \n",
    "    # Get the indices of the rows that are NOT singletons\n",
    "    non_singleton_indices = np.where(~np.isin(y_train, singletons))[0]\n",
    "    \n",
    "    # Filter both X_train and y_train to keep only the non-singletons\n",
    "    X_train = X_train[non_singleton_indices]\n",
    "    y_train = y_train[non_singleton_indices]\n",
    "    \n",
    "    print(f\"Cleaned training set shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(\"Training set is clean. No singletons found.\")\n",
    "\n",
    "print(\"-\" * 43)\n",
    "\n",
    "\n",
    "# --- PART 2: CREATE THE BEAUTIFUL OUTPUT CALLBACK ---\n",
    "\n",
    "class TrainingProgressCallback(Callback):\n",
    "    \"\"\"A custom callback to print a single, clean line of progress for each epoch.\"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Access the metrics from the logs dictionary\n",
    "        acc = logs.get('accuracy', 0)\n",
    "        val_acc = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "\n",
    "        # Create progress bars using ASCII block characters\n",
    "        acc_bar = '█' * int(acc * 20) + '·' * (20 - int(acc * 20))\n",
    "        val_acc_bar = '█' * int(val_acc * 20) + '·' * (20 - int(val_acc * 20))\n",
    "\n",
    "        # Print the formatted output string\n",
    "        print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Loss: {loss:.4f} | Acc: {acc:.2%} [{acc_bar}] | Val_Loss: {val_loss:.4f} | Val_Acc: {val_acc:.2%} [{val_acc_bar}]\")\n",
    "\n",
    "\n",
    "# --- PART 3: TRAIN THE MODEL WITH THE FIX AND THE NEW CALLBACK ---\n",
    "\n",
    "# Define training parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Manually create the validation set from our now-guaranteed-clean training data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Start training ---\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    # We remove the default verbose output and add our custom callbacks\n",
    "    verbose=0, \n",
    "    callbacks=[early_stopping, TrainingProgressCallback()]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training complete. ---\")\n",
    "\n",
    "# --- Save the trained model immediately to secure our progress ---\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODEL_PATH = MODELS_DIR / \"16s_genus_classifier.keras\"\n",
    "print(f\"\\nSaving trained model to: {MODEL_PATH}\")\n",
    "model.save(MODEL_PATH)\n",
    "print(\"✅ Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511bfe5-0cd7-4f3a-8a0d-9201fcbb57e6",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Final Model Performance\n",
    "\n",
    "The training process only gives us the validation accuracy, which is used to guide the training. The true measure of a model's performance comes from evaluating it on the completely unseen **test set**.\n",
    "\n",
    "We will now use the `model.evaluate()` method to get the final loss and accuracy scores. This is our definitive report card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9312c934-7c51-476f-9ff7-b3941cdc7e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Post-Training Workflow ---\n",
      "Clearing TensorFlow session...\n",
      "✅ Memory cleared.\n",
      "\n",
      "Loading model from: C:\\Users\\jampa\\Music\\atlas-v3\\models\\16s_genus_classifier.keras\n",
      "✅ Model loaded successfully.\n",
      "\n",
      "Loading test data...\n",
      "✅ Test data loaded successfully.\n",
      "\n",
      "Evaluating model on the test set...\n",
      "35/35 [==============================] - 2s 19ms/step - loss: 0.5575 - accuracy: 0.8895\n",
      "\n",
      "--- Final Model Evaluation ---\n",
      "Test Set Loss:     0.5575\n",
      "Test Set Accuracy: 88.95%\n",
      "----------------------------\n",
      "\n",
      "--- Generating Training History Plots ---\n",
      "\n",
      "Could not generate plots because the 'history' object was not found in memory.\n",
      "This is expected if you have restarted the kernel after training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# --- FIX: Add missing imports for loading data ---\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "\n",
    "# --- Define all file paths required for this cell ---\n",
    "project_root = Path.cwd().parent\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"16s_genus_classifier.keras\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_16s.npz\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_16s.npy\"\n",
    "\n",
    "# --- Clean up memory ---\n",
    "print(\"--- Starting Post-Training Workflow ---\")\n",
    "print(\"Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "print(\"✅ Memory cleared.\")\n",
    "\n",
    "# --- Load the model AND the test data from their files ---\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "loaded_model = load_model(MODEL_PATH)\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "print(f\"\\nLoading test data...\")\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "print(\"✅ Test data loaded successfully.\")\n",
    "\n",
    "# --- Evaluate the loaded model on the loaded test data ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "print(f\"Test Set Loss:     {loss:.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy:.2%}\")\n",
    "print(\"----------------------------\\n\")\n",
    "\n",
    "# --- Visualize the training history ---\n",
    "# NOTE: This part requires the 'history' object from the training cell.\n",
    "# If you have restarted the kernel, you will get a NameError here.\n",
    "# To see the plots again, you would need to re-run the training cell.\n",
    "try:\n",
    "    print(\"--- Generating Training History Plots ---\")\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle('Model Training History', fontsize=18, y=1.02)\n",
    "\n",
    "    # Plot 1: Accuracy\n",
    "    ax1.plot(history_df.index + 1, history_df['accuracy'], label='Training Accuracy', marker='o', color='blue')\n",
    "    ax1.plot(history_df.index + 1, history_df['val_accuracy'], label='Validation Accuracy', marker='o', color='green')\n",
    "    ax1.set_title('Model Accuracy Over Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--')\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    ax2.plot(history_df.index + 1, history_df['loss'], label='Training Loss', marker='o', color='red')\n",
    "    ax2.plot(history_df.index + 1, history_df['val_loss'], label='Validation Loss', marker='o', color='orange')\n",
    "    ax2.set_title('Model Loss Over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss (Cross-Entropy)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nCould not generate plots because the 'history' object was not found in memory.\")\n",
    "    print(\"This is expected if you have restarted the kernel after training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1610ee1-c85a-40f6-a55b-56ef3ddf3a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
