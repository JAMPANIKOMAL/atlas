{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7882a96e-398e-432d-94a6-e78a9864ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n",
      "\n",
      "Environment is set up. Ready to begin Explorer pipeline development.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#             ATLAS v3: \"Explorer\" Unsupervised Pipeline Development\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To develop the \"Explorer\" pipeline, the unsupervised learning component\n",
    "#       of ATLAS. This pipeline is responsible for processing sequences that\n",
    "#       were NOT classified by the \"Filter\" models, discovering novel taxonomic\n",
    "#       groups, and providing a \"best guess\" annotation for them.\n",
    "#\n",
    "#   METHODOLOGY:\n",
    "#\n",
    "#       1.  Simulate Input: Create a sample FASTA file of \"unclassified\"\n",
    "#           sequences for development purposes.\n",
    "#       2.  Sequence Vectorization: Implement the Doc2Vec algorithm to convert\n",
    "#           raw DNA sequences into meaningful numerical vectors (embeddings).\n",
    "#           This involves creating a \"corpus\" of k-mers and training a model.\n",
    "#       3.  Clustering: Apply the HDBSCAN algorithm to the sequence vectors\n",
    "#           to group them into clusters of related organisms. HDBSCAN is\n",
    "#           chosen for its ability to handle noise and find clusters of\n",
    "#           varying shapes.\n",
    "#       4.  Interpretation: For each discovered cluster, select a representative\n",
    "#           sequence and (conceptually) outline how a BLAST search would be\n",
    "#           used to provide a taxonomic hypothesis.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# Gensim for Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# HDBSCAN for clustering\n",
    "import hdbscan\n",
    "\n",
    "# Scikit-learn for helper functions\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "# --- Define Directories ---\n",
    "# We will use the existing directory structure\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nEnvironment is set up. Ready to begin Explorer pipeline development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c40883-fc4c-4cf0-97ce-d5404399e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating input data for the Explorer pipeline...\n",
      "  - Source: SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "  - Destination: unclassified_sample_for_explorer.fasta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96661ff9bb14807be5f80a0582c97cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Sampling records:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Created simulated input file with 5000 sequences.\n",
      "\n",
      "Loading simulated sequences into memory...\n",
      "  - Successfully loaded 5000 sequences.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                  STEP 1 (REVISED): SIMULATE THE INPUT DATA\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To create a sample FASTA file that represents the \"unclassified\"\n",
    "#       sequences that would be the output of the \"Filter\" pipelines.\n",
    "#\n",
    "#   RATIONALE (UPDATED):\n",
    "#\n",
    "#       Based on a more rigorous approach, we will source our \"unclassified\"\n",
    "#       sequences from a database that was NOT used to train our most recent\n",
    "#       (ITS) model. Using the full SILVA database provides a diverse set of\n",
    "#       16S and 18S sequences that are novel from the perspective of the ITS\n",
    "#       classifier. This avoids data leakage and creates a more realistic\n",
    "#       development environment for the Explorer pipeline.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Configuration ---\n",
    "SIMULATED_INPUT_PATH = RAW_DATA_DIR / \"unclassified_sample_for_explorer.fasta\"\n",
    "# --- FIX: Use the full SILVA database as the source ---\n",
    "SOURCE_FILE_PATH = RAW_DATA_DIR / \"SILVA_138.1_SSURef_NR99_tax_silva.fasta\"\n",
    "NUM_SEQUENCES_TO_SIMULATE = 5000\n",
    "\n",
    "# --- Main Logic ---\n",
    "# This check prevents us from re-creating the file on every run\n",
    "if not SIMULATED_INPUT_PATH.exists():\n",
    "    print(f\"Simulating input data for the Explorer pipeline...\")\n",
    "    print(f\"  - Source: {SOURCE_FILE_PATH.name}\")\n",
    "    print(f\"  - Destination: {SIMULATED_INPUT_PATH.name}\")\n",
    "    \n",
    "    simulated_records = []\n",
    "    try:\n",
    "        with open(SOURCE_FILE_PATH, \"r\") as handle_in:\n",
    "            # Use tqdm to show progress as reading the large file can take a moment\n",
    "            records_iterator = SeqIO.parse(handle_in, \"fasta\")\n",
    "            for i, record in tqdm(enumerate(records_iterator), total=NUM_SEQUENCES_TO_SIMULATE, desc=\"  - Sampling records\"):\n",
    "                if i >= NUM_SEQUENCES_TO_SIMULATE:\n",
    "                    break\n",
    "                simulated_records.append(record)\n",
    "        \n",
    "        # Write the collected records to the new file\n",
    "        with open(SIMULATED_INPUT_PATH, \"w\") as handle_out:\n",
    "            SeqIO.write(simulated_records, handle_out, \"fasta\")\n",
    "            \n",
    "        print(f\"\\n[SUCCESS] Created simulated input file with {len(simulated_records)} sequences.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n[ERROR] Source file not found: {SOURCE_FILE_PATH}\")\n",
    "        print(\"        Please ensure the full SILVA FASTA file exists in `data/raw`.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] An error occurred: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Simulated input file already exists. No action needed.\")\n",
    "    print(f\"  - Location: {SIMULATED_INPUT_PATH}\")\n",
    "\n",
    "# --- Load the sequences into memory for the next steps ---\n",
    "print(\"\\nLoading simulated sequences into memory...\")\n",
    "try:\n",
    "    unclassified_sequences = list(SeqIO.parse(SIMULATED_INPUT_PATH, \"fasta\"))\n",
    "    print(f\"  - Successfully loaded {len(unclassified_sequences)} sequences.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Could not load sequences. Please check for the file at {SIMULATED_INPUT_PATH}\")\n",
    "    unclassified_sequences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6d717d-35cd-48d7-8425-853b8d8b0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2.1: Preparing the Doc2Vec Corpus ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b939f332b7754e67befcde6f66967d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Processing sequences:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Corpus prepared with 5000 documents.\n",
      "\n",
      "--- Step 2.2: Training the Doc2Vec Model ---\n",
      "  - Vocabulary built with 5793 unique k-mers.\n",
      "  - Starting training (this may take a minute)...\n",
      "  - Training complete.\n",
      "\n",
      "--- Step 2.3: Saving the Model ---\n",
      "  - Model saved successfully to: C:\\Users\\jampa\\Music\\atlas\\models\\explorer_doc2vec.model\n",
      "\n",
      "--- Step 2.4: Extracting Sequence Vectors ---\n",
      "  - Vectors extracted and normalized.\n",
      "\n",
      "=============================================\n",
      "    VECTORIZATION COMPLETE\n",
      "=============================================\n",
      "  - Final shape of our vector matrix: (5000, 100)\n",
      "  - This corresponds to 5000 sequences, each with 100 features.\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                   STEP 2: SEQUENCE VECTORIZATION (DOC2VEC)\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To convert our list of 5,000 \"unclassified\" DNA sequences into\n",
    "#       high-quality numerical vectors using the Doc2Vec algorithm.\n",
    "#\n",
    "#   RATIONALE:\n",
    "#\n",
    "#       Unlike simple k-mer counting, Doc2Vec learns the contextual\n",
    "#       relationships between k-mers within a sequence. This produces a\n",
    "#       dense vector embedding for each sequence where similar sequences are\n",
    "#       mapped to nearby points in vector space, making them ideal for\n",
    "#       clustering algorithms.\n",
    "#\n",
    "#   WORKFLOW:\n",
    "#\n",
    "#       1.  Prepare a \"corpus\" by converting each DNA sequence into a list of\n",
    "#           its constituent k-mers (our \"words\").\n",
    "#       2.  Tag each document (sequence) with its unique ID.\n",
    "#       3.  Define and train a Gensim Doc2Vec model on this corpus.\n",
    "#       4.  Save the trained model for future use.\n",
    "#       5.  Extract the final 100-dimensional vector for each sequence.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Configuration for this phase ---\n",
    "KMER_SIZE = 6          # A smaller k-mer is good for finding general patterns\n",
    "VECTOR_SIZE = 100      # The dimensionality of our final sequence vectors\n",
    "DOC2VEC_MODEL_PATH = MODELS_DIR / \"explorer_doc2vec.model\"\n",
    "\n",
    "# --- 1. Prepare the Corpus for Doc2Vec ---\n",
    "print(\"--- Step 2.1: Preparing the Doc2Vec Corpus ---\")\n",
    "\n",
    "def sequence_to_kmers(sequence_str, k):\n",
    "    \"\"\"Converts a DNA sequence string into a list of k-mers.\"\"\"\n",
    "    return [sequence_str[i:i+k] for i in range(len(sequence_str) - k + 1)]\n",
    "\n",
    "# Create a list of TaggedDocument objects, which is the required input for Gensim\n",
    "# Each document's \"words\" are its k-mers, and its \"tag\" is its sequence ID.\n",
    "corpus = [\n",
    "    TaggedDocument(\n",
    "        words=sequence_to_kmers(str(seq.seq), KMER_SIZE),\n",
    "        tags=[seq.id]\n",
    "    )\n",
    "    for seq in tqdm(unclassified_sequences, desc=\"  - Processing sequences\")\n",
    "]\n",
    "\n",
    "print(f\"  - Corpus prepared with {len(corpus)} documents.\")\n",
    "\n",
    "\n",
    "# --- 2. Define and Train the Doc2Vec Model ---\n",
    "print(\"\\n--- Step 2.2: Training the Doc2Vec Model ---\")\n",
    "# Instantiate the model with key parameters\n",
    "# `dm=1` specifies the 'distributed memory' (PV-DM) algorithm\n",
    "# `min_count=3` ignores rare k-mers to reduce noise\n",
    "# `window=8` looks at a context of 8 k-mers on either side\n",
    "# `epochs=40` is a good number of training iterations for this dataset size\n",
    "doc2vec_model = Doc2Vec(\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    dm=1,\n",
    "    min_count=3,\n",
    "    window=8,\n",
    "    epochs=40,\n",
    "    workers=4 # Use 4 CPU cores for training\n",
    ")\n",
    "\n",
    "# Build the vocabulary from our corpus\n",
    "doc2vec_model.build_vocab(corpus)\n",
    "print(f\"  - Vocabulary built with {len(doc2vec_model.wv.key_to_index)} unique k-mers.\")\n",
    "\n",
    "# Train the model\n",
    "print(\"  - Starting training (this may take a minute)...\")\n",
    "doc2vec_model.train(\n",
    "    corpus,\n",
    "    total_examples=doc2vec_model.corpus_count,\n",
    "    epochs=doc2vec_model.epochs\n",
    ")\n",
    "print(\"  - Training complete.\")\n",
    "\n",
    "\n",
    "# --- 3. Save the Trained Model ---\n",
    "print(f\"\\n--- Step 2.3: Saving the Model ---\")\n",
    "try:\n",
    "    doc2vec_model.save(str(DOC2VEC_MODEL_PATH))\n",
    "    print(f\"  - Model saved successfully to: {DOC2VEC_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not save the model: {e}\")\n",
    "\n",
    "\n",
    "# --- 4. Extract the Final Vectors ---\n",
    "print(\"\\n--- Step 2.4: Extracting Sequence Vectors ---\")\n",
    "# The `model.dv` object holds the final vector for each document tag (sequence ID)\n",
    "sequence_vectors = np.array([doc2vec_model.dv[seq.id] for seq in unclassified_sequences])\n",
    "\n",
    "# It's good practice to normalize the vectors for clustering\n",
    "sequence_vectors = normalize(sequence_vectors)\n",
    "print(\"  - Vectors extracted and normalized.\")\n",
    "\n",
    "# --- Final Verification ---\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(\"    VECTORIZATION COMPLETE\")\n",
    "print(\"=\"*45)\n",
    "print(f\"  - Final shape of our vector matrix: {sequence_vectors.shape}\")\n",
    "print(f\"  - This corresponds to {sequence_vectors.shape[0]} sequences, each with {sequence_vectors.shape[1]} features.\")\n",
    "print(\"=\"*45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a52d6f-adfc-47d6-9888-241f39f9356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3.1: Performing HDBSCAN Clustering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas-v3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jampa\\.conda\\envs\\atlas-v3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Clustering complete.\n",
      "\n",
      "--- Step 3.2: Analyzing Clustering Results ---\n",
      "\n",
      "=============================================\n",
      "    CLUSTERING COMPLETE\n",
      "=============================================\n",
      "  - Number of new clusters discovered: 2\n",
      "  - Number of sequences labeled as noise: 4935\n",
      "  - Total sequences processed: 5000\n",
      "=============================================\n",
      "\n",
      "--- ASCII PREVIEW: Clustering Results (First 10 Rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AY846379.1.1791</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AB001445.1.1538</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AY929368.1.1768</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KM209255.204.1909</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AY955002.1.1727</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HL281554.1.1313</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AB002515.1.1332</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AB002523.1.1496</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LF644976.16.1783</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KY857824.1.1808</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sequence_id  cluster_label\n",
       "0    AY846379.1.1791             -1\n",
       "1    AB001445.1.1538             -1\n",
       "2    AY929368.1.1768             -1\n",
       "3  KM209255.204.1909             -1\n",
       "4    AY955002.1.1727             -1\n",
       "5    HL281554.1.1313             -1\n",
       "6    AB002515.1.1332             -1\n",
       "7    AB002523.1.1496             -1\n",
       "8   LF644976.16.1783             -1\n",
       "9    KY857824.1.1808             -1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                      STEP 3: CLUSTERING WITH HDBSCAN\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To apply the HDBSCAN (Hierarchical Density-Based Spatial Clustering\n",
    "#       of Applications with Noise) algorithm to our sequence vectors to\n",
    "#       identify clusters of potentially related, novel organisms.\n",
    "#\n",
    "#   RATIONALE:\n",
    "#\n",
    "#       HDBSCAN is the ideal choice for this biological discovery task. It\n",
    "#       excels at identifying clusters of varying densities and, crucially,\n",
    "#       can mark data points as 'noise' if they do not belong to any distinct\n",
    "#       group. This allows us to separate potentially novel families of\n",
    "#       organisms from truly unique, singleton sequences.\n",
    "#\n",
    "#   WORKFLOW:\n",
    "#\n",
    "#       1.  Instantiate the HDBSCAN clusterer with parameters optimized for\n",
    "#           our data (e.g., a minimum cluster size of 5).\n",
    "#       2.  Fit the clusterer to our `sequence_vectors` matrix.\n",
    "#       3.  Analyze and print a summary of the results, including the number\n",
    "#           of clusters discovered and the number of sequences classified as noise.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- 1. Instantiate and Fit the HDBSCAN Clusterer ---\n",
    "print(\"--- Step 3.1: Performing HDBSCAN Clustering ---\")\n",
    "\n",
    "# We set `min_cluster_size=5`, meaning a group needs at least 5 related\n",
    "# sequences to be considered a distinct cluster. This helps filter out\n",
    "# very small, insignificant groupings.\n",
    "# `min_samples=1` helps find clusters in less dense regions.\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=5,\n",
    "    min_samples=1,\n",
    "    metric='euclidean', # Standard distance metric for vector spaces\n",
    "    cluster_selection_method='eom' # Excess of Mass is a robust method\n",
    ")\n",
    "\n",
    "# Fit the model to our sequence vectors. This is where the clustering happens.\n",
    "cluster_labels = clusterer.fit_predict(sequence_vectors)\n",
    "print(\"  - Clustering complete.\")\n",
    "\n",
    "\n",
    "# --- 2. Analyze the Results ---\n",
    "print(\"\\n--- Step 3.2: Analyzing Clustering Results ---\")\n",
    "\n",
    "# The cluster labels are an array of integers. -1 represents noise.\n",
    "# We can find the number of unique clusters by finding the max label value.\n",
    "num_clusters = len(np.unique(cluster_labels)) - 1 # Subtract 1 for the noise label (-1)\n",
    "num_noise_points = np.sum(cluster_labels == -1)\n",
    "\n",
    "# --- Final Verification ---\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(\"    CLUSTERING COMPLETE\")\n",
    "print(\"=\"*45)\n",
    "print(f\"  - Number of new clusters discovered: {num_clusters}\")\n",
    "print(f\"  - Number of sequences labeled as noise: {num_noise_points}\")\n",
    "print(f\"  - Total sequences processed: {len(cluster_labels)}\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# --- Store the results in our main DataFrame for easy access ---\n",
    "# We'll create a new DataFrame to hold our final results\n",
    "df_results = pd.DataFrame({\n",
    "    'sequence_id': [seq.id for seq in unclassified_sequences],\n",
    "    'cluster_label': cluster_labels\n",
    "})\n",
    "\n",
    "print(\"\\n--- ASCII PREVIEW: Clustering Results (First 10 Rows) ---\")\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465c3d26-5ae9-474e-b709-1a1770b1b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4.1: Analyzing Discovered Clusters and Extracting Representatives ---\n",
      "\n",
      "######################################################################\n",
      "####### CLUSTER 0 ANALYSIS\n",
      "######################################################################\n",
      "\n",
      "  - Number of sequences in cluster: 51\n",
      "  - Representative Sequence ID: JN639430.1.1262\n",
      "\n",
      "  - Full Representative Sequence:\n",
      "    ACGGGUGAGUAACGCGUAGGUAACCUACCUCAUAGCGGGGGAUAACUAUUGGAAACGAUAGCUAAUACCGCAUAAAAGUGUUUAACCCAUGUUAAACAUUUAAAAGGUGCAACUGCAUCACUAUGAGAUGGACCUGCGUUGUAUUAGCUAGUUGGUGAGGUAACGGCUCACCAAGGCGACGAUACAUAGCCGACCUGAGAGGGUGAUCGGCCACACUGGGACUGAGACACGGCCCAGACUCCUACGGGAGGCAGCAGUAGGGAAUCUUCGGCAAUGGACGGAAGUCUGACCGAGCAACGCCGCGUGAGUGAAGAAGGUUUUCGGAUCGUAAAGCUCUGUUGUUAGAGAAGAAUGAUGGUGGGAGUGGAAAAUCCACCAUGUGACGGUAACUAACCAGAAAGGGACGGCUAACUACGUGCCAGCAGCCGCGGUAAUACGUAGGUCCCGAGCGUUGUCCGGAUUUAUUGGGCGUAAAGCGAGCGCAGGCGGUUCUUUAAGUCUGAAGUUAAAGGCAGUGGCUCAACCAUUGUACGCUUUGGAAACUGGAGAACUUGAGUGCAGAAGGGGAGAGUGGAAUUCCAUGUGUAGCGGUGAAAUGCGUAGAUAUAUGGAGGAACACCGGUGGCGAAAGCGGCUCUCUGGUCUGUAACUGACGCUGAGGCUCGAAAGCGUGGGGAGCAAACAGGAUUAGAUACCCUGGUAGUCCACGCCGUAAACGAUGAGUGCUAGGUGUUAGGCCCUUUCCGGGGCUUAGUGCCGGAGCUAACGCAUUAAGCACUCCGCCUGGGGAGUACGACCGCAAGGUUGAAACUCAAAGGAAUUGACGGGGGCCCGCACAAGCGGUGGAGCAUGUGGUUUAAUUCGAAGCAACGCGAAGAACCUUACCAGGUCUUGACAUCCUCCUGACCGGUCUAGAGAUAGGCUUUCCCUUCGGGGCAGGAGUGACAGGUGGUGCAUGGUUGUCGUCAGCUCGUGUCGUGAGAUGUUGGGUUAAGUCCCGCAACGAGCGCAACCCCUAUUGUUAGUUGCCAUCAUUAAGUUGGGCACUCUAGCGAGACUGCCGGUAAUAAACCGGAGGAAGGUGGGGAUGACGUCAAAUCAUCAUGCCCCUUAUGACCUGGGCUACACACGUGCUACAAUGGUUGGUACAACGAGUCGCAAGCCGGUGACGGCAAGCUAAUCUCUUAAAGCCAAUCUCAGUUCGGAUUGUAGGCUGCAACUCGCCUACAUGAAGUCGGAAUCGCUAGUAAUCGCGGAUCAGC\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "  ACTION: To identify this novel group, the above sequence should be\n",
      "          submitted to the NCBI BLAST web tool (blastn suite).\n",
      "          https://blast.ncbi.nlm.nih.gov/Blast.cgi\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "######################################################################\n",
      "####### CLUSTER 1 ANALYSIS\n",
      "######################################################################\n",
      "\n",
      "  - Number of sequences in cluster: 14\n",
      "  - Representative Sequence ID: JN600331.1.1531\n",
      "\n",
      "  - Full Representative Sequence:\n",
      "    AGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGCCUAACACAUGCAAGUCGAGCGGUAGCACAAGAGAGCUUGCUCUCUGGGUGACGAGCGGCGGACGGGUGAGUAAUGUCUGGGAAACUGCCUGAUGGAGGGGGAUAACUACUGGAAACGGUAGCUAAUACCGCAUAACGUCUUCGGACCAAAGAGGGGGACCUUCGGGCCUCUUGCCAUCGGAUGUGCCCAGAUGGGAUUAGCUAGUAGGUGAGGUAAUGGCUCACCUAGGCGACGAUCCCUAGCUGGUCUGAGAGGAUGACCAGCCACACUGGAACUGAGACACGGUCCAGACUCCUACGGGAGGCAGCAGUGGGGAAUAUUGCACAAUGGGCGCAAGCCUGAUGCAGCCAUGCCGCGUGUGUGAAGAAGGCCUUCGGGUUGUAAAGCACUUUCAGCGGGGAGGAAGGCAGUAAGGUUAAUAACCUUGCUGAUUGACGUUACCCGCAGAAGAAGCACCGGCUAACUCCGUGCCAGCAGCCGCGGUAAUACGGAGGGUGCAAGCGUUAAUCGGAAUGACUGGGCGUAAAGCGCACGCAGGCGGUCUGUUAAGUUGGAUGUGAAAUCCCCGGGCUUAACCUGGGAACUGCAUUCAAAACUGACAGGCUAGAGUCUUGUAGAGGGGGGUAGAAUUCCAGGUGUAGCGGUGAAAUGCGUAGAGAUCUGGAGGAAUACCGGUGGCGAAGGCGGCCCCCUGGACAAAGACUGACGCUCAGGUGCGAAAGCGUGGGGAGCAAACAGGAUUAGAUACCCUGGUAGUCCACGCUGUAAACGAUGUCGACUUGGAGGUUGUGCCCUUGAGGCGUGGCUUCCGGAGCUAACGCGUUAAGUCGACCGCCUGGGGAGUACGGCCGCAAGGUUAAAACUCAAAUGAAUUGACGGGGGCCCGCACAAGCGGUGGAGCAUGUGGUUUAAUUCGAUGCAACGCGAAGAACCUUACCUACUCUUGACAUCCACAGAAUUCGGUAGAGAUACCUUAGUGCCUUCGGGAACUGUGAGACAGGUGCUGCAUGGCUGUCGUCAGCUCGUGUUGUGAAAUGUUGGGUUAAGUCCCGCAACGAGCGCAACCCUUAUCCUUUGUUGCCAGCACGUAAUGGUGGGAACUCAAAGGAGACUGCCGGUGAUAAACCGGAGGAAGGUGGGGAUGACGUCAAGUCAUCAUGGCCCUUACGAGUAGGGCUACACACGUGCUACAAUGGCGUAUACAAAGAGAAGCGACCUCGCGAGAGCAAGCGGACCUCAUAAAGUACGUCGUAGUCCGGAUUGGAGUCUGCAACUCGACUCCAUGAAGUCGGAAUCGCUAGUAAUCGUAGAUCAGAAUGCUACGGUGAAUACGUUCCCGGGCCUUGUACACACCGCCCGUCACACCAUGGGAGUGGGUUGCAAAAGAAGUAGGUAGCUUAACCUUCGGGAGGGCGCUUACCACUUUGUGAUUCAUGACUGGGGUGAAGUCGUAACAAGGUAACCGUAGGGGAACCUGCGGUUGGAUCACCU\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "  ACTION: To identify this novel group, the above sequence should be\n",
      "          submitted to the NCBI BLAST web tool (blastn suite).\n",
      "          https://blast.ncbi.nlm.nih.gov/Blast.cgi\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[SUCCESS] Explorer pipeline development and simulation complete.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                     STEP 4: CLUSTER INTERPRETATION\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To analyze the newly discovered clusters and extract a single,\n",
    "#       representative sequence from each one. This sequence can then be used\n",
    "#       to infer the potential taxonomic identity of the entire cluster.\n",
    "#\n",
    "#   RATIONALE:\n",
    "#\n",
    "#       A cluster is just a group of numbers until we can assign a biological\n",
    "#       hypothesis to it. By finding the sequence closest to the cluster's\n",
    "#       center in vector space, we select the member that best represents the\n",
    "#       group's shared characteristics. This sequence is the ideal candidate\n",
    "#       for a BLAST search to find its nearest relatives in public databases.\n",
    "#\n",
    "#   WORKFLOW:\n",
    "#\n",
    "#       1.  Iterate through each unique cluster label found by HDBSCAN (ignoring -1).\n",
    "#       2.  For each cluster, calculate its vector centroid (the mean vector).\n",
    "#       3.  Find the sequence within the cluster whose vector is closest to the\n",
    "#           centroid using Euclidean distance.\n",
    "#       4.  Print a summary report for each cluster, including its size and the\n",
    "#           ID and full sequence of its representative member.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- 1. Find and Analyze Each Cluster ---\n",
    "print(\"--- Step 4.1: Analyzing Discovered Clusters and Extracting Representatives ---\")\n",
    "\n",
    "# Get the set of unique cluster labels, excluding the noise label (-1)\n",
    "unique_cluster_ids = sorted(np.unique(cluster_labels))\n",
    "if -1 in unique_cluster_ids:\n",
    "    unique_cluster_ids.remove(-1)\n",
    "\n",
    "# Loop through each discovered cluster ID\n",
    "for cluster_id in unique_cluster_ids:\n",
    "    # --- Find all members of the current cluster ---\n",
    "    # Get the indices of all sequences belonging to this cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "    \n",
    "    # Get the corresponding high-dimensional vectors for these sequences\n",
    "    cluster_vectors = sequence_vectors[cluster_indices]\n",
    "    \n",
    "    # --- Calculate the Centroid ---\n",
    "    # The centroid is the mean vector of all members\n",
    "    centroid = np.mean(cluster_vectors, axis=0)\n",
    "    \n",
    "    # --- Find the Most Representative Sequence ---\n",
    "    # Calculate the distance of each member vector to the centroid\n",
    "    distances = [np.linalg.norm(vec - centroid) for vec in cluster_vectors]\n",
    "    \n",
    "    # The index of the minimum distance within our `cluster_vectors` array\n",
    "    representative_index_in_cluster = np.argmin(distances)\n",
    "    \n",
    "    # The original index in the full 5,000-sequence dataset\n",
    "    original_index = cluster_indices[representative_index_in_cluster]\n",
    "    \n",
    "    # Get the Bio.SeqRecord object for our representative sequence\n",
    "    representative_sequence_record = unclassified_sequences[original_index]\n",
    "    \n",
    "    # --- Print the Report for this Cluster ---\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(f\"####### CLUSTER {cluster_id} ANALYSIS\")\n",
    "    print(\"#\"*70)\n",
    "    print(f\"\\n  - Number of sequences in cluster: {len(cluster_indices)}\")\n",
    "    print(f\"  - Representative Sequence ID: {representative_sequence_record.id}\")\n",
    "    print(\"\\n  - Full Representative Sequence:\")\n",
    "    print(\"    \" + str(representative_sequence_record.seq))\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"  ACTION: To identify this novel group, the above sequence should be\")\n",
    "    print(\"          submitted to the NCBI BLAST web tool (blastn suite).\")\n",
    "    print(\"          https://blast.ncbi.nlm.nih.gov/Blast.cgi\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\n\\n[SUCCESS] Explorer pipeline development and simulation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c9877-9758-445f-9709-40a24d0326c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
