{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d4d919-9086-4bd0-9533-87ddf6232260",
   "metadata": {},
   "source": [
    "# 16S Model Training and Evaluation\n",
    "\n",
    "**Objective:** To build, train, and evaluate a deep learning classifier for the 16S rRNA gene using the pre-processed data.\n",
    "\n",
    "**Methodology:**\n",
    "1. Load the training/testing data and encoders from disk.\n",
    "2. Define the neural network architecture using TensorFlow/Keras.\n",
    "3. Train the model on the training data, using the GPU if available.\n",
    "4. Evaluate the final model's accuracy on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c866ac0-2727-4016-bb94-936338adb111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Setup ---\n",
      "TensorFlow Version: 2.10.1\n",
      "✅ GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set up project path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# --- Verification Step: Check for GPU ---\n",
    "# This will tell us if TensorFlow can see your GPU.\n",
    "print(\"--- TensorFlow Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"✅ GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92b03-1dbf-4385-bf3b-44a02129c5bf",
   "metadata": {},
   "source": [
    "### Step 2: Load Pre-processed Data and Encoders\n",
    "\n",
    "We will now load all the artifacts that were saved by our data preparation notebook. This includes the training data, testing data, and the crucial `vectorizer` and `label_encoder` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893ba25a-a7c8-4eb5-8eb6-5a8b9546301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from disk...\n",
      "✅ Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "Shape of X_train: (4449, 12850)\n",
      "Shape of y_train: (4449,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1113, 12850)\n",
      "Shape of y_test:  (1113,)\n",
      "Number of classes (genera): 345\n"
     ]
    }
   ],
   "source": [
    "# --- Define file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_16s.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_16s.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_16s.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_16s.npy\"\n",
    "\n",
    "VECTORIZER_PATH = MODELS_DIR / \"16s_genus_vectorizer.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"16s_genus_label_encoder.pkl\"\n",
    "\n",
    "\n",
    "# --- Load the data and encoders ---\n",
    "print(\"Loading data from disk...\")\n",
    "X_train = load_npz(X_TRAIN_PATH)\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_train = np.load(Y_TRAIN_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Note: We don't need to load the vectorizer right now, but we will need it for a final script.\n",
    "# The label_encoder is important because it tells us the number of classes.\n",
    "print(\"✅ Data loading complete.\")\n",
    "\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34222ab-f697-4762-b3b3-bea2714fa00d",
   "metadata": {},
   "source": [
    "### Step 3: Define the Neural Network Architecture\n",
    "\n",
    "We will now define our deep learning model using the Keras `Sequential` API. The architecture will consist of a series of layers:\n",
    "\n",
    "-   An **Input Layer** that accepts our k-mer feature vectors.\n",
    "-   Two hidden **Dense** layers with ReLU activation, which act as the primary learning components of the network.\n",
    "-   **Dropout** layers placed after each Dense layer to prevent the model from overfitting to the training data.\n",
    "-   An **Output Layer** with a `softmax` activation function, which will output the probability for each of the possible genera.\n",
    "\n",
    "The model will then be compiled with an `adam` optimizer and a `sparse_categorical_crossentropy` loss function, which are standard and effective choices for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d13394-a57d-4129-97f9-9dfc1f07f4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Defined and Compiled.\n",
      "Here is a summary:\n",
      "\n",
      "+-----------------------------------------------------------------+\n",
      "| Input Layer:         (None, 12850)                     |\n",
      "| Dense Layer (ReLU):    (None, 2048)                               |\n",
      "| Dropout (0.5):         (None, 2048)                               |\n",
      "| Dense Layer (ReLU):    (None, 1024)                               |\n",
      "| Dropout (0.5):         (None, 1024)                               |\n",
      "| Output Layer (Softmax):(None, 345)                                |\n",
      "+-----------------------------------------------------------------+\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              26318848  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 345)               353625    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,770,649\n",
      "Trainable params: 28,770,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# --- Get model parameters from our loaded data ---\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1] # The number of unique k-mer features\n",
    "\n",
    "# --- Define the Sequential model ---\n",
    "model = Sequential([\n",
    "    # Input layer and first hidden layer\n",
    "    Dense(2048, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# --- Compile the model ---\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- Print a summary of the model's architecture ---\n",
    "print(\"Model Architecture Defined and Compiled.\")\n",
    "print(\"Here is a summary:\")\n",
    "\n",
    "# ASCII Art representation\n",
    "print(\"\\n+-----------------------------------------------------------------+\")\n",
    "print(f\"| Input Layer:         (None, {input_shape})                     |\")\n",
    "print(\"| Dense Layer (ReLU):    (None, 2048)                               |\")\n",
    "print(\"| Dropout (0.5):         (None, 2048)                               |\")\n",
    "print(\"| Dense Layer (ReLU):    (None, 1024)                               |\")\n",
    "print(\"| Dropout (0.5):         (None, 1024)                               |\")\n",
    "print(f\"| Output Layer (Softmax):(None, {num_classes})                                |\")\n",
    "print(\"+-----------------------------------------------------------------+\\n\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcc6eb-d21d-40f0-b38d-7f98861679b1",
   "metadata": {},
   "source": [
    "### Step 4: Train the Neural Network\n",
    "\n",
    "We will now begin the training process by calling the `model.fit()` method. This function will feed the training data (`X_train`, `y_train`) to the model for a specified number of cycles, or **epochs**.\n",
    "\n",
    "During training, it will:\n",
    "-   Show the progress for each epoch.\n",
    "-   Calculate the `loss` and `accuracy` on the training data.\n",
    "-   After each epoch, it will evaluate the model on the **validation data** (a small portion of the training set held aside) to monitor for overfitting.\n",
    "-   We will use an `EarlyStopping` callback, which automatically stops the training process if the validation accuracy does not improve for a set number of epochs. This saves time and prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb44afb1-13d7-40c0-91a1-0aa6408a523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pre-flight Check for Stratification ---\n",
      "Training set is clean. No singletons found.\n",
      "-------------------------------------------\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas-v3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 2048), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Loss: 3.3892 | Acc: 43.36% [████████············] | Val_Loss: 2.0605 | Val_Acc: 66.52% [█████████████·······]\n",
      "Epoch 02/50 | Loss: 2.1652 | Acc: 59.07% [███████████·········] | Val_Loss: 1.2511 | Val_Acc: 74.83% [██████████████······]\n",
      "Epoch 03/50 | Loss: 1.8083 | Acc: 63.44% [████████████········] | Val_Loss: 1.0011 | Val_Acc: 77.08% [███████████████·····]\n",
      "Epoch 04/50 | Loss: 1.5675 | Acc: 67.53% [█████████████·······] | Val_Loss: 0.7746 | Val_Acc: 83.60% [████████████████····]\n",
      "Epoch 05/50 | Loss: 1.4509 | Acc: 69.63% [█████████████·······] | Val_Loss: 0.6079 | Val_Acc: 86.74% [█████████████████···]\n",
      "Epoch 06/50 | Loss: 1.3515 | Acc: 71.00% [██████████████······] | Val_Loss: 0.5751 | Val_Acc: 89.44% [█████████████████···]\n",
      "Epoch 07/50 | Loss: 1.3018 | Acc: 72.25% [██████████████······] | Val_Loss: 0.6036 | Val_Acc: 88.09% [█████████████████···]\n",
      "Epoch 08/50 | Loss: 1.2694 | Acc: 72.60% [██████████████······] | Val_Loss: 0.4341 | Val_Acc: 90.34% [██████████████████··]\n",
      "Epoch 09/50 | Loss: 1.1509 | Acc: 75.15% [███████████████·····] | Val_Loss: 0.4605 | Val_Acc: 90.34% [██████████████████··]\n",
      "Epoch 10/50 | Loss: 1.0885 | Acc: 76.40% [███████████████·····] | Val_Loss: 0.4244 | Val_Acc: 92.81% [██████████████████··]\n",
      "Epoch 11/50 | Loss: 1.0623 | Acc: 77.37% [███████████████·····] | Val_Loss: 0.3466 | Val_Acc: 92.36% [██████████████████··]\n",
      "Epoch 12/50 | Loss: 1.0502 | Acc: 76.95% [███████████████·····] | Val_Loss: 0.3911 | Val_Acc: 91.91% [██████████████████··]\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 13/50 | Loss: 1.0165 | Acc: 77.77% [███████████████·····] | Val_Loss: 0.3784 | Val_Acc: 92.13% [██████████████████··]\n",
      "Epoch 13: early stopping\n",
      "\n",
      "--- Training complete. ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "# --- FIX: Add the missing import statement ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- PART 1: FIX THE STRATIFICATION VALUE-ERROR ---\n",
    "# We must ensure our y_train set doesn't have any singleton classes before splitting it.\n",
    "\n",
    "print(\"--- Pre-flight Check for Stratification ---\")\n",
    "# Count the occurrences of each class in the training labels\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Find which classes have fewer than 2 members (the new singletons)\n",
    "singletons = unique_classes[class_counts < 2]\n",
    "\n",
    "if len(singletons) > 0:\n",
    "    print(f\"Found {len(singletons)} singleton class(es) in the training set. Removing them...\")\n",
    "    \n",
    "    # Get the indices of the rows that are NOT singletons\n",
    "    non_singleton_indices = np.where(~np.isin(y_train, singletons))[0]\n",
    "    \n",
    "    # Filter both X_train and y_train to keep only the non-singletons\n",
    "    X_train = X_train[non_singleton_indices]\n",
    "    y_train = y_train[non_singleton_indices]\n",
    "    \n",
    "    print(f\"Cleaned training set shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(\"Training set is clean. No singletons found.\")\n",
    "\n",
    "print(\"-\" * 43)\n",
    "\n",
    "\n",
    "# --- PART 2: CREATE THE BEAUTIFUL OUTPUT CALLBACK ---\n",
    "\n",
    "class TrainingProgressCallback(Callback):\n",
    "    \"\"\"A custom callback to print a single, clean line of progress for each epoch.\"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Access the metrics from the logs dictionary\n",
    "        acc = logs.get('accuracy', 0)\n",
    "        val_acc = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "\n",
    "        # Create progress bars using ASCII block characters\n",
    "        acc_bar = '█' * int(acc * 20) + '·' * (20 - int(acc * 20))\n",
    "        val_acc_bar = '█' * int(val_acc * 20) + '·' * (20 - int(val_acc * 20))\n",
    "\n",
    "        # Print the formatted output string\n",
    "        print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Loss: {loss:.4f} | Acc: {acc:.2%} [{acc_bar}] | Val_Loss: {val_loss:.4f} | Val_Acc: {val_acc:.2%} [{val_acc_bar}]\")\n",
    "\n",
    "\n",
    "# --- PART 3: TRAIN THE MODEL WITH THE FIX AND THE NEW CALLBACK ---\n",
    "\n",
    "# Define training parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Manually create the validation set from our now-guaranteed-clean training data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Start training ---\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    # We remove the default verbose output and add our custom callbacks\n",
    "    verbose=0, \n",
    "    callbacks=[early_stopping, TrainingProgressCallback()]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23d508-4d9c-49ab-a692-64f15b7c909c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
