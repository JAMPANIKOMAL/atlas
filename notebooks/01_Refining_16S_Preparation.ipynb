{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dc9749-67fb-4b6b-b8a8-f48600f33a69",
   "metadata": {},
   "source": [
    "# 16S Data Preparation: Refinement and Development\n",
    "\n",
    "**Objective:** Refine the data preparation pipeline for the 16S rRNA gene (Bacteria & Archaea). \n",
    "\n",
    "**Methodology:**\n",
    "1. Start with a small, manageable sample of the full SILVA database.\n",
    "2. Interactively develop and test each step of the pipeline (Filtering, Parsing, K-mer Counting, Vectorizing).\n",
    "3. Ensure the logic is robust before converting it to a final `.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e806a1-9866-455a-908e-723f29c7390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas-v3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports if needed later\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e08f9e-f8f7-452f-ae27-fe9f6dbef08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "# Create the processed data directory if it doesn't exist yet\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to the full, original SILVA database file\n",
    "FULL_SILVA_PATH = RAW_DATA_DIR / \"SILVA_138.1_SSURef_NR99_tax_silva.fasta\"\n",
    "\n",
    "# Path to the small sample file we will create for development\n",
    "SAMPLE_SILVA_PATH = RAW_DATA_DIR / \"SILVA_sample_10k.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdbdd4-2e5f-4a3b-a23a-cfbe6788f413",
   "metadata": {},
   "source": [
    "### Step 1: Create a Small Sample for Development\n",
    "\n",
    "We will read the first 10,000 sequences from the full SILVA database and save them to a new file. This allows us to develop the rest of the pipeline quickly without waiting for the full dataset to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a8ecfd-ea9d-4782-aa13-d6f1d9fe2da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample file already exists. No action needed.\n",
      "   Location: C:\\Users\\jampa\\Music\\atlas-v3\\data\\raw\\SILVA_sample_10k.fasta\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "# We only create the file if it doesn't already exist.\n",
    "# This saves time if we have to re-run the notebook.\n",
    "if not SAMPLE_SILVA_PATH.exists():\n",
    "    print(f\"Full SILVA file found. Creating a sample of {SAMPLE_SIZE} sequences...\")\n",
    "    print(f\"This may take a moment...\")\n",
    "    \n",
    "    # Use a progress bar to see what's happening\n",
    "    with open(FULL_SILVA_PATH, \"r\") as handle_in:\n",
    "        # Use a generator expression for memory efficiency\n",
    "        records_iterator = (record for record in SeqIO.parse(handle_in, \"fasta\"))\n",
    "        \n",
    "        sample_records = []\n",
    "        for i, record in tqdm(enumerate(records_iterator), total=SAMPLE_SIZE):\n",
    "            if i >= SAMPLE_SIZE:\n",
    "                break\n",
    "            sample_records.append(record)\n",
    "            \n",
    "    # Write the collected sample records to the new file\n",
    "    with open(SAMPLE_SILVA_PATH, \"w\") as handle_out:\n",
    "        SeqIO.write(sample_records, handle_out, \"fasta\")\n",
    "        \n",
    "    print(f\"✅ Successfully created sample file with {len(sample_records)} sequences.\")\n",
    "    print(f\"   Location: {SAMPLE_SILVA_PATH}\")\n",
    "else:\n",
    "    print(f\"✅ Sample file already exists. No action needed.\")\n",
    "    print(f\"   Location: {SAMPLE_SILVA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2dfac-2bd0-4178-951c-b4a4a20039af",
   "metadata": {},
   "source": [
    "### Step 2: Filter the Sample for Prokaryotes (Bacteria & Archaea)\n",
    "\n",
    "Now we will read our `SILVA_sample_10k.fasta` file and create a list in memory containing only the records that are explicitly labeled as \"Bacteria\" or \"Archaea\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d85410-10a9-45ae-a646-3b9bf342e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from sample file: C:\\Users\\jampa\\Music\\atlas-v3\\data\\raw\\SILVA_sample_10k.fasta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120420ff262149b687259767ea6aee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Filtering Summary ---\n",
      "Total sequences in sample: 10000\n",
      "Found 6883 prokaryote sequences.\n",
      "✅ Filtering complete.\n"
     ]
    }
   ],
   "source": [
    "prokaryote_records = []\n",
    "\n",
    "# We use tqdm again to see the progress\n",
    "print(f\"Reading from sample file: {SAMPLE_SILVA_PATH}\")\n",
    "\n",
    "with open(SAMPLE_SILVA_PATH, \"r\") as handle:\n",
    "    for record in tqdm(SeqIO.parse(handle, \"fasta\"), total=SAMPLE_SIZE):\n",
    "        # The description line contains the full taxonomy string\n",
    "        description = record.description.lower() # Use .lower() for a case-insensitive match\n",
    "        \n",
    "        if \"bacteria\" in description or \"archaea\" in description:\n",
    "            prokaryote_records.append(record)\n",
    "\n",
    "# Print a summary to verify the result\n",
    "print(\"\\n--- Filtering Summary ---\")\n",
    "print(f\"Total sequences in sample: {SAMPLE_SIZE}\")\n",
    "print(f\"Found {len(prokaryote_records)} prokaryote sequences.\")\n",
    "print(f\"✅ Filtering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a82e75-282f-4864-80d2-122ef27e3e01",
   "metadata": {},
   "source": [
    "### Step 3: Parse Taxonomy from Filtered Records\n",
    "\n",
    "The description for each sequence in the FASTA file contains the full taxonomic lineage, separated by semicolons (e.g., `Bacteria;Proteobacteria;Gammaproteobacteria...`). \n",
    "\n",
    "We will now:\n",
    "1. Define a function to parse this string into distinct taxonomic ranks.\n",
    "2. Apply this function to our list of 6,883 prokaryote records.\n",
    "3. Store the structured data in a pandas DataFrame for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616f5ed0-60d3-4f13-b201-22cfe793782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b360c3bdebdc4373806f0c727bee7ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parsing complete. Created a DataFrame with 6883 rows.\n",
      "Here's a preview of the structured data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Pseudomonadales</td>\n",
       "      <td>Pseudomonadaceae</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>Pseudomonas amygdali pv. morsprunorum</td>\n",
       "      <td>AB001445.1.1538</td>\n",
       "      <td>AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Enterobacterales</td>\n",
       "      <td>Pectobacteriaceae</td>\n",
       "      <td>Dickeya</td>\n",
       "      <td>Dickeya phage phiDP10.3</td>\n",
       "      <td>KM209255.204.1909</td>\n",
       "      <td>AGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGCCUAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Actinobacteriota</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinomycetaceae</td>\n",
       "      <td>F0332</td>\n",
       "      <td>None</td>\n",
       "      <td>HL281554.1.1313</td>\n",
       "      <td>GACGAACGCUGGCGGCGUGCUUAACACAUGCAAGUCGAACGAGUGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Streptococcaceae</td>\n",
       "      <td>Streptococcus</td>\n",
       "      <td>Streptococcus equi</td>\n",
       "      <td>AB002515.1.1332</td>\n",
       "      <td>GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Streptococcaceae</td>\n",
       "      <td>Streptococcus</td>\n",
       "      <td>Streptococcus porcinus</td>\n",
       "      <td>AB002523.1.1496</td>\n",
       "      <td>UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kingdom            phylum                class             order  \\\n",
       "0  Bacteria    Proteobacteria  Gammaproteobacteria   Pseudomonadales   \n",
       "1  Bacteria    Proteobacteria  Gammaproteobacteria  Enterobacterales   \n",
       "2  Bacteria  Actinobacteriota       Actinobacteria   Actinomycetales   \n",
       "3  Bacteria        Firmicutes              Bacilli   Lactobacillales   \n",
       "4  Bacteria        Firmicutes              Bacilli   Lactobacillales   \n",
       "\n",
       "              family          genus                                species  \\\n",
       "0   Pseudomonadaceae    Pseudomonas  Pseudomonas amygdali pv. morsprunorum   \n",
       "1  Pectobacteriaceae        Dickeya                Dickeya phage phiDP10.3   \n",
       "2   Actinomycetaceae          F0332                                   None   \n",
       "3   Streptococcaceae  Streptococcus                     Streptococcus equi   \n",
       "4   Streptococcaceae  Streptococcus                 Streptococcus porcinus   \n",
       "\n",
       "                  id                                           sequence  \n",
       "0    AB001445.1.1538  AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...  \n",
       "1  KM209255.204.1909  AGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGCCUAACA...  \n",
       "2    HL281554.1.1313  GACGAACGCUGGCGGCGUGCUUAACACAUGCAAGUCGAACGAGUGG...  \n",
       "3    AB002515.1.1332  GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...  \n",
       "4    AB002523.1.1496  UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list to hold our structured data\n",
    "parsed_data = []\n",
    "\n",
    "# Define a set of useless terms we want to ignore in the taxonomy\n",
    "DISCARD_RANKS = {'uncultured', 'unidentified', 'metagenome'}\n",
    "\n",
    "def parse_silva_taxonomy(taxonomy_str):\n",
    "    \"\"\"\n",
    "    Parses a SILVA taxonomy string (e.g., \"Bacteria;Firmicutes;...\")\n",
    "    into a dictionary of ranks.\n",
    "    \"\"\"\n",
    "    # Start with a dictionary of empty ranks\n",
    "    parsed_ranks = {\n",
    "        'kingdom': None, 'phylum': None, 'class': None, \n",
    "        'order': None, 'family': None, 'genus': None, 'species': None\n",
    "    }\n",
    "    \n",
    "    # Split the string by ';' and remove any useless terms\n",
    "    ranks = [\n",
    "        rank.strip() for rank in taxonomy_str.split(';') \n",
    "        if rank.strip() and rank.strip().lower() not in DISCARD_RANKS\n",
    "    ]\n",
    "    \n",
    "    if not ranks:\n",
    "        return parsed_ranks # Return empty if nothing is left\n",
    "        \n",
    "    # Assign ranks based on their position\n",
    "    # This is a safe way that avoids errors if some ranks are missing\n",
    "    if len(ranks) > 0: parsed_ranks['kingdom'] = ranks[0]\n",
    "    if len(ranks) > 1: parsed_ranks['phylum'] = ranks[1]\n",
    "    if len(ranks) > 2: parsed_ranks['class'] = ranks[2]\n",
    "    if len(ranks) > 3: parsed_ranks['order'] = ranks[3]\n",
    "    if len(ranks) > 4: parsed_ranks['family'] = ranks[4]\n",
    "    if len(ranks) > 5: parsed_ranks['genus'] = ranks[5]\n",
    "    # Species often contains two words, but we'll just take the whole string for now\n",
    "    if len(ranks) > 6: parsed_ranks['species'] = ranks[6]\n",
    "        \n",
    "    return parsed_ranks\n",
    "\n",
    "# Loop through our filtered list of records\n",
    "for record in tqdm(prokaryote_records):\n",
    "    # The taxonomy string is the part of the description after the first space\n",
    "    accession, taxonomy_str = record.description.split(' ', 1)\n",
    "    \n",
    "    # Parse the taxonomy string using our function\n",
    "    taxonomy_dict = parse_silva_taxonomy(taxonomy_str)\n",
    "    \n",
    "    # Also store the sequence and its ID\n",
    "    taxonomy_dict['id'] = record.id\n",
    "    taxonomy_dict['sequence'] = str(record.seq)\n",
    "    \n",
    "    parsed_data.append(taxonomy_dict)\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(f\"✅ Parsing complete. Created a DataFrame with {len(df)} rows.\")\n",
    "print(\"Here's a preview of the structured data:\")\n",
    "df.head() # Display the first 5 rows of our new table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72274c-e354-45a5-b863-8f3dc31a8ab3",
   "metadata": {},
   "source": [
    "### Step 4: Clean and Filter the DataFrame\n",
    "\n",
    "Our goal is to train a model to predict the `genus`. Therefore, we must perform two cleaning steps:\n",
    "\n",
    "1.  **Remove Missing Targets:** Drop any rows from our DataFrame where the `genus` column is empty (`None`). A sequence without a label is useless for training a supervised model.\n",
    "2.  **Remove Singletons:** Remove any genera that only have one single sequence representing them. The `train_test_split` function requires at least two members of each class for proper stratified splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f4d3d2-0208-48d2-b363-16a62176fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Removed rows with missing 'genus' labels.\n",
      "  - Started with: 6883 rows\n",
      "  - Remaining:    6563 rows\n",
      "  - Removed:      320 rows\n",
      "\n",
      "Step 2: Removed 'singleton' genera (genera with only 1 example).\n",
      "  - Started with: 6563 rows\n",
      "  - Remaining:    5562 rows\n",
      "  - Removed:      1001 rows\n",
      "\n",
      "✅ Cleaning complete. Our final DataFrame for feature engineering has 5562 sequences.\n"
     ]
    }
   ],
   "source": [
    "# The taxonomic rank we want our model to predict\n",
    "TARGET_RANK = 'genus'\n",
    "\n",
    "# --- 1. Remove Missing Targets ---\n",
    "initial_rows = len(df)\n",
    "df_cleaned = df.dropna(subset=[TARGET_RANK]).copy() # Use .copy() to avoid warnings\n",
    "rows_after_dropna = len(df_cleaned)\n",
    "\n",
    "print(f\"Step 1: Removed rows with missing '{TARGET_RANK}' labels.\")\n",
    "print(f\"  - Started with: {initial_rows} rows\")\n",
    "print(f\"  - Remaining:    {rows_after_dropna} rows\")\n",
    "print(f\"  - Removed:      {initial_rows - rows_after_dropna} rows\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Remove Singletons ---\n",
    "# First, count how many times each genus appears\n",
    "class_counts = df_cleaned[TARGET_RANK].value_counts()\n",
    "\n",
    "# We will now require a class to have at least 3 members to be included.\n",
    "genera_to_keep = class_counts[class_counts >= 3].index\n",
    "\n",
    "# Filter the DataFrame to keep only those genera\n",
    "df_filtered = df_cleaned[df_cleaned[TARGET_RANK].isin(genera_to_keep)].copy()\n",
    "rows_after_filter = len(df_filtered)\n",
    "\n",
    "print(f\"Step 2: Removed 'singleton' genera (genera with only 1 example).\")\n",
    "print(f\"  - Started with: {rows_after_dropna} rows\")\n",
    "print(f\"  - Remaining:    {rows_after_filter} rows\")\n",
    "print(f\"  - Removed:      {rows_after_dropna - rows_after_filter} rows\\n\")\n",
    "\n",
    "\n",
    "print(f\"✅ Cleaning complete. Our final DataFrame for feature engineering has {len(df_filtered)} sequences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45928f4e-c637-43bb-a1ca-ccd7d3b7412c",
   "metadata": {},
   "source": [
    "### Step 5: Feature Engineering (K-mer Counting)\n",
    "\n",
    "A machine learning model cannot understand a DNA sequence like \"AGTC...\". We must convert it into numbers. Our strategy is to count the occurrences of small DNA sub-sequences of a fixed length, called **k-mers**.\n",
    "\n",
    "For example, if k=3, the sequence \"AGTAG\" contains the k-mers: \"AGT\", \"GTA\", \"TAG\".\n",
    "\n",
    "We will now:\n",
    "1. Define a function to calculate these k-mer counts for a single sequence.\n",
    "2. Apply this function to every sequence in our cleaned DataFrame.\n",
    "3. Store the results in a new `kmer_counts` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0beb4e8e-baab-49ae-8ae5-57717d63bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating 6-mer counts for 5562 sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327e7dbc6a45407f97ea687b1c356348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Feature engineering complete.\n",
      "Here's a preview of the new 'kmer_counts' column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>kmer_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...</td>\n",
       "      <td>{'AACUGA': 2, 'ACUGAA': 1, 'CUGAAG': 1, 'UGAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...</td>\n",
       "      <td>{'GCCUAA': 1, 'CCUAAU': 1, 'CUAAUA': 2, 'UAAUA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...</td>\n",
       "      <td>{'UCCUGG': 1, 'CCUGGC': 1, 'CUGGCU': 1, 'UGGCU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCGUUGUUUCCAUCGCUCUACCAUGCAGUCGACGCUGAGCUCAGCU...</td>\n",
       "      <td>{'GCGUUG': 2, 'CGUUGU': 2, 'GUUGUU': 1, 'UUGUU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UUGCGGCCACCUACACAUGCAGUCGAGCGGAUGAAGGGAGCUUGCU...</td>\n",
       "      <td>{'UUGCGG': 1, 'UGCGGC': 1, 'GCGGCC': 1, 'CGGCC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...   \n",
       "3  GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...   \n",
       "4  UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...   \n",
       "5  GCGUUGUUUCCAUCGCUCUACCAUGCAGUCGACGCUGAGCUCAGCU...   \n",
       "6  UUGCGGCCACCUACACAUGCAGUCGAGCGGAUGAAGGGAGCUUGCU...   \n",
       "\n",
       "                                         kmer_counts  \n",
       "0  {'AACUGA': 2, 'ACUGAA': 1, 'CUGAAG': 1, 'UGAAG...  \n",
       "3  {'GCCUAA': 1, 'CCUAAU': 1, 'CUAAUA': 2, 'UAAUA...  \n",
       "4  {'UCCUGG': 1, 'CCUGGC': 1, 'CUGGCU': 1, 'UGGCU...  \n",
       "5  {'GCGUUG': 2, 'CGUUGU': 2, 'GUUGUU': 1, 'UUGUU...  \n",
       "6  {'UUGCGG': 1, 'UGCGGC': 1, 'GCGGCC': 1, 'CGGCC...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the length of our k-mers. From your original config, 6 is a good choice for 16S.\n",
    "KMER_SIZE = 6\n",
    "\n",
    "def get_kmer_counts(sequence, k):\n",
    "    \"\"\"Calculates the k-mer counts for a given DNA sequence.\"\"\"\n",
    "    # Use a Counter for efficiency\n",
    "    counts = Counter()\n",
    "    num_kmers = len(sequence) - k + 1\n",
    "    \n",
    "    for i in range(num_kmers):\n",
    "        kmer = sequence[i:i+k]\n",
    "        # Important: Ignore k-mers with 'N' (unknown bases)\n",
    "        if \"N\" not in kmer.upper():\n",
    "            counts[kmer] += 1\n",
    "            \n",
    "    return dict(counts)\n",
    "\n",
    "# Apply our function to the 'sequence' column.\n",
    "# The tqdm wrapper will show a progress bar.\n",
    "print(f\"Calculating {KMER_SIZE}-mer counts for {len(df_filtered)} sequences...\")\n",
    "\n",
    "df_filtered['kmer_counts'] = list(tqdm(\n",
    "    (get_kmer_counts(seq, KMER_SIZE) for seq in df_filtered['sequence']), \n",
    "    total=len(df_filtered)\n",
    "))\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n✅ Feature engineering complete.\")\n",
    "print(\"Here's a preview of the new 'kmer_counts' column:\")\n",
    "\n",
    "# Show the original sequence and its corresponding k-mer counts\n",
    "df_filtered[['sequence', 'kmer_counts']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d4e0c-f499-437c-9a09-ba0b9fa06be9",
   "metadata": {},
   "source": [
    "### Step 6: Vectorize Features and Labels\n",
    "\n",
    "This is the final transformation step. We will use tools from `scikit-learn` to create our final `X` (features) and `y` (labels) matrices.\n",
    "\n",
    "1.  **`DictVectorizer`:** This will take our `kmer_counts` column and create the massive feature matrix (`X`), where each column represents one unique k-mer.\n",
    "2.  **`LabelEncoder`:** This will take our `genus` column (which contains text) and convert it into integer labels (`y`) that the model can understand (e.g., *Pseudomonas* -> 0, *Dickeya* -> 1, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b2115f-a011-4078-8310-35150b937631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing k-mer counts...\n",
      "Encoding target labels...\n",
      "\n",
      "✅ Vectorization and encoding complete.\n",
      "Shape of our feature matrix (X): (5562, 12850)\n",
      "Shape of our label vector (y): (5562,)\n",
      "\n",
      "We now have 5562 sequences ready for the model.\n",
      "Each sequence is described by 12850 unique k-mer features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. Vectorize the features (X) ---\n",
    "print(\"Vectorizing k-mer counts...\")\n",
    "vectorizer = DictVectorizer(sparse=True) # Use a sparse matrix for memory efficiency\n",
    "X = vectorizer.fit_transform(df_filtered['kmer_counts'])\n",
    "\n",
    "\n",
    "# --- 2. Encode the labels (y) ---\n",
    "print(\"Encoding target labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_RANK])\n",
    "\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n✅ Vectorization and encoding complete.\")\n",
    "print(f\"Shape of our feature matrix (X): {X.shape}\")\n",
    "print(f\"Shape of our label vector (y): {y.shape}\")\n",
    "print(f\"\\nWe now have {X.shape[0]} sequences ready for the model.\")\n",
    "print(f\"Each sequence is described by {X.shape[1]} unique k-mer features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686d986-90db-4c3b-abc3-d98ebc864aa5",
   "metadata": {},
   "source": [
    "### Step 7: Split Data into Training and Testing Sets\n",
    "\n",
    "This is the final step before model training. We will divide our dataset into two parts:\n",
    "\n",
    "-   **Training Set (80%):** The data the model will learn from.\n",
    "-   **Testing Set (20%):** The data the model will be evaluated on to see how well it performs on unseen sequences.\n",
    "\n",
    "We will use `train_test_split` from `scikit-learn`, ensuring we use `stratify=y`. This is crucial as it guarantees that the proportion of each genus is the same in both the training and testing sets, which prevents bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89608883-6664-4277-a379-9e55261572ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data splitting complete.\n",
      "Shape of X_train: (4449, 12850)\n",
      "Shape of y_train: (4449,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1113, 12850)\n",
      "Shape of y_test:  (1113,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define our settings\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42 # Ensures the split is the same every time we run the code\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SPLIT_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Very important for imbalanced datasets!\n",
    ")\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"✅ Data splitting complete.\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d430718-17cf-43ad-80fe-74866aa52d8f",
   "metadata": {},
   "source": [
    "### Step 8: Save All Processed Artifacts\n",
    "\n",
    "Our data is now fully prepared. The final step is to save all our objects to files in the `data/processed/` directory. This will allow our future model training script to load them directly.\n",
    "\n",
    "We will save:\n",
    "- The training and testing data (`X_train`, `X_test`, `y_train`, `y_test`).\n",
    "- The fitted `DictVectorizer` and `LabelEncoder`, which are essential for processing new, unseen data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa3d3ee-43cf-4d3f-b5a8-bb096e1a997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to C:\\Users\\jampa\\Music\\atlas-v3\\data\\processed...\n",
      "  - Data saved successfully.\n",
      "Saving encoders to C:\\Users\\jampa\\Music\\atlas-v3\\models...\n",
      "  - Encoders saved successfully.\n",
      "\n",
      "✅ All artifacts have been saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "# --- Define unique filenames for this 16S pipeline ---\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_16s.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_16s.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_16s.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_16s.npy\"\n",
    "\n",
    "VECTORIZER_PATH = project_root / \"models\" / \"16s_genus_vectorizer.pkl\"\n",
    "LABEL_ENCODER_PATH = project_root / \"models\" / \"16s_genus_label_encoder.pkl\"\n",
    "\n",
    "# Create the models directory if it doesn't exist yet\n",
    "(project_root / \"models\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Save the data matrices ---\n",
    "print(f\"Saving data to {PROCESSED_DATA_DIR}...\")\n",
    "save_npz(X_TRAIN_PATH, X_train)\n",
    "save_npz(X_TEST_PATH, X_test)\n",
    "np.save(Y_TRAIN_PATH, y_train)\n",
    "np.save(Y_TEST_PATH, y_test)\n",
    "print(\"  - Data saved successfully.\")\n",
    "\n",
    "# --- Save the encoders using pickle ---\n",
    "print(f\"Saving encoders to {project_root / 'models'}...\")\n",
    "with open(VECTORIZER_PATH, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    \n",
    "with open(LABEL_ENCODER_PATH, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"  - Encoders saved successfully.\")\n",
    "    \n",
    "print(\"\\n✅ All artifacts have been saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdfc1f2-70d8-42c3-8e6c-20a342e2a013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
