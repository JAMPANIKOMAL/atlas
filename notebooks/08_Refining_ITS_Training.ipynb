{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec56556-4692-41db-aaba-ff9ef105b73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n",
      "\n",
      "--- TensorFlow Setup ---\n",
      "  - TensorFlow Version: 2.10.1\n",
      "  - GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n",
      "\n",
      "--- Loading ITS Data ---\n",
      "  - Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "  - Shape of X_train: (6696, 18837)\n",
      "  - Shape of y_train: (6696,)\n",
      "  ------------------------------\n",
      "  - Shape of X_test:  (1674, 18837)\n",
      "  - Shape of y_test:  (1674,)\n",
      "  - Number of classes (genera): 634\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                    ATLAS v3: ITS Model Training (Fungi)\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To build, train, and evaluate a deep learning classifier for the ITS\n",
    "#       region using the pre-processed data from the UNITE database.\n",
    "#\n",
    "#   METHODOLOGY:\n",
    "#\n",
    "#       1.  Load the ITS-specific training/testing data and encoders from disk.\n",
    "#       2.  Define the standard neural network architecture.\n",
    "#       3.  Train the model on the training data using the GPU.\n",
    "#       4.  Save, reload, and evaluate the final model's accuracy on the\n",
    "#           unseen test set, following our memory-safe workflow.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    project_root = Path.cwd().parent\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "\n",
    "# --- 1. Verification Step: Check for GPU ---\n",
    "print(\"\\n--- TensorFlow Setup ---\")\n",
    "print(f\"  - TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"  - GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"  - WARNING: No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "\n",
    "# --- 2. Define ITS-specific file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_its.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_its.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_its.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_its.npy\"\n",
    "\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"its_genus_label_encoder.pkl\"\n",
    "\n",
    "\n",
    "# --- 3. Load the data and encoders ---\n",
    "print(\"\\n--- Loading ITS Data ---\")\n",
    "try:\n",
    "    X_train = load_npz(X_TRAIN_PATH)\n",
    "    X_test = load_npz(X_TEST_PATH)\n",
    "    y_train = np.load(Y_TRAIN_PATH)\n",
    "    y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "    with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    print(\"  - Data loading complete.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n[ERROR] Processed data not found.\")\n",
    "    print(\"        Please run `python src/pipeline_its/01_prepare_data.py` before starting this notebook.\")\n",
    "    # Stop execution if files are missing\n",
    "    raise\n",
    "\n",
    "# --- 4. Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"  - Shape of X_train: {X_train.shape}\")\n",
    "print(f\"  - Shape of y_train: {y_train.shape}\")\n",
    "print(\"  ------------------------------\")\n",
    "print(f\"  - Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"  - Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"  - Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f935e-92a6-4a74-a0e3-3e01dca98e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
