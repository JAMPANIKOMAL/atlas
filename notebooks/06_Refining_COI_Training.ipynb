{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f280c0-56a6-41da-842a-c686152dae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Setup ---\n",
      "TensorFlow Version: 2.10.1\n",
      "GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n",
      "\n",
      "--- Loading COI Data ---\n",
      "Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "Shape of X_train: (7916, 41040)\n",
      "Shape of y_train: (7916,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1980, 41040)\n",
      "Shape of y_test:  (1980,)\n",
      "Number of classes (genera): 111\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ATLAS v3 - COI PIPELINE - SETUP AND DATA LOADING\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To set up the environment for COI model training and load all\n",
    "#   pre-processed data artifacts from disk.\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   1.  Import all necessary libraries.\n",
    "#   2.  Set up the project's root path.\n",
    "#   3.  Verify that TensorFlow can detect and utilize the GPU.\n",
    "#   4.  Define the file paths for the COI-specific artifacts.\n",
    "#   5.  Load the training data, testing data, and the label encoder.\n",
    "#   6.  Print a summary of the loaded data shapes for verification.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# --- 1. Verification Step: Check for GPU ---\n",
    "print(\"--- TensorFlow Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "# --- 2. Define COI-specific file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_coi.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_coi.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_coi.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_coi.npy\"\n",
    "\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"coi_genus_label_encoder.pkl\"\n",
    "\n",
    "# --- 3. Load the data and encoders ---\n",
    "print(\"\\n--- Loading COI Data ---\")\n",
    "X_train = load_npz(X_TRAIN_PATH)\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_train = np.load(Y_TRAIN_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "print(\"Data loading complete.\")\n",
    "\n",
    "# --- 4. Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20e78e6-7ec6-4c58-bda2-8a46d25826e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining COI Model Architecture ---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              84051968  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 111)               113775    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,263,919\n",
      "Trainable params: 86,263,919\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--- Preparing Data and Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas-v3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 2048), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08/50 | Loss: 0.3441 | Acc: 95.79% [███████████████████·] | Val_Loss: 0.3090 | Val_Acc: 95.45% [███████████████████·]Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 09/50 | Loss: 0.4052 | Acc: 95.70% [███████████████████·] | Val_Loss: 0.3046 | Val_Acc: 96.59% [███████████████████·]Epoch 9: early stopping\n",
      "\n",
      "\n",
      "--- Training complete. ---\n",
      "\n",
      "Saving trained model to: C:\\Users\\jampa\\Music\\atlas\\models\\coi_genus_classifier.keras\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: DEFINE AND TRAIN THE COI MODEL\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To define the neural network architecture, train it on the prepared COI\n",
    "#   data, and save the resulting model artifact.\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   1.  Define the Keras Sequential model architecture, dynamically sized to\n",
    "#       the input features and output classes of the COI dataset.\n",
    "#   2.  Perform a pre-flight check on the training data to ensure there are no\n",
    "#       \"singleton\" classes, which would cause errors during validation splitting.\n",
    "#   3.  Instantiate the custom callback for clean, single-line training progress.\n",
    "#   4.  Execute the training using `model.fit()`, with an EarlyStopping\n",
    "#       callback to prevent overfitting and save time.\n",
    "#   5.  Immediately save the best version of the trained model to disk.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports for this cell ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- PART 1: Define and Compile Model ---\n",
    "print(\"--- Defining COI Model Architecture ---\")\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2048, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- PART 2: Prepare Data and Train ---\n",
    "print(\"\\n--- Preparing Data and Starting Training ---\")\n",
    "\n",
    "# Pre-flight check for singletons in the training set\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "if np.min(counts) < 2:\n",
    "    print(\"WARNING: Singletons found in y_train. Cleaning...\")\n",
    "    non_singleton_indices = np.where(~np.isin(y_train, unique[counts < 2]))[0]\n",
    "    X_train = X_train[non_singleton_indices]\n",
    "    y_train = y_train[non_singleton_indices]\n",
    "\n",
    "# Create validation set\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Define custom callback\n",
    "class TrainingProgressCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = logs.get('accuracy', 0); val_acc = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0); val_loss = logs.get('val_loss', 0)\n",
    "        acc_bar = '█' * int(acc * 20) + '·' * (20 - int(acc * 20))\n",
    "        val_acc_bar = '█' * int(val_acc * 20) + '·' * (20 - int(val_acc * 20))\n",
    "        print(f\"\\rEpoch {epoch+1:02d}/50 | Loss: {loss:.4f} | Acc: {acc:.2%} [{acc_bar}] | Val_Loss: {val_loss:.4f} | Val_Acc: {val_acc:.2%} [{val_acc_bar}]\", end='')\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Start training\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping, TrainingProgressCallback()]\n",
    ")\n",
    "print(\"\\n\\n--- Training complete. ---\")\n",
    "\n",
    "\n",
    "# --- PART 3: Save the Model ---\n",
    "MODEL_PATH = MODELS_DIR / \"coi_genus_classifier.keras\"\n",
    "print(f\"\\nSaving trained model to: {MODEL_PATH}\")\n",
    "model.save(MODEL_PATH)\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe515a98-57e7-4daf-b641-1a43be2d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: FINAL MODEL EVALUATION AND VISUALIZATION\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To perform a definitive, unbiased evaluation of the trained COI model on\n",
    "#   the unseen test set. This provides the final \"report card\" for this model.\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   1.  Proactively clear system memory to ensure a stable evaluation environment.\n",
    "#   2.  Load the saved model and the corresponding test data from their files.\n",
    "#   3.  Run `model.evaluate()` on the test set to get the final accuracy score.\n",
    "#   4.  Attempt to plot the training history for visual analysis, gracefully\n",
    "#       handling the case where the kernel may have been restarted.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports for this cell ---\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "\n",
    "# --- Define all necessary file paths ---\n",
    "project_root = Path.cwd().parent\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"coi_genus_classifier.keras\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_coi.npz\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_coi.npy\"\n",
    "\n",
    "# --- 1. Clean up memory ---\n",
    "print(\"--- Starting Post-Training Workflow ---\")\n",
    "print(\"Clearing TensorFlow session and running garbage collection...\")\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "print(\"Memory cleared successfully.\")\n",
    "\n",
    "# --- 2. Load model and test data ---\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "loaded_model = load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "print(f\"\\nLoading test data...\")\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "print(\"Test data loaded successfully.\")\n",
    "\n",
    "# --- 3. Evaluate the model ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\n--- Final COI Model Evaluation ---\")\n",
    "print(f\"Test Set Loss:     {loss:.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy:.2%}\")\n",
    "print(\"--------------------------------\\n\")\n",
    "\n",
    "# --- 4. Visualize training history (if possible) ---\n",
    "try:\n",
    "    print(\"--- Generating Training History Plots ---\")\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle('COI Model Training History', fontsize=18)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1.plot(history_df.index + 1, history_df['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history_df.index + 1, history_df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "    ax1.set_title('Model Accuracy Over Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--')\n",
    "\n",
    "    # Plot Loss\n",
    "    ax2.plot(history_df.index + 1, history_df['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history_df.index + 1, history_df['val_loss'], label='Validation Loss', marker='o')\n",
    "    ax2.set_title('Model Loss Over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nCould not generate plots because the 'history' object was not found in memory.\")\n",
    "    print(\"This is expected if the kernel was restarted after training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
