{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dc9749-67fb-4b6b-b8a8-f48600f33a69",
   "metadata": {},
   "source": [
    "# 16S Data Preparation: Refinement and Development\n",
    "\n",
    "**Objective:** Refine the data preparation pipeline for the 16S rRNA gene (Bacteria & Archaea). \n",
    "\n",
    "**Methodology:**\n",
    "1. Start with a small, manageable sample of the full SILVA database.\n",
    "2. Interactively develop and test each step of the pipeline (Filtering, Parsing, K-mer Counting, Vectorizing).\n",
    "3. Ensure the logic is robust before converting it to a final `.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e806a1-9866-455a-908e-723f29c7390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports if needed later\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e08f9e-f8f7-452f-ae27-fe9f6dbef08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "# Create the processed data directory if it doesn't exist yet\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to the full, original SILVA database file\n",
    "FULL_SILVA_PATH = RAW_DATA_DIR / \"SILVA_138.1_SSURef_NR99_tax_silva.fasta\"\n",
    "\n",
    "# Path to the small sample file we will create for development\n",
    "SAMPLE_SILVA_PATH = RAW_DATA_DIR / \"SILVA_sample_10k.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdbdd4-2e5f-4a3b-a23a-cfbe6788f413",
   "metadata": {},
   "source": [
    "### Step 1: Create a Small Sample for Development\n",
    "\n",
    "We will read the first 10,000 sequences from the full SILVA database and save them to a new file. This allows us to develop the rest of the pipeline quickly without waiting for the full dataset to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a8ecfd-ea9d-4782-aa13-d6f1d9fe2da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample file already exists. No action needed.\n",
      "   Location: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\SILVA_sample_10k.fasta\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "# We only create the file if it doesn't already exist.\n",
    "# This saves time if we have to re-run the notebook.\n",
    "if not SAMPLE_SILVA_PATH.exists():\n",
    "    print(f\"Full SILVA file found. Creating a sample of {SAMPLE_SIZE} sequences...\")\n",
    "    print(f\"This may take a moment...\")\n",
    "    \n",
    "    # Use a progress bar to see what's happening\n",
    "    with open(FULL_SILVA_PATH, \"r\") as handle_in:\n",
    "        # Use a generator expression for memory efficiency\n",
    "        records_iterator = (record for record in SeqIO.parse(handle_in, \"fasta\"))\n",
    "        \n",
    "        sample_records = []\n",
    "        for i, record in tqdm(enumerate(records_iterator), total=SAMPLE_SIZE):\n",
    "            if i >= SAMPLE_SIZE:\n",
    "                break\n",
    "            sample_records.append(record)\n",
    "            \n",
    "    # Write the collected sample records to the new file\n",
    "    with open(SAMPLE_SILVA_PATH, \"w\") as handle_out:\n",
    "        SeqIO.write(sample_records, handle_out, \"fasta\")\n",
    "        \n",
    "    print(f\"✅ Successfully created sample file with {len(sample_records)} sequences.\")\n",
    "    print(f\"   Location: {SAMPLE_SILVA_PATH}\")\n",
    "else:\n",
    "    print(f\"✅ Sample file already exists. No action needed.\")\n",
    "    print(f\"   Location: {SAMPLE_SILVA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2dfac-2bd0-4178-951c-b4a4a20039af",
   "metadata": {},
   "source": [
    "### Step 2: Filter the Sample for Prokaryotes (Bacteria & Archaea)\n",
    "\n",
    "Now we will read our `SILVA_sample_10k.fasta` file and create a list in memory containing only the records that are explicitly labeled as \"Bacteria\" or \"Archaea\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d85410-10a9-45ae-a646-3b9bf342e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from sample file: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\SILVA_sample_10k.fasta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120420ff262149b687259767ea6aee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Filtering Summary ---\n",
      "Total sequences in sample: 10000\n",
      "Found 6883 prokaryote sequences.\n",
      "✅ Filtering complete.\n"
     ]
    }
   ],
   "source": [
    "prokaryote_records = []\n",
    "\n",
    "# We use tqdm again to see the progress\n",
    "print(f\"Reading from sample file: {SAMPLE_SILVA_PATH}\")\n",
    "\n",
    "with open(SAMPLE_SILVA_PATH, \"r\") as handle:\n",
    "    for record in tqdm(SeqIO.parse(handle, \"fasta\"), total=SAMPLE_SIZE):\n",
    "        # The description line contains the full taxonomy string\n",
    "        description = record.description.lower() # Use .lower() for a case-insensitive match\n",
    "        \n",
    "        if \"bacteria\" in description or \"archaea\" in description:\n",
    "            prokaryote_records.append(record)\n",
    "\n",
    "# Print a summary to verify the result\n",
    "print(\"\\n--- Filtering Summary ---\")\n",
    "print(f\"Total sequences in sample: {SAMPLE_SIZE}\")\n",
    "print(f\"Found {len(prokaryote_records)} prokaryote sequences.\")\n",
    "print(f\"✅ Filtering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a82e75-282f-4864-80d2-122ef27e3e01",
   "metadata": {},
   "source": [
    "### Step 3: Parse Taxonomy from Filtered Records\n",
    "\n",
    "The description for each sequence in the FASTA file contains the full taxonomic lineage, separated by semicolons (e.g., `Bacteria;Proteobacteria;Gammaproteobacteria...`). \n",
    "\n",
    "We will now:\n",
    "1. Define a function to parse this string into distinct taxonomic ranks.\n",
    "2. Apply this function to our list of 6,883 prokaryote records.\n",
    "3. Store the structured data in a pandas DataFrame for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616f5ed0-60d3-4f13-b201-22cfe793782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b360c3bdebdc4373806f0c727bee7ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parsing complete. Created a DataFrame with 6883 rows.\n",
      "Here's a preview of the structured data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Pseudomonadales</td>\n",
       "      <td>Pseudomonadaceae</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>Pseudomonas amygdali pv. morsprunorum</td>\n",
       "      <td>AB001445.1.1538</td>\n",
       "      <td>AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Enterobacterales</td>\n",
       "      <td>Pectobacteriaceae</td>\n",
       "      <td>Dickeya</td>\n",
       "      <td>Dickeya phage phiDP10.3</td>\n",
       "      <td>KM209255.204.1909</td>\n",
       "      <td>AGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGCCUAACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Actinobacteriota</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinomycetaceae</td>\n",
       "      <td>F0332</td>\n",
       "      <td>None</td>\n",
       "      <td>HL281554.1.1313</td>\n",
       "      <td>GACGAACGCUGGCGGCGUGCUUAACACAUGCAAGUCGAACGAGUGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Streptococcaceae</td>\n",
       "      <td>Streptococcus</td>\n",
       "      <td>Streptococcus equi</td>\n",
       "      <td>AB002515.1.1332</td>\n",
       "      <td>GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Streptococcaceae</td>\n",
       "      <td>Streptococcus</td>\n",
       "      <td>Streptococcus porcinus</td>\n",
       "      <td>AB002523.1.1496</td>\n",
       "      <td>UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kingdom            phylum                class             order  \\\n",
       "0  Bacteria    Proteobacteria  Gammaproteobacteria   Pseudomonadales   \n",
       "1  Bacteria    Proteobacteria  Gammaproteobacteria  Enterobacterales   \n",
       "2  Bacteria  Actinobacteriota       Actinobacteria   Actinomycetales   \n",
       "3  Bacteria        Firmicutes              Bacilli   Lactobacillales   \n",
       "4  Bacteria        Firmicutes              Bacilli   Lactobacillales   \n",
       "\n",
       "              family          genus                                species  \\\n",
       "0   Pseudomonadaceae    Pseudomonas  Pseudomonas amygdali pv. morsprunorum   \n",
       "1  Pectobacteriaceae        Dickeya                Dickeya phage phiDP10.3   \n",
       "2   Actinomycetaceae          F0332                                   None   \n",
       "3   Streptococcaceae  Streptococcus                     Streptococcus equi   \n",
       "4   Streptococcaceae  Streptococcus                 Streptococcus porcinus   \n",
       "\n",
       "                  id                                           sequence  \n",
       "0    AB001445.1.1538  AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...  \n",
       "1  KM209255.204.1909  AGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGCCUAACA...  \n",
       "2    HL281554.1.1313  GACGAACGCUGGCGGCGUGCUUAACACAUGCAAGUCGAACGAGUGG...  \n",
       "3    AB002515.1.1332  GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...  \n",
       "4    AB002523.1.1496  UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list to hold our structured data\n",
    "parsed_data = []\n",
    "\n",
    "# Define a set of useless terms we want to ignore in the taxonomy\n",
    "DISCARD_RANKS = {'uncultured', 'unidentified', 'metagenome'}\n",
    "\n",
    "def parse_silva_taxonomy(taxonomy_str):\n",
    "    \"\"\"\n",
    "    Parses a SILVA taxonomy string (e.g., \"Bacteria;Firmicutes;...\")\n",
    "    into a dictionary of ranks.\n",
    "    \"\"\"\n",
    "    # Start with a dictionary of empty ranks\n",
    "    parsed_ranks = {\n",
    "        'kingdom': None, 'phylum': None, 'class': None, \n",
    "        'order': None, 'family': None, 'genus': None, 'species': None\n",
    "    }\n",
    "    \n",
    "    # Split the string by ';' and remove any useless terms\n",
    "    ranks = [\n",
    "        rank.strip() for rank in taxonomy_str.split(';') \n",
    "        if rank.strip() and rank.strip().lower() not in DISCARD_RANKS\n",
    "    ]\n",
    "    \n",
    "    if not ranks:\n",
    "        return parsed_ranks # Return empty if nothing is left\n",
    "        \n",
    "    # Assign ranks based on their position\n",
    "    # This is a safe way that avoids errors if some ranks are missing\n",
    "    if len(ranks) > 0: parsed_ranks['kingdom'] = ranks[0]\n",
    "    if len(ranks) > 1: parsed_ranks['phylum'] = ranks[1]\n",
    "    if len(ranks) > 2: parsed_ranks['class'] = ranks[2]\n",
    "    if len(ranks) > 3: parsed_ranks['order'] = ranks[3]\n",
    "    if len(ranks) > 4: parsed_ranks['family'] = ranks[4]\n",
    "    if len(ranks) > 5: parsed_ranks['genus'] = ranks[5]\n",
    "    # Species often contains two words, but we'll just take the whole string for now\n",
    "    if len(ranks) > 6: parsed_ranks['species'] = ranks[6]\n",
    "        \n",
    "    return parsed_ranks\n",
    "\n",
    "# Loop through our filtered list of records\n",
    "for record in tqdm(prokaryote_records):\n",
    "    # The taxonomy string is the part of the description after the first space\n",
    "    accession, taxonomy_str = record.description.split(' ', 1)\n",
    "    \n",
    "    # Parse the taxonomy string using our function\n",
    "    taxonomy_dict = parse_silva_taxonomy(taxonomy_str)\n",
    "    \n",
    "    # Also store the sequence and its ID\n",
    "    taxonomy_dict['id'] = record.id\n",
    "    taxonomy_dict['sequence'] = str(record.seq)\n",
    "    \n",
    "    parsed_data.append(taxonomy_dict)\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(f\"✅ Parsing complete. Created a DataFrame with {len(df)} rows.\")\n",
    "print(\"Here's a preview of the structured data:\")\n",
    "df.head() # Display the first 5 rows of our new table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72274c-e354-45a5-b863-8f3dc31a8ab3",
   "metadata": {},
   "source": [
    "### Step 4: Clean and Filter the DataFrame\n",
    "\n",
    "Our goal is to train a model to predict the `genus`. Therefore, we must perform two cleaning steps:\n",
    "\n",
    "1.  **Remove Missing Targets:** Drop any rows from our DataFrame where the `genus` column is empty (`None`). A sequence without a label is useless for training a supervised model.\n",
    "2.  **Remove Singletons:** Remove any genera that only have one single sequence representing them. The `train_test_split` function requires at least two members of each class for proper stratified splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f4d3d2-0208-48d2-b363-16a62176fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Removed rows with missing 'genus' labels.\n",
      "  - Started with: 6883 rows\n",
      "  - Remaining:    6563 rows\n",
      "  - Removed:      320 rows\n",
      "\n",
      "Step 2: Removed 'singleton' genera (genera with only 1 example).\n",
      "  - Started with: 6563 rows\n",
      "  - Remaining:    5562 rows\n",
      "  - Removed:      1001 rows\n",
      "\n",
      "✅ Cleaning complete. Our final DataFrame for feature engineering has 5562 sequences.\n"
     ]
    }
   ],
   "source": [
    "# The taxonomic rank we want our model to predict\n",
    "TARGET_RANK = 'genus'\n",
    "\n",
    "# --- 1. Remove Missing Targets ---\n",
    "initial_rows = len(df)\n",
    "df_cleaned = df.dropna(subset=[TARGET_RANK]).copy() # Use .copy() to avoid warnings\n",
    "rows_after_dropna = len(df_cleaned)\n",
    "\n",
    "print(f\"Step 1: Removed rows with missing '{TARGET_RANK}' labels.\")\n",
    "print(f\"  - Started with: {initial_rows} rows\")\n",
    "print(f\"  - Remaining:    {rows_after_dropna} rows\")\n",
    "print(f\"  - Removed:      {initial_rows - rows_after_dropna} rows\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Remove Singletons ---\n",
    "# First, count how many times each genus appears\n",
    "class_counts = df_cleaned[TARGET_RANK].value_counts()\n",
    "\n",
    "# We will now require a class to have at least 3 members to be included.\n",
    "genera_to_keep = class_counts[class_counts >= 3].index\n",
    "\n",
    "# Filter the DataFrame to keep only those genera\n",
    "df_filtered = df_cleaned[df_cleaned[TARGET_RANK].isin(genera_to_keep)].copy()\n",
    "rows_after_filter = len(df_filtered)\n",
    "\n",
    "print(f\"Step 2: Removed 'singleton' genera (genera with only 1 example).\")\n",
    "print(f\"  - Started with: {rows_after_dropna} rows\")\n",
    "print(f\"  - Remaining:    {rows_after_filter} rows\")\n",
    "print(f\"  - Removed:      {rows_after_dropna - rows_after_filter} rows\\n\")\n",
    "\n",
    "\n",
    "print(f\"✅ Cleaning complete. Our final DataFrame for feature engineering has {len(df_filtered)} sequences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45928f4e-c637-43bb-a1ca-ccd7d3b7412c",
   "metadata": {},
   "source": [
    "### Step 5: Feature Engineering (K-mer Counting)\n",
    "\n",
    "A machine learning model cannot understand a DNA sequence like \"AGTC...\". We must convert it into numbers. Our strategy is to count the occurrences of small DNA sub-sequences of a fixed length, called **k-mers**.\n",
    "\n",
    "For example, if k=3, the sequence \"AGTAG\" contains the k-mers: \"AGT\", \"GTA\", \"TAG\".\n",
    "\n",
    "We will now:\n",
    "1. Define a function to calculate these k-mer counts for a single sequence.\n",
    "2. Apply this function to every sequence in our cleaned DataFrame.\n",
    "3. Store the results in a new `kmer_counts` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0beb4e8e-baab-49ae-8ae5-57717d63bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating 6-mer counts for 5562 sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327e7dbc6a45407f97ea687b1c356348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Feature engineering complete.\n",
      "Here's a preview of the new 'kmer_counts' column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>kmer_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...</td>\n",
       "      <td>{'AACUGA': 2, 'ACUGAA': 1, 'CUGAAG': 1, 'UGAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...</td>\n",
       "      <td>{'GCCUAA': 1, 'CCUAAU': 1, 'CUAAUA': 2, 'UAAUA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...</td>\n",
       "      <td>{'UCCUGG': 1, 'CCUGGC': 1, 'CUGGCU': 1, 'UGGCU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCGUUGUUUCCAUCGCUCUACCAUGCAGUCGACGCUGAGCUCAGCU...</td>\n",
       "      <td>{'GCGUUG': 2, 'CGUUGU': 2, 'GUUGUU': 1, 'UUGUU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UUGCGGCCACCUACACAUGCAGUCGAGCGGAUGAAGGGAGCUUGCU...</td>\n",
       "      <td>{'UUGCGG': 1, 'UGCGGC': 1, 'GCGGCC': 1, 'CGGCC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  AACUGAAGAGUUUGAUCAUGGCUCAGAUUGAACGCUGGCGGCAGGC...   \n",
       "3  GCCUAAUACAUGCAAGUUGACGACAGAUGAUACGUAGCUUGCUACA...   \n",
       "4  UCCUGGCUCAGGACGAACGCUGGCGGCGUGCCUAAUACAUGCAAGU...   \n",
       "5  GCGUUGUUUCCAUCGCUCUACCAUGCAGUCGACGCUGAGCUCAGCU...   \n",
       "6  UUGCGGCCACCUACACAUGCAGUCGAGCGGAUGAAGGGAGCUUGCU...   \n",
       "\n",
       "                                         kmer_counts  \n",
       "0  {'AACUGA': 2, 'ACUGAA': 1, 'CUGAAG': 1, 'UGAAG...  \n",
       "3  {'GCCUAA': 1, 'CCUAAU': 1, 'CUAAUA': 2, 'UAAUA...  \n",
       "4  {'UCCUGG': 1, 'CCUGGC': 1, 'CUGGCU': 1, 'UGGCU...  \n",
       "5  {'GCGUUG': 2, 'CGUUGU': 2, 'GUUGUU': 1, 'UUGUU...  \n",
       "6  {'UUGCGG': 1, 'UGCGGC': 1, 'GCGGCC': 1, 'CGGCC...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the length of our k-mers. From your original config, 6 is a good choice for 16S.\n",
    "KMER_SIZE = 6\n",
    "\n",
    "def get_kmer_counts(sequence, k):\n",
    "    \"\"\"Calculates the k-mer counts for a given DNA sequence.\"\"\"\n",
    "    # Use a Counter for efficiency\n",
    "    counts = Counter()\n",
    "    num_kmers = len(sequence) - k + 1\n",
    "    \n",
    "    for i in range(num_kmers):\n",
    "        kmer = sequence[i:i+k]\n",
    "        # Important: Ignore k-mers with 'N' (unknown bases)\n",
    "        if \"N\" not in kmer.upper():\n",
    "            counts[kmer] += 1\n",
    "            \n",
    "    return dict(counts)\n",
    "\n",
    "# Apply our function to the 'sequence' column.\n",
    "# The tqdm wrapper will show a progress bar.\n",
    "print(f\"Calculating {KMER_SIZE}-mer counts for {len(df_filtered)} sequences...\")\n",
    "\n",
    "df_filtered['kmer_counts'] = list(tqdm(\n",
    "    (get_kmer_counts(seq, KMER_SIZE) for seq in df_filtered['sequence']), \n",
    "    total=len(df_filtered)\n",
    "))\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n✅ Feature engineering complete.\")\n",
    "print(\"Here's a preview of the new 'kmer_counts' column:\")\n",
    "\n",
    "# Show the original sequence and its corresponding k-mer counts\n",
    "df_filtered[['sequence', 'kmer_counts']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d4e0c-f499-437c-9a09-ba0b9fa06be9",
   "metadata": {},
   "source": [
    "### Step 6: Vectorize Features and Labels\n",
    "\n",
    "This is the final transformation step. We will use tools from `scikit-learn` to create our final `X` (features) and `y` (labels) matrices.\n",
    "\n",
    "1.  **`DictVectorizer`:** This will take our `kmer_counts` column and create the massive feature matrix (`X`), where each column represents one unique k-mer.\n",
    "2.  **`LabelEncoder`:** This will take our `genus` column (which contains text) and convert it into integer labels (`y`) that the model can understand (e.g., *Pseudomonas* -> 0, *Dickeya* -> 1, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b2115f-a011-4078-8310-35150b937631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing k-mer counts...\n",
      "Encoding target labels...\n",
      "\n",
      "✅ Vectorization and encoding complete.\n",
      "Shape of our feature matrix (X): (5562, 12850)\n",
      "Shape of our label vector (y): (5562,)\n",
      "\n",
      "We now have 5562 sequences ready for the model.\n",
      "Each sequence is described by 12850 unique k-mer features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. Vectorize the features (X) ---\n",
    "print(\"Vectorizing k-mer counts...\")\n",
    "vectorizer = DictVectorizer(sparse=True) # Use a sparse matrix for memory efficiency\n",
    "X = vectorizer.fit_transform(df_filtered['kmer_counts'])\n",
    "\n",
    "\n",
    "# --- 2. Encode the labels (y) ---\n",
    "print(\"Encoding target labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_RANK])\n",
    "\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n✅ Vectorization and encoding complete.\")\n",
    "print(f\"Shape of our feature matrix (X): {X.shape}\")\n",
    "print(f\"Shape of our label vector (y): {y.shape}\")\n",
    "print(f\"\\nWe now have {X.shape[0]} sequences ready for the model.\")\n",
    "print(f\"Each sequence is described by {X.shape[1]} unique k-mer features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686d986-90db-4c3b-abc3-d98ebc864aa5",
   "metadata": {},
   "source": [
    "### Step 7: Split Data into Training and Testing Sets\n",
    "\n",
    "This is the final step before model training. We will divide our dataset into two parts:\n",
    "\n",
    "-   **Training Set (80%):** The data the model will learn from.\n",
    "-   **Testing Set (20%):** The data the model will be evaluated on to see how well it performs on unseen sequences.\n",
    "\n",
    "We will use `train_test_split` from `scikit-learn`, ensuring we use `stratify=y`. This is crucial as it guarantees that the proportion of each genus is the same in both the training and testing sets, which prevents bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89608883-6664-4277-a379-9e55261572ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data splitting complete.\n",
      "Shape of X_train: (4449, 12850)\n",
      "Shape of y_train: (4449,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1113, 12850)\n",
      "Shape of y_test:  (1113,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define our settings\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42 # Ensures the split is the same every time we run the code\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SPLIT_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Very important for imbalanced datasets!\n",
    ")\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"✅ Data splitting complete.\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d430718-17cf-43ad-80fe-74866aa52d8f",
   "metadata": {},
   "source": [
    "### Step 8: Save All Processed Artifacts\n",
    "\n",
    "Our data is now fully prepared. The final step is to save all our objects to files in the `data/processed/` directory. This will allow our future model training script to load them directly.\n",
    "\n",
    "We will save:\n",
    "- The training and testing data (`X_train`, `X_test`, `y_train`, `y_test`).\n",
    "- The fitted `DictVectorizer` and `LabelEncoder`, which are essential for processing new, unseen data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa3d3ee-43cf-4d3f-b5a8-bb096e1a997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to C:\\Users\\jampa\\Music\\atlas\\data\\processed...\n",
      "  - Data saved successfully.\n",
      "Saving encoders to C:\\Users\\jampa\\Music\\atlas\\models...\n",
      "  - Encoders saved successfully.\n",
      "\n",
      "✅ All artifacts have been saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "# --- Define unique filenames for this 16S pipeline ---\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_16s.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_16s.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_16s.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_16s.npy\"\n",
    "\n",
    "VECTORIZER_PATH = project_root / \"models\" / \"16s_genus_vectorizer.pkl\"\n",
    "LABEL_ENCODER_PATH = project_root / \"models\" / \"16s_genus_label_encoder.pkl\"\n",
    "\n",
    "# Create the models directory if it doesn't exist yet\n",
    "(project_root / \"models\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Save the data matrices ---\n",
    "print(f\"Saving data to {PROCESSED_DATA_DIR}...\")\n",
    "save_npz(X_TRAIN_PATH, X_train)\n",
    "save_npz(X_TEST_PATH, X_test)\n",
    "np.save(Y_TRAIN_PATH, y_train)\n",
    "np.save(Y_TEST_PATH, y_test)\n",
    "print(\"  - Data saved successfully.\")\n",
    "\n",
    "# --- Save the encoders using pickle ---\n",
    "print(f\"Saving encoders to {project_root / 'models'}...\")\n",
    "with open(VECTORIZER_PATH, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    \n",
    "with open(LABEL_ENCODER_PATH, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"  - Encoders saved successfully.\")\n",
    "    \n",
    "print(\"\\n✅ All artifacts have been saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdfc1f2-70d8-42c3-8e6c-20a342e2a013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d4d919-9086-4bd0-9533-87ddf6232260",
   "metadata": {},
   "source": [
    "# 16S Model Training and Evaluation\n",
    "\n",
    "**Objective:** To build, train, and evaluate a deep learning classifier for the 16S rRNA gene using the pre-processed data.\n",
    "\n",
    "**Methodology:**\n",
    "1. Load the training/testing data and encoders from disk.\n",
    "2. Define the neural network architecture using TensorFlow/Keras.\n",
    "3. Train the model on the training data, using the GPU if available.\n",
    "4. Evaluate the final model's accuracy on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c866ac0-2727-4016-bb94-936338adb111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Setup ---\n",
      "TensorFlow Version: 2.10.1\n",
      "✅ GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set up project path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# --- Verification Step: Check for GPU ---\n",
    "# This will tell us if TensorFlow can see your GPU.\n",
    "print(\"--- TensorFlow Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"✅ GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b92b03-1dbf-4385-bf3b-44a02129c5bf",
   "metadata": {},
   "source": [
    "### Step 2: Load Pre-processed Data and Encoders\n",
    "\n",
    "We will now load all the artifacts that were saved by our data preparation notebook. This includes the training data, testing data, and the crucial `vectorizer` and `label_encoder` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893ba25a-a7c8-4eb5-8eb6-5a8b9546301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from disk...\n",
      "✅ Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "Shape of X_train: (4449, 12850)\n",
      "Shape of y_train: (4449,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1113, 12850)\n",
      "Shape of y_test:  (1113,)\n",
      "Number of classes (genera): 345\n"
     ]
    }
   ],
   "source": [
    "# --- Define file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_16s.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_16s.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_16s.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_16s.npy\"\n",
    "\n",
    "VECTORIZER_PATH = MODELS_DIR / \"16s_genus_vectorizer.pkl\"\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"16s_genus_label_encoder.pkl\"\n",
    "\n",
    "\n",
    "# --- Load the data and encoders ---\n",
    "print(\"Loading data from disk...\")\n",
    "X_train = load_npz(X_TRAIN_PATH)\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_train = np.load(Y_TRAIN_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Note: We don't need to load the vectorizer right now, but we will need it for a final script.\n",
    "# The label_encoder is important because it tells us the number of classes.\n",
    "print(\"✅ Data loading complete.\")\n",
    "\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34222ab-f697-4762-b3b3-bea2714fa00d",
   "metadata": {},
   "source": [
    "### Step 3: Define the Neural Network Architecture\n",
    "\n",
    "We will now define our deep learning model using the Keras `Sequential` API. The architecture will consist of a series of layers:\n",
    "\n",
    "-   An **Input Layer** that accepts our k-mer feature vectors.\n",
    "-   Two hidden **Dense** layers with ReLU activation, which act as the primary learning components of the network.\n",
    "-   **Dropout** layers placed after each Dense layer to prevent the model from overfitting to the training data.\n",
    "-   An **Output Layer** with a `softmax` activation function, which will output the probability for each of the possible genera.\n",
    "\n",
    "The model will then be compiled with an `adam` optimizer and a `sparse_categorical_crossentropy` loss function, which are standard and effective choices for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d13394-a57d-4129-97f9-9dfc1f07f4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Defined and Compiled.\n",
      "Here is a summary:\n",
      "\n",
      "+-----------------------------------------------------------------+\n",
      "| Input Layer:         (None, 12850)                     |\n",
      "| Dense Layer (ReLU):    (None, 2048)                               |\n",
      "| Dropout (0.5):         (None, 2048)                               |\n",
      "| Dense Layer (ReLU):    (None, 1024)                               |\n",
      "| Dropout (0.5):         (None, 1024)                               |\n",
      "| Output Layer (Softmax):(None, 345)                                |\n",
      "+-----------------------------------------------------------------+\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              26318848  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 345)               353625    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,770,649\n",
      "Trainable params: 28,770,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# --- Get model parameters from our loaded data ---\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1] # The number of unique k-mer features\n",
    "\n",
    "# --- Define the Sequential model ---\n",
    "model = Sequential([\n",
    "    # Input layer and first hidden layer\n",
    "    Dense(2048, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# --- Compile the model ---\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- Print a summary of the model's architecture ---\n",
    "print(\"Model Architecture Defined and Compiled.\")\n",
    "print(\"Here is a summary:\")\n",
    "\n",
    "# ASCII Art representation\n",
    "print(\"\\n+-----------------------------------------------------------------+\")\n",
    "print(f\"| Input Layer:         (None, {input_shape})                     |\")\n",
    "print(\"| Dense Layer (ReLU):    (None, 2048)                               |\")\n",
    "print(\"| Dropout (0.5):         (None, 2048)                               |\")\n",
    "print(\"| Dense Layer (ReLU):    (None, 1024)                               |\")\n",
    "print(\"| Dropout (0.5):         (None, 1024)                               |\")\n",
    "print(f\"| Output Layer (Softmax):(None, {num_classes})                                |\")\n",
    "print(\"+-----------------------------------------------------------------+\\n\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcc6eb-d21d-40f0-b38d-7f98861679b1",
   "metadata": {},
   "source": [
    "### Step 4: Train the Neural Network\n",
    "\n",
    "We will now begin the training process by calling the `model.fit()` method. This function will feed the training data (`X_train`, `y_train`) to the model for a specified number of cycles, or **epochs**.\n",
    "\n",
    "During training, it will:\n",
    "-   Show the progress for each epoch.\n",
    "-   Calculate the `loss` and `accuracy` on the training data.\n",
    "-   After each epoch, it will evaluate the model on the **validation data** (a small portion of the training set held aside) to monitor for overfitting.\n",
    "-   We will use an `EarlyStopping` callback, which automatically stops the training process if the validation accuracy does not improve for a set number of epochs. This saves time and prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb44afb1-13d7-40c0-91a1-0aa6408a523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pre-flight Check for Stratification ---\n",
      "Training set is clean. No singletons found.\n",
      "-------------------------------------------\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 2048), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Loss: 3.4091 | Acc: 43.06% [████████············] | Val_Loss: 1.8653 | Val_Acc: 63.60% [████████████········]\n",
      "Epoch 02/50 | Loss: 2.1825 | Acc: 58.74% [███████████·········] | Val_Loss: 1.2338 | Val_Acc: 74.83% [██████████████······]\n",
      "Epoch 03/50 | Loss: 1.8218 | Acc: 63.89% [████████████········] | Val_Loss: 1.0215 | Val_Acc: 77.75% [███████████████·····]\n",
      "Epoch 04/50 | Loss: 1.5980 | Acc: 66.88% [█████████████·······] | Val_Loss: 0.7356 | Val_Acc: 84.27% [████████████████····]\n",
      "Epoch 05/50 | Loss: 1.4675 | Acc: 69.31% [█████████████·······] | Val_Loss: 0.6119 | Val_Acc: 86.74% [█████████████████···]\n",
      "Epoch 06/50 | Loss: 1.4046 | Acc: 70.70% [██████████████······] | Val_Loss: 0.5698 | Val_Acc: 87.19% [█████████████████···]\n",
      "Epoch 07/50 | Loss: 1.4341 | Acc: 69.48% [█████████████·······] | Val_Loss: 0.5089 | Val_Acc: 90.34% [██████████████████··]\n",
      "Epoch 08/50 | Loss: 1.3033 | Acc: 72.23% [██████████████······] | Val_Loss: 0.4667 | Val_Acc: 90.79% [██████████████████··]\n",
      "Epoch 09/50 | Loss: 1.2526 | Acc: 73.08% [██████████████······] | Val_Loss: 0.4559 | Val_Acc: 89.21% [█████████████████···]\n",
      "Epoch 10/50 | Loss: 1.2542 | Acc: 73.53% [██████████████······] | Val_Loss: 0.3261 | Val_Acc: 94.83% [██████████████████··]\n",
      "Epoch 11/50 | Loss: 1.1710 | Acc: 74.85% [██████████████······] | Val_Loss: 0.3140 | Val_Acc: 92.58% [██████████████████··]\n",
      "Epoch 12/50 | Loss: 1.1881 | Acc: 74.78% [██████████████······] | Val_Loss: 0.3869 | Val_Acc: 91.69% [██████████████████··]\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 13/50 | Loss: 1.1282 | Acc: 75.95% [███████████████·····] | Val_Loss: 0.3896 | Val_Acc: 91.01% [██████████████████··]\n",
      "Epoch 13: early stopping\n",
      "\n",
      "--- Training complete. ---\n",
      "\n",
      "Saving trained model to: C:\\Users\\jampa\\Music\\atlas\\models\\16s_genus_classifier.keras\n",
      "✅ Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "# --- FIX: Add the missing import statement ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- PART 1: FIX THE STRATIFICATION VALUE-ERROR ---\n",
    "# We must ensure our y_train set doesn't have any singleton classes before splitting it.\n",
    "\n",
    "print(\"--- Pre-flight Check for Stratification ---\")\n",
    "# Count the occurrences of each class in the training labels\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Find which classes have fewer than 2 members (the new singletons)\n",
    "singletons = unique_classes[class_counts < 2]\n",
    "\n",
    "if len(singletons) > 0:\n",
    "    print(f\"Found {len(singletons)} singleton class(es) in the training set. Removing them...\")\n",
    "    \n",
    "    # Get the indices of the rows that are NOT singletons\n",
    "    non_singleton_indices = np.where(~np.isin(y_train, singletons))[0]\n",
    "    \n",
    "    # Filter both X_train and y_train to keep only the non-singletons\n",
    "    X_train = X_train[non_singleton_indices]\n",
    "    y_train = y_train[non_singleton_indices]\n",
    "    \n",
    "    print(f\"Cleaned training set shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(\"Training set is clean. No singletons found.\")\n",
    "\n",
    "print(\"-\" * 43)\n",
    "\n",
    "\n",
    "# --- PART 2: CREATE THE BEAUTIFUL OUTPUT CALLBACK ---\n",
    "\n",
    "class TrainingProgressCallback(Callback):\n",
    "    \"\"\"A custom callback to print a single, clean line of progress for each epoch.\"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Access the metrics from the logs dictionary\n",
    "        acc = logs.get('accuracy', 0)\n",
    "        val_acc = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "\n",
    "        # Create progress bars using ASCII block characters\n",
    "        acc_bar = '█' * int(acc * 20) + '·' * (20 - int(acc * 20))\n",
    "        val_acc_bar = '█' * int(val_acc * 20) + '·' * (20 - int(val_acc * 20))\n",
    "\n",
    "        # Print the formatted output string\n",
    "        print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Loss: {loss:.4f} | Acc: {acc:.2%} [{acc_bar}] | Val_Loss: {val_loss:.4f} | Val_Acc: {val_acc:.2%} [{val_acc_bar}]\")\n",
    "\n",
    "\n",
    "# --- PART 3: TRAIN THE MODEL WITH THE FIX AND THE NEW CALLBACK ---\n",
    "\n",
    "# Define training parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Manually create the validation set from our now-guaranteed-clean training data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Start training ---\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    # We remove the default verbose output and add our custom callbacks\n",
    "    verbose=0, \n",
    "    callbacks=[early_stopping, TrainingProgressCallback()]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training complete. ---\")\n",
    "\n",
    "# --- Save the trained model immediately to secure our progress ---\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODEL_PATH = MODELS_DIR / \"16s_genus_classifier.keras\"\n",
    "print(f\"\\nSaving trained model to: {MODEL_PATH}\")\n",
    "model.save(MODEL_PATH)\n",
    "print(\"✅ Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511bfe5-0cd7-4f3a-8a0d-9201fcbb57e6",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Final Model Performance\n",
    "\n",
    "The training process only gives us the validation accuracy, which is used to guide the training. The true measure of a model's performance comes from evaluating it on the completely unseen **test set**.\n",
    "\n",
    "We will now use the `model.evaluate()` method to get the final loss and accuracy scores. This is our definitive report card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9312c934-7c51-476f-9ff7-b3941cdc7e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Post-Training Workflow ---\n",
      "Clearing TensorFlow session...\n",
      "✅ Memory cleared.\n",
      "\n",
      "Loading model from: C:\\Users\\jampa\\Music\\atlas\\models\\16s_genus_classifier.keras\n",
      "✅ Model loaded successfully.\n",
      "\n",
      "Loading test data...\n",
      "✅ Test data loaded successfully.\n",
      "\n",
      "Evaluating model on the test set...\n",
      "35/35 [==============================] - 2s 19ms/step - loss: 0.5575 - accuracy: 0.8895\n",
      "\n",
      "--- Final Model Evaluation ---\n",
      "Test Set Loss:     0.5575\n",
      "Test Set Accuracy: 88.95%\n",
      "----------------------------\n",
      "\n",
      "--- Generating Training History Plots ---\n",
      "\n",
      "Could not generate plots because the 'history' object was not found in memory.\n",
      "This is expected if you have restarted the kernel after training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# --- FIX: Add missing imports for loading data ---\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "\n",
    "# --- Define all file paths required for this cell ---\n",
    "project_root = Path.cwd().parent\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"16s_genus_classifier.keras\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_16s.npz\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_16s.npy\"\n",
    "\n",
    "# --- Clean up memory ---\n",
    "print(\"--- Starting Post-Training Workflow ---\")\n",
    "print(\"Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "print(\"✅ Memory cleared.\")\n",
    "\n",
    "# --- Load the model AND the test data from their files ---\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "loaded_model = load_model(MODEL_PATH)\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "print(f\"\\nLoading test data...\")\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "print(\"✅ Test data loaded successfully.\")\n",
    "\n",
    "# --- Evaluate the loaded model on the loaded test data ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "print(f\"Test Set Loss:     {loss:.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy:.2%}\")\n",
    "print(\"----------------------------\\n\")\n",
    "\n",
    "# --- Visualize the training history ---\n",
    "# NOTE: This part requires the 'history' object from the training cell.\n",
    "# If you have restarted the kernel, you will get a NameError here.\n",
    "# To see the plots again, you would need to re-run the training cell.\n",
    "try:\n",
    "    print(\"--- Generating Training History Plots ---\")\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle('Model Training History', fontsize=18, y=1.02)\n",
    "\n",
    "    # Plot 1: Accuracy\n",
    "    ax1.plot(history_df.index + 1, history_df['accuracy'], label='Training Accuracy', marker='o', color='blue')\n",
    "    ax1.plot(history_df.index + 1, history_df['val_accuracy'], label='Validation Accuracy', marker='o', color='green')\n",
    "    ax1.set_title('Model Accuracy Over Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--')\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    ax2.plot(history_df.index + 1, history_df['loss'], label='Training Loss', marker='o', color='red')\n",
    "    ax2.plot(history_df.index + 1, history_df['val_loss'], label='Validation Loss', marker='o', color='orange')\n",
    "    ax2.set_title('Model Loss Over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss (Cross-Entropy)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nCould not generate plots because the 'history' object was not found in memory.\")\n",
    "    print(\"This is expected if you have restarted the kernel after training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1610ee1-c85a-40f6-a55b-56ef3ddf3a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8138e9b-575f-4fb0-85b8-1fb5061eae5d",
   "metadata": {},
   "source": [
    "# 18S Data Preparation: Refinement and Development\n",
    "\n",
    "**Objective:** Refine the data preparation pipeline for the 18S rRNA gene (Eukaryotes) using the SILVA database.\n",
    "\n",
    "**Methodology:**\n",
    "1.  Filter the full SILVA database to create a dedicated `Eukaryotes_only.fasta` file. This is a one-time, computationally intensive step.\n",
    "2.  Create a small, manageable sample from this Eukaryote-only file for rapid development.\n",
    "3.  Develop and test a robust taxonomy parser specifically designed for the complexities of eukaryotic lineages.\n",
    "4.  Apply the full data cleaning and feature engineering workflow, and save the final artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ac9057-045e-44e7-9a79-19abc4bc5fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set up project path\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3eb1f7a-329a-4f4a-a7a1-736f5bb5d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source SILVA database found.\n",
      "  - Location: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\SILVA_138.1_SSURef_NR99_tax_silva.fasta\n"
     ]
    }
   ],
   "source": [
    "# --- Define Core Directories ---\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "# Create the processed data directory if it doesn't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Define 18S Specific File Paths ---\n",
    "\n",
    "# Path to the full, original SILVA database file (our main source)\n",
    "FULL_SILVA_PATH = RAW_DATA_DIR / \"SILVA_138.1_SSURef_NR99_tax_silva.fasta\"\n",
    "\n",
    "# Path to the intermediate file we will create, containing ONLY eukaryotes\n",
    "EUKARYOTE_ONLY_PATH = RAW_DATA_DIR / \"SILVA_eukaryotes_only.fasta\"\n",
    "\n",
    "# Path to the small sample file we will create from the eukaryote-only file\n",
    "SAMPLE_EUKARYOTE_PATH = RAW_DATA_DIR / \"SILVA_eukaryotes_sample_10k.fasta\"\n",
    "\n",
    "# --- Verification Step ---\n",
    "if FULL_SILVA_PATH.exists():\n",
    "    print(\"Source SILVA database found.\")\n",
    "    print(f\"  - Location: {FULL_SILVA_PATH}\")\n",
    "else:\n",
    "    print(f\"ERROR: The source SILVA database was not found at the expected location.\")\n",
    "    print(f\"  - Expected: {FULL_SILVA_PATH}\")\n",
    "    print(\"Please download the file and place it in the 'data/raw' directory before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db75bd5-e2e3-40e6-95a3-e7c793162cbe",
   "metadata": {},
   "source": [
    "### Step 1: Filter Full SILVA Database for Eukaryotes\n",
    "\n",
    "This is a one-time, computationally intensive step. We will read the entire source SILVA database and write a new FASTA file containing only the sequences belonging to the \"Eukaryota\" kingdom.\n",
    "\n",
    "This script is designed to be run only once. If the `SILVA_eukaryotes_only.fasta` file is found, this step will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc12fd1-eb7c-46e6-aa1e-141345d365f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eukaryote-only file not found. Starting filtering process...\n",
      "This will take a significant amount of time. Please be patient.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca42ca35a8e04c57a1ad79e30466e118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering full SILVA DB: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Filtering complete.\n",
      "   Found and wrote 58,545 Eukaryote sequences.\n",
      "   New file created at: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\SILVA_eukaryotes_only.fasta\n"
     ]
    }
   ],
   "source": [
    "# This check prevents us from re-running this long process unnecessarily.\n",
    "if not EUKARYOTE_ONLY_PATH.exists():\n",
    "    print(f\"Eukaryote-only file not found. Starting filtering process...\")\n",
    "    print(\"This will take a significant amount of time. Please be patient.\")\n",
    "    \n",
    "    eukaryote_count = 0\n",
    "    \n",
    "    # Open both the input and output files\n",
    "    with open(FULL_SILVA_PATH, \"r\") as handle_in, open(EUKARYOTE_ONLY_PATH, \"w\") as handle_out:\n",
    "        # Use tqdm to monitor the progress of this long-running task\n",
    "        for record in tqdm(SeqIO.parse(handle_in, \"fasta\"), desc=\"Filtering full SILVA DB\"):\n",
    "            # Check for the keyword \"Eukaryota\" in the description\n",
    "            if \"Eukaryota\" in record.description:\n",
    "                # Write the record to our new file\n",
    "                SeqIO.write(record, handle_out, \"fasta\")\n",
    "                eukaryote_count += 1\n",
    "                \n",
    "    print(f\"\\n✅ Filtering complete.\")\n",
    "    print(f\"   Found and wrote {eukaryote_count:,} Eukaryote sequences.\")\n",
    "    print(f\"   New file created at: {EUKARYOTE_ONLY_PATH}\")\n",
    "\n",
    "else:\n",
    "    print(f\"✅ Eukaryote-only file already exists. Skipping filtering step.\")\n",
    "    print(f\"   Location: {EUKARYOTE_ONLY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6dca25-6d83-4ddd-a85b-88b02fa50249",
   "metadata": {},
   "source": [
    "### Step 2: Create a Development Sample from Eukaryote Data\n",
    "\n",
    "Now that we have a clean file containing only Eukaryote sequences, we will create a smaller 10,000-sequence sample. This will serve as our development dataset for the rest of this notebook, ensuring all subsequent steps are fast and interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46829734-f195-4205-86dc-8fd71f2fa742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sample of 10000 Eukaryote sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efebbf2d6f7c469f8e215f42bf35126c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully created sample file with 10000 sequences.\n",
      "   Location: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\SILVA_eukaryotes_sample_10k.fasta\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "# We only create the sample file if it doesn't already exist.\n",
    "if not SAMPLE_EUKARYOTE_PATH.exists():\n",
    "    print(f\"Creating a sample of {SAMPLE_SIZE} Eukaryote sequences...\")\n",
    "    \n",
    "    sample_records = []\n",
    "    with open(EUKARYOTE_ONLY_PATH, \"r\") as handle_in:\n",
    "        # Use a generator expression for memory efficiency\n",
    "        records_iterator = (record for record in SeqIO.parse(handle_in, \"fasta\"))\n",
    "        \n",
    "        # Use tqdm to show progress\n",
    "        for i, record in tqdm(enumerate(records_iterator), total=SAMPLE_SIZE):\n",
    "            if i >= SAMPLE_SIZE:\n",
    "                break\n",
    "            sample_records.append(record)\n",
    "            \n",
    "    # Write the collected sample records to the new file\n",
    "    with open(SAMPLE_EUKARYOTE_PATH, \"w\") as handle_out:\n",
    "        SeqIO.write(sample_records, handle_out, \"fasta\")\n",
    "        \n",
    "    print(f\"\\n✅ Successfully created sample file with {len(sample_records)} sequences.\")\n",
    "    print(f\"   Location: {SAMPLE_EUKARYOTE_PATH}\")\n",
    "else:\n",
    "    print(f\"✅ Eukaryote sample file already exists. No action needed.\")\n",
    "    print(f\"   Location: {SAMPLE_EUKARYOTE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ca32a-dbfe-4982-8cb2-2924ecd75cd4",
   "metadata": {},
   "source": [
    "### Step 3: Develop a Robust Parser for Eukaryotic Taxonomy\n",
    "\n",
    "This is the most critical step for the 18S pipeline. Eukaryotic taxonomy strings in SILVA are inconsistent and contain many non-standard ranks that must be filtered out.\n",
    "\n",
    "We will:\n",
    "1.  Define a set of known non-standard ranks to discard.\n",
    "2.  Create a new parsing function that intelligently identifies and assigns the correct major ranks (Phylum, Class, Order, Family, Genus, Species).\n",
    "3.  Apply this function to our 10,000-sequence sample and inspect the resulting DataFrame for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04dc830a-947b-47aa-ad7a-ba9defcc305d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef7928ac56c437dbeab79ffd66ccaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing sample taxonomy:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Parsing complete. Created a DataFrame with 10000 rows.\n",
      "Here's a preview of the structured data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Chlorophyta</td>\n",
       "      <td>Chlorophyta</td>\n",
       "      <td>Chlorophyceae</td>\n",
       "      <td>Sphaeropleales</td>\n",
       "      <td>Monoraphidium</td>\n",
       "      <td>Monoraphidium sp. Itas 9/21 14-6w</td>\n",
       "      <td>AY846379.1.1791</td>\n",
       "      <td>AACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Charophyta</td>\n",
       "      <td>Spermatophyta</td>\n",
       "      <td>Magnoliophyta</td>\n",
       "      <td>Oxalidales</td>\n",
       "      <td>Connarus</td>\n",
       "      <td>Connarus championii</td>\n",
       "      <td>AY929368.1.1768</td>\n",
       "      <td>AAGAUUAAGCCAUGCAUGUGUAAGUAUGAACUAAUUCAGACUGUGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Chlorophyta</td>\n",
       "      <td>Chlorophyta</td>\n",
       "      <td>Mamiellophyceae</td>\n",
       "      <td>Mamiellales</td>\n",
       "      <td>Micromonas</td>\n",
       "      <td>Micromonas pusilla</td>\n",
       "      <td>AY955002.1.1727</td>\n",
       "      <td>UGUCUAAGUAUAAGCGUUAUACUGUGAAACUGCGAAUGGCUCAUUA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Chlorophyta</td>\n",
       "      <td>Chlorophyta</td>\n",
       "      <td>Chlorophyceae</td>\n",
       "      <td>Chlamydomonadales</td>\n",
       "      <td>Chlamydomonas</td>\n",
       "      <td>Chlamydomonas orbicularis</td>\n",
       "      <td>LF644976.16.1783</td>\n",
       "      <td>AGCGUUGCAUGCCUGCAGGUCGACUCUAGAGGGGAUCCAGAUCUCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Amorphea</td>\n",
       "      <td>Amoebozoa</td>\n",
       "      <td>Centrohelida</td>\n",
       "      <td>Heterophryidae</td>\n",
       "      <td>Chlamydaster</td>\n",
       "      <td>Chlamydaster sterni</td>\n",
       "      <td>KY857824.1.1808</td>\n",
       "      <td>AACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     kingdom       phylum          class            order             family  \\\n",
       "0  Eukaryota  Chlorophyta    Chlorophyta    Chlorophyceae     Sphaeropleales   \n",
       "1  Eukaryota   Charophyta  Spermatophyta    Magnoliophyta         Oxalidales   \n",
       "2  Eukaryota  Chlorophyta    Chlorophyta  Mamiellophyceae        Mamiellales   \n",
       "3  Eukaryota  Chlorophyta    Chlorophyta    Chlorophyceae  Chlamydomonadales   \n",
       "4  Eukaryota     Amorphea      Amoebozoa     Centrohelida     Heterophryidae   \n",
       "\n",
       "           genus                            species                id  \\\n",
       "0  Monoraphidium  Monoraphidium sp. Itas 9/21 14-6w   AY846379.1.1791   \n",
       "1       Connarus                Connarus championii   AY929368.1.1768   \n",
       "2     Micromonas                 Micromonas pusilla   AY955002.1.1727   \n",
       "3  Chlamydomonas          Chlamydomonas orbicularis  LF644976.16.1783   \n",
       "4   Chlamydaster                Chlamydaster sterni   KY857824.1.1808   \n",
       "\n",
       "                                            sequence  \n",
       "0  AACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUA...  \n",
       "1  AAGAUUAAGCCAUGCAUGUGUAAGUAUGAACUAAUUCAGACUGUGA...  \n",
       "2  UGUCUAAGUAUAAGCGUUAUACUGUGAAACUGCGAAUGGCUCAUUA...  \n",
       "3  AGCGUUGCAUGCCUGCAGGUCGACUCUAGAGGGGAUCCAGAUCUCC...  \n",
       "4  AACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUA...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's also check a few random samples to see how the parser handled them:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Amorphea</td>\n",
       "      <td>Amorphea</td>\n",
       "      <td>Amoebozoa</td>\n",
       "      <td>Dictyostelia</td>\n",
       "      <td>Dictyostelium</td>\n",
       "      <td>Dictyostelium dimigraformum</td>\n",
       "      <td>AM168038.1.1867</td>\n",
       "      <td>AACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8829</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Apicomplexa</td>\n",
       "      <td>Apicomplexa</td>\n",
       "      <td>Conoidasida</td>\n",
       "      <td>Gregarinasina</td>\n",
       "      <td>Eugregarinorida</td>\n",
       "      <td>uncultured eukaryote</td>\n",
       "      <td>AB275104.1.1590</td>\n",
       "      <td>AUUAAGUAAGUUUUAAUUUAUUUGGUUAUAAAAAAUUAACGGAUAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6450</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Euglenozoa</td>\n",
       "      <td>Kinetoplastea</td>\n",
       "      <td>Metakinetoplastina</td>\n",
       "      <td>Trypanosomatida</td>\n",
       "      <td>Trypanosoma</td>\n",
       "      <td>Trypanosoma wauwau</td>\n",
       "      <td>KR653211.1.2208</td>\n",
       "      <td>UGAUCUGGUUGAUUCUGCCAGUAGUCAUAUGCUUGUUUCAAGGACU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Dinoflagellata</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>Gymnodiniphycidae</td>\n",
       "      <td>Kareniaceae</td>\n",
       "      <td>Karlodinium</td>\n",
       "      <td>Karenia mikimotoi</td>\n",
       "      <td>JF791035.1.1777</td>\n",
       "      <td>ACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Dinoflagellata</td>\n",
       "      <td>Dinophyceae</td>\n",
       "      <td>Gymnodiniphycidae</td>\n",
       "      <td>Gymnodinium clade</td>\n",
       "      <td>Gymnodinium</td>\n",
       "      <td>Lepidodinium viride</td>\n",
       "      <td>JF791033.1.1777</td>\n",
       "      <td>ACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUAA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        kingdom          phylum          class               order  \\\n",
       "5303  Eukaryota        Amorphea       Amorphea           Amoebozoa   \n",
       "8829  Eukaryota     Apicomplexa    Apicomplexa         Conoidasida   \n",
       "6450  Eukaryota      Euglenozoa  Kinetoplastea  Metakinetoplastina   \n",
       "2007  Eukaryota  Dinoflagellata    Dinophyceae   Gymnodiniphycidae   \n",
       "2272  Eukaryota  Dinoflagellata    Dinophyceae   Gymnodiniphycidae   \n",
       "\n",
       "                 family            genus                      species  \\\n",
       "5303       Dictyostelia    Dictyostelium  Dictyostelium dimigraformum   \n",
       "8829      Gregarinasina  Eugregarinorida         uncultured eukaryote   \n",
       "6450    Trypanosomatida      Trypanosoma           Trypanosoma wauwau   \n",
       "2007        Kareniaceae      Karlodinium            Karenia mikimotoi   \n",
       "2272  Gymnodinium clade      Gymnodinium          Lepidodinium viride   \n",
       "\n",
       "                   id                                           sequence  \n",
       "5303  AM168038.1.1867  AACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUA...  \n",
       "8829  AB275104.1.1590  AUUAAGUAAGUUUUAAUUUAUUUGGUUAUAAAAAAUUAACGGAUAA...  \n",
       "6450  KR653211.1.2208  UGAUCUGGUUGAUUCUGCCAGUAGUCAUAUGCUUGUUUCAAGGACU...  \n",
       "2007  JF791035.1.1777  ACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUAA...  \n",
       "2272  JF791033.1.1777  ACCUGGUUGAUCCUGCCAGUAGUCAUAUGCUUGUCUCAAAGAUUAA...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A list to hold our structured data\n",
    "parsed_data = []\n",
    "\n",
    "# Define a set of non-standard, intermediate, or otherwise unhelpful ranks to discard.\n",
    "# This list is based on common patterns found in the SILVA eukaryote taxonomy.\n",
    "DISCARD_RANKS = {\n",
    "    'cellular organisms', 'Opisthokonta', 'Holozoa', 'Metazoa (Animalia)', \n",
    "    'Eumetazoa', 'Bilateria', 'Protostomia', 'Deuterostomia', 'Sar', \n",
    "    'Stramenopila', 'Alveolata', 'Rhizaria', 'Archaeplastida', 'Glaucophyta',\n",
    "    'Chloroplastida', 'Rhodophyceae', 'Streptophyta', 'Embryophyta', \n",
    "    'Tracheophyta', 'Phragmoplastophyta', 'Excavata', 'Discoba', 'Metamonada'\n",
    "}\n",
    "\n",
    "def parse_eukaryote_taxonomy(taxonomy_str):\n",
    "    \"\"\"\n",
    "    A more robust parser specifically for the complex SILVA Eukaryote taxonomy.\n",
    "    \"\"\"\n",
    "    # Start with a dictionary of empty ranks\n",
    "    parsed_ranks = {\n",
    "        'kingdom': None, 'phylum': None, 'class': None, 'order': None,\n",
    "        'family': None, 'genus': None, 'species': None\n",
    "    }\n",
    "    \n",
    "    # Split the string by ';' and convert the discard list to lowercase for case-insensitive matching\n",
    "    discard_lower = {r.lower() for r in DISCARD_RANKS}\n",
    "    \n",
    "    # Filter out the discardable ranks\n",
    "    ranks = [\n",
    "        rank.strip() for rank in taxonomy_str.split(';')\n",
    "        if rank.strip() and rank.strip().lower() not in discard_lower\n",
    "    ]\n",
    "    \n",
    "    if not ranks:\n",
    "        return parsed_ranks\n",
    "        \n",
    "    # The first item is always the kingdom\n",
    "    parsed_ranks['kingdom'] = ranks[0]\n",
    "    \n",
    "    # If there are more ranks, the last one is either the genus or species\n",
    "    if len(ranks) > 1:\n",
    "        last_item = ranks[-1]\n",
    "        # A simple heuristic: if it contains a space, it's likely a species\n",
    "        if ' ' in last_item:\n",
    "            parsed_ranks['species'] = last_item\n",
    "            if len(ranks) > 2: # Genus is the second to last item\n",
    "                parsed_ranks['genus'] = ranks[-2]\n",
    "            remaining_ranks = ranks[1:-2]\n",
    "        else: # The last item is the genus\n",
    "            parsed_ranks['genus'] = last_item\n",
    "            remaining_ranks = ranks[1:-1]\n",
    "            \n",
    "        # Assign the remaining ranks from right to left (most specific to least specific)\n",
    "        if len(remaining_ranks) > 0: parsed_ranks['family'] = remaining_ranks[-1]\n",
    "        if len(remaining_ranks) > 1: parsed_ranks['order'] = remaining_ranks[-2]\n",
    "        if len(remaining_ranks) > 2: parsed_ranks['class'] = remaining_ranks[-3]\n",
    "        if len(remaining_ranks) > 3: parsed_ranks['phylum'] = remaining_ranks[-4]\n",
    "        # A fallback for phylum if the hierarchy is short\n",
    "        elif not parsed_ranks['phylum'] and len(ranks) > 1:\n",
    "             parsed_ranks['phylum'] = ranks[1]\n",
    "\n",
    "    return parsed_ranks\n",
    "\n",
    "# --- Apply the parser to our sample data ---\n",
    "\n",
    "# Loop through our sample file records\n",
    "with open(SAMPLE_EUKARYOTE_PATH, \"r\") as handle:\n",
    "    for record in tqdm(SeqIO.parse(handle, \"fasta\"), total=SAMPLE_SIZE, desc=\"Parsing sample taxonomy\"):\n",
    "        # The taxonomy string is the part of the description after the first space\n",
    "        accession, taxonomy_str = record.description.split(' ', 1)\n",
    "        \n",
    "        # Parse the taxonomy string using our new, robust function\n",
    "        taxonomy_dict = parse_eukaryote_taxonomy(taxonomy_str)\n",
    "        \n",
    "        # Also store the sequence and its ID\n",
    "        taxonomy_dict['id'] = record.id\n",
    "        taxonomy_dict['sequence'] = str(record.seq)\n",
    "        \n",
    "        parsed_data.append(taxonomy_dict)\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(f\"\\n✅ Parsing complete. Created a DataFrame with {len(df)} rows.\")\n",
    "print(\"Here's a preview of the structured data:\")\n",
    "display(df.head()) # Use display() for better formatting in Jupyter\n",
    "\n",
    "print(\"\\nLet's also check a few random samples to see how the parser handled them:\")\n",
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f31811-e441-455d-abd5-2caeb422fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Cleaning and filtering data ---\n",
      "Removed 2 rows with missing 'genus' labels.\n",
      "Removed 1964 rows for rare genera (less than 3 members).\n",
      "Final DataFrame has 8034 sequences.\n",
      "---------------------------------------------\n",
      "--- Step 5: Engineering 6-mer features ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abcff472a494c9db9708485b59adc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating k-mers:   0%|          | 0/8034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "--- Step 6: Vectorizing data ---\n",
      "Feature matrix shape: (8034, 14058)\n",
      "Label vector shape:   (8034,)\n",
      "---------------------------------------------\n",
      "--- Step 7: Splitting data into training/testing sets ---\n",
      "Training set shape: (6427, 14058)\n",
      "Testing set shape:  (1607, 14058)\n",
      "---------------------------------------------\n",
      "--- Step 8: Saving all 18S artifacts to disk ---\n",
      "✅ All artifacts saved successfully.\n",
      "\n",
      "--- 18S DATA PREPARATION COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import save_npz\n",
    "import pickle\n",
    "\n",
    "# --- Configuration for this phase ---\n",
    "TARGET_RANK = 'genus'\n",
    "MIN_CLASS_MEMBERS = 3\n",
    "KMER_SIZE = 6 # A standard k-mer size for the 18S gene\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Step 4: Clean and Filter the DataFrame ---\n",
    "print(\"--- Step 4: Cleaning and filtering data ---\")\n",
    "initial_rows = len(df)\n",
    "df_cleaned = df.dropna(subset=[TARGET_RANK]).copy()\n",
    "print(f\"Removed {initial_rows - len(df_cleaned)} rows with missing '{TARGET_RANK}' labels.\")\n",
    "\n",
    "class_counts = df_cleaned[TARGET_RANK].value_counts()\n",
    "classes_to_keep = class_counts[class_counts >= MIN_CLASS_MEMBERS].index\n",
    "df_filtered = df_cleaned[df_cleaned[TARGET_RANK].isin(classes_to_keep)].copy()\n",
    "print(f\"Removed {len(df_cleaned) - len(df_filtered)} rows for rare genera (less than {MIN_CLASS_MEMBERS} members).\")\n",
    "print(f\"Final DataFrame has {len(df_filtered)} sequences.\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 5: Feature Engineering (K-mer Counting) ---\n",
    "print(f\"--- Step 5: Engineering {KMER_SIZE}-mer features ---\")\n",
    "def get_kmer_counts(sequence, k):\n",
    "    counts = Counter()\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        if \"N\" not in kmer.upper():\n",
    "            counts[kmer] += 1\n",
    "    return dict(counts)\n",
    "\n",
    "df_filtered['kmer_counts'] = list(tqdm((get_kmer_counts(seq, KMER_SIZE) for seq in df_filtered['sequence']), total=len(df_filtered), desc=\"Calculating k-mers\"))\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 6: Vectorize Features and Labels ---\n",
    "print(\"--- Step 6: Vectorizing data ---\")\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X = vectorizer.fit_transform(df_filtered['kmer_counts'])\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_RANK])\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label vector shape:   {y.shape}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 7: Split Data ---\n",
    "print(\"--- Step 7: Splitting data into training/testing sets ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape:  {X_test.shape}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 8: Save All Processed Artifacts ---\n",
    "print(\"--- Step 8: Saving all 18S artifacts to disk ---\")\n",
    "save_npz(PROCESSED_DATA_DIR / \"X_train_18s.npz\", X_train)\n",
    "save_npz(PROCESSED_DATA_DIR / \"X_test_18s.npz\", X_test)\n",
    "np.save(PROCESSED_DATA_DIR / \"y_train_18s.npy\", y_train)\n",
    "np.save(PROCESSED_DATA_DIR / \"y_test_18s.npy\", y_test)\n",
    "with open(MODELS_DIR / \"18s_genus_vectorizer.pkl\", 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(MODELS_DIR / \"18s_genus_label_encoder.pkl\", 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"✅ All artifacts saved successfully.\")\n",
    "print(\"\\n--- 18S DATA PREPARATION COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cae9e-ef23-40a0-89f0-a4877da120db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8477e711-f561-415a-a736-dffd202354b8",
   "metadata": {},
   "source": [
    "# 18S Model Training and Evaluation\n",
    "\n",
    "**Objective:** To build, train, and evaluate a deep learning classifier for the 18S rRNA gene (Eukaryotes) using the pre-processed data.\n",
    "\n",
    "**Methodology:**\n",
    "1. Load the 18S-specific training/testing data and encoders from disk.\n",
    "2. Define the neural network architecture.\n",
    "3. Train the model on the training data using the GPU.\n",
    "4. Save, reload, and evaluate the final model's accuracy on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbd40b8-8e82-4b8c-865d-4e96c71c939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Setup ---\n",
      "TensorFlow Version: 2.10.1\n",
      "GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set up project path\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# --- Verification Step: Check for GPU ---\n",
    "print(\"--- TensorFlow Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a70ef60-91ba-4250-b5f5-4bf149d984dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading 18S Data ---\n",
      "Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "Shape of X_train: (6427, 14058)\n",
      "Shape of y_train: (6427,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1607, 14058)\n",
      "Shape of y_test:  (1607,)\n",
      "Number of classes (genera): 616\n"
     ]
    }
   ],
   "source": [
    "# --- Define 18S-specific file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_18s.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_18s.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_18s.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_18s.npy\"\n",
    "\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"18s_genus_label_encoder.pkl\"\n",
    "\n",
    "# --- Load the data and encoders ---\n",
    "print(\"--- Loading 18S Data ---\")\n",
    "X_train = load_npz(X_TRAIN_PATH)\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_train = np.load(Y_TRAIN_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "print(\"Data loading complete.\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f40cca7d-9520-4e9c-be72-3f37c1ca92da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining 18S Model Architecture ---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              28792832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 616)               631400    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,522,408\n",
      "Trainable params: 31,522,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--- Preparing Data and Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 2048), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Loss: 2.4823 | Acc: 50.31% [██████████··········] | Val_Loss: 1.5871 | Val_Acc: 66.25% [█████████████·······]Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 19/50 | Loss: 2.4734 | Acc: 50.67% [██████████··········] | Val_Loss: 1.6282 | Val_Acc: 68.58% [█████████████·······]Epoch 19: early stopping\n",
      "\n",
      "\n",
      "--- Training complete. ---\n",
      "\n",
      "Saving trained model to: C:\\Users\\jampa\\Music\\atlas\\models\\18s_genus_classifier.keras\n",
      "✅ Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- PART 1: Define and Compile Model ---\n",
    "print(\"--- Defining 18S Model Architecture ---\")\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2048, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- PART 2: Prepare Data and Train ---\n",
    "print(\"\\n--- Preparing Data and Starting Training ---\")\n",
    "\n",
    "# Pre-flight check for singletons in the training set\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "if np.min(counts) < 2:\n",
    "    print(\"WARNING: Singletons found in y_train. Cleaning...\")\n",
    "    non_singleton_indices = np.where(~np.isin(y_train, unique[counts < 2]))[0]\n",
    "    X_train = X_train[non_singleton_indices]\n",
    "    y_train = y_train[non_singleton_indices]\n",
    "\n",
    "# Create validation set\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Define custom callback\n",
    "class TrainingProgressCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = logs.get('accuracy', 0); val_acc = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0); val_loss = logs.get('val_loss', 0)\n",
    "        acc_bar = '█' * int(acc * 20) + '·' * (20 - int(acc * 20))\n",
    "        val_acc_bar = '█' * int(val_acc * 20) + '·' * (20 - int(val_acc * 20))\n",
    "        print(f\"\\rEpoch {epoch+1:02d}/50 | Loss: {loss:.4f} | Acc: {acc:.2%} [{acc_bar}] | Val_Loss: {val_loss:.4f} | Val_Acc: {val_acc:.2%} [{val_acc_bar}]\", end='')\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Start training\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping, TrainingProgressCallback()]\n",
    ")\n",
    "print(\"\\n\\n--- Training complete. ---\")\n",
    "\n",
    "\n",
    "# --- PART 3: Save the Model ---\n",
    "MODEL_PATH = MODELS_DIR / \"18s_genus_classifier.keras\"\n",
    "print(f\"\\nSaving trained model to: {MODEL_PATH}\")\n",
    "model.save(MODEL_PATH)\n",
    "print(\"✅ Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3e89ec-8ed7-4945-b0f8-bb8d31b37aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Evaluation ---\n",
      "Clearing TensorFlow session...\n",
      "Memory cleared successfully.\n",
      "\n",
      "Loading model from: C:\\Users\\jampa\\Music\\atlas\\models\\18s_genus_classifier.keras\n",
      "Model loaded successfully.\n",
      "\n",
      "Loading test data...\n",
      "Test data loaded successfully.\n",
      "\n",
      "Evaluating model on the test set...\n",
      "51/51 [==============================] - 2s 19ms/step - loss: 1.9340 - accuracy: 0.6409\n",
      "\n",
      "--- Final 18S Model Evaluation ---\n",
      "Test Set Loss:     1.9340\n",
      "Test Set Accuracy: 64.09%\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: FINAL MODEL EVALUATION\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To perform a definitive, unbiased evaluation of the trained 18S model on\n",
    "#   the unseen test set. This is a memory-safe operation.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "\n",
    "# --- Define all necessary file paths ---\n",
    "project_root = Path.cwd().parent\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"18s_genus_classifier.keras\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_18s.npz\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_18s.npy\"\n",
    "\n",
    "# --- 1. Clean up memory ---\n",
    "print(\"--- Starting Final Evaluation ---\")\n",
    "print(\"Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "print(\"Memory cleared successfully.\")\n",
    "\n",
    "# --- 2. Load model and test data ---\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "loaded_model = load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "print(f\"\\nLoading test data...\")\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "print(\"Test data loaded successfully.\")\n",
    "\n",
    "# --- 3. Evaluate the model ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\n--- Final 18S Model Evaluation ---\")\n",
    "print(f\"Test Set Loss:     {loss:.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy:.2%}\")\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d720866-c67d-4663-8af1-c05ddef3596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Training History Plots ---\n",
      "\n",
      "Could not generate plots because the 'history' object was not found in memory.\n",
      "This is expected if the kernel was restarted after training.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: VISUALIZE TRAINING HISTORY (Optional)\n",
    "# =============================================================================\n",
    "#\n",
    "# NOTE: This cell will only work if the kernel has NOT been restarted since\n",
    "# the training cell was run, as it depends on the 'history' object in memory.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    print(\"--- Generating Training History Plots ---\")\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle('18S Model Training History', fontsize=18)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1.plot(history_df.index + 1, history_df['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history_df.index + 1, history_df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "    ax1.set_title('Model Accuracy Over Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--')\n",
    "\n",
    "    # Plot Loss\n",
    "    ax2.plot(history_df.index + 1, history_df['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history_df.index + 1, history_df['val_loss'], label='Validation Loss', marker='o')\n",
    "    ax2.set_title('Model Loss Over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nCould not generate plots because the 'history' object was not found in memory.\")\n",
    "    print(\"This is expected if the kernel was restarted after training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b80f0-d19a-41ab-8003-b624bf0313e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0cb662-8edb-4e6b-aeae-a2b7fd00132c",
   "metadata": {},
   "source": [
    "# COI Data Preparation: Refinement and Development\n",
    "\n",
    "**Objective:** Refine the data preparation pipeline for the COI gene, the primary barcode for animal identification, using the BOLD database.\n",
    "\n",
    "**Methodology:**\n",
    "1.  Work from a small, manageable sample of the full BOLD database.\n",
    "2.  Develop a new taxonomy parser specifically designed for the pipe-separated (`|`) format of BOLD FASTA headers.\n",
    "3.  Apply the full data cleaning and feature engineering workflow.\n",
    "4.  Save the final, uniquely named artifacts for the COI pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9544b2c0-fdfc-487e-ba7f-1da3bb40981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Set up project path\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(f\"Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d4c780-3ee8-4b26-b585-7c4090a5b1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source BOLD database found.\n",
      "  - Location: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\BOLD_Public.29-Aug-2025.fasta\n"
     ]
    }
   ],
   "source": [
    "# --- Define Core Directories ---\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Define COI Specific File Paths ---\n",
    "\n",
    "# --- FIX: Use the exact filename of the downloaded BOLD database ---\n",
    "# Path to the full, original BOLD database file\n",
    "FULL_BOLD_PATH = RAW_DATA_DIR / \"BOLD_Public.29-Aug-2025.fasta\"\n",
    "\n",
    "# Path to the small sample file we will create for development\n",
    "SAMPLE_BOLD_PATH = RAW_DATA_DIR / \"BOLD_sample_10k.fasta\"\n",
    "\n",
    "# --- Verification Step ---\n",
    "if FULL_BOLD_PATH.exists():\n",
    "    print(\"Source BOLD database found.\")\n",
    "    print(f\"  - Location: {FULL_BOLD_PATH}\")\n",
    "else:\n",
    "    print(f\"ERROR: The source BOLD database was not found at the expected location.\")\n",
    "    print(f\"  - Expected: {FULL_BOLD_PATH}\")\n",
    "    print(\"Please ensure the file is downloaded and correctly named in the 'data/raw' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99a1dfd-91b0-49af-910a-9b2b27cb504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sample of 10000 sequences from the BOLD database...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb02e44bfa684ccf973165c15cb008d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling BOLD DB:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully created sample file with 10000 sequences.\n",
      "   Location: C:\\Users\\jampa\\Music\\atlas\\data\\raw\\BOLD_sample_10k.fasta\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: CREATE A DEVELOPMENT SAMPLE FROM BOLD DATA\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To create a smaller 10,000-sequence sample from the full BOLD database.\n",
    "#\n",
    "# RATIONALE:\n",
    "#   Working with a smaller sample allows for rapid, interactive development\n",
    "#   and debugging of the subsequent parsing and cleaning steps without the\n",
    "#   long processing times required for the full dataset. This script is\n",
    "#   designed to be run only once; if the sample file is found, this\n",
    "#   step will be skipped.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "# --- Main Logic ---\n",
    "# This check prevents us from re-running this process unnecessarily.\n",
    "if not SAMPLE_BOLD_PATH.exists():\n",
    "    print(f\"Creating a sample of {SAMPLE_SIZE} sequences from the BOLD database...\")\n",
    "    \n",
    "    sample_records = []\n",
    "    # Open the full BOLD database file for reading\n",
    "    with open(FULL_BOLD_PATH, \"r\") as handle_in:\n",
    "        # Create an efficient iterator over the FASTA records\n",
    "        records_iterator = (record for record in SeqIO.parse(handle_in, \"fasta\"))\n",
    "        \n",
    "        # Use tqdm to show a progress bar while iterating\n",
    "        for i, record in tqdm(enumerate(records_iterator), total=SAMPLE_SIZE, desc=\"Sampling BOLD DB\"):\n",
    "            # Stop once we have collected the desired number of samples\n",
    "            if i >= SAMPLE_SIZE:\n",
    "                break\n",
    "            sample_records.append(record)\n",
    "            \n",
    "    # Write the collected records to our new sample file\n",
    "    with open(SAMPLE_BOLD_PATH, \"w\") as handle_out:\n",
    "        SeqIO.write(sample_records, handle_out, \"fasta\")\n",
    "        \n",
    "    print(f\"\\nSuccessfully created sample file with {len(sample_records)} sequences.\")\n",
    "    print(f\"   Location: {SAMPLE_BOLD_PATH}\")\n",
    "else:\n",
    "    print(f\"BOLD sample file already exists. No action needed.\")\n",
    "    print(f\"   Location: {SAMPLE_BOLD_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad3abc1-ff3f-48ec-9e4a-58b63b5e7215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying revised BOLD taxonomy parser (v2) to sample data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bd3595045e43a2b8d5604bbd9cb68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing BOLD headers:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing complete. Created a DataFrame with 10000 rows.\n",
      "\n",
      "Preview of the structured data (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Lepidoptera</td>\n",
       "      <td>Geometridae</td>\n",
       "      <td>Oenochrominae</td>\n",
       "      <td>Arhodia</td>\n",
       "      <td>AANIC003-10|COI-5P|Australia|Animalia,Arthropo...</td>\n",
       "      <td>AACATTATATTTTATTTTTGGTATTTGAGCTGGTATAATTGGAACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Lepidoptera</td>\n",
       "      <td>Geometridae</td>\n",
       "      <td>Oenochrominae</td>\n",
       "      <td>Arhodia</td>\n",
       "      <td>AANIC101-10|COI-5P|Australia|Animalia,Arthropo...</td>\n",
       "      <td>AACATTATATTTTATTTTTGGTATTTGAGCTGGTATAATTGGAACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Lepidoptera</td>\n",
       "      <td>Geometridae</td>\n",
       "      <td>Oenochrominae</td>\n",
       "      <td>Arhodia</td>\n",
       "      <td>AANIC102-10|COI-5P|Australia|Animalia,Arthropo...</td>\n",
       "      <td>AACATTATATTTTATTTTTGGTATTTGAGCTGGTATAATTGGAACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Lepidoptera</td>\n",
       "      <td>Geometridae</td>\n",
       "      <td>Oenochrominae</td>\n",
       "      <td>Arhodia</td>\n",
       "      <td>AANIC104-10|COI-5P|Australia|Animalia,Arthropo...</td>\n",
       "      <td>AACATTATATTTTATTTTTGGTATTTGAGCAGGTATAATTGGAACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Lepidoptera</td>\n",
       "      <td>Geometridae</td>\n",
       "      <td>Oenochrominae</td>\n",
       "      <td>Arhodia</td>\n",
       "      <td>AANIC105-10|COI-5P|Australia|Animalia,Arthropo...</td>\n",
       "      <td>AACATTATATTTTATTTTTGGTATTTGAGCAGGTATAATTGGAACT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kingdom      phylum    class        order       family          genus  \\\n",
       "0  Animalia  Arthropoda  Insecta  Lepidoptera  Geometridae  Oenochrominae   \n",
       "1  Animalia  Arthropoda  Insecta  Lepidoptera  Geometridae  Oenochrominae   \n",
       "2  Animalia  Arthropoda  Insecta  Lepidoptera  Geometridae  Oenochrominae   \n",
       "3  Animalia  Arthropoda  Insecta  Lepidoptera  Geometridae  Oenochrominae   \n",
       "4  Animalia  Arthropoda  Insecta  Lepidoptera  Geometridae  Oenochrominae   \n",
       "\n",
       "   species                                                 id  \\\n",
       "0  Arhodia  AANIC003-10|COI-5P|Australia|Animalia,Arthropo...   \n",
       "1  Arhodia  AANIC101-10|COI-5P|Australia|Animalia,Arthropo...   \n",
       "2  Arhodia  AANIC102-10|COI-5P|Australia|Animalia,Arthropo...   \n",
       "3  Arhodia  AANIC104-10|COI-5P|Australia|Animalia,Arthropo...   \n",
       "4  Arhodia  AANIC105-10|COI-5P|Australia|Animalia,Arthropo...   \n",
       "\n",
       "                                            sequence  \n",
       "0  AACATTATATTTTATTTTTGGTATTTGAGCTGGTATAATTGGAACT...  \n",
       "1  AACATTATATTTTATTTTTGGTATTTGAGCTGGTATAATTGGAACT...  \n",
       "2  AACATTATATTTTATTTTTGGTATTTGAGCTGGTATAATTGGAACT...  \n",
       "3  AACATTATATTTTATTTTTGGTATTTGAGCAGGTATAATTGGAACT...  \n",
       "4  AACATTATATTTTATTTTTGGTATTTGAGCAGGTATAATTGGAACT...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview of the structured data (5 random rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Dolichopodidae</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AACTA6487-20|COI-5P|Australia|Animalia,Arthrop...</td>\n",
       "      <td>CATTATATTTTATTTTTGGAGCATGAGCAGGTATAGTTGGTACATC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Hymenoptera</td>\n",
       "      <td>Thynnidae</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AACTA4384-20|COI-5P|Australia|Animalia,Arthrop...</td>\n",
       "      <td>CTTTATTTTATTTTCGCTATTTGAGCTGGATTAATAGGAACATCAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8288</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Phoridae</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AACTA6591-20|COI-5P|Australia|Animalia,Arthrop...</td>\n",
       "      <td>ATTATATTTTATTTTTGGTGCTTGAGCAGGAATAGTAGGAACTTCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5337</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Diptera</td>\n",
       "      <td>Hybotidae</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AACTA3416-20|COI-5P|Australia|Animalia,Arthrop...</td>\n",
       "      <td>TCTTTATTTTATATTTGGTGCCTGAGCCGGAATAGTAGGAACATCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6459</th>\n",
       "      <td>Animalia</td>\n",
       "      <td>Arthropoda</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>Hymenoptera</td>\n",
       "      <td>Bethylidae</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AACTA4703-20|COI-5P|Australia|Animalia,Arthrop...</td>\n",
       "      <td>ATATTATATTTTGTATTTAGAATATGATCAGGTATAATTGGATCAT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       kingdom      phylum    class        order          family genus  \\\n",
       "8283  Animalia  Arthropoda  Insecta      Diptera  Dolichopodidae  None   \n",
       "6151  Animalia  Arthropoda  Insecta  Hymenoptera       Thynnidae  None   \n",
       "8288  Animalia  Arthropoda  Insecta      Diptera        Phoridae  None   \n",
       "5337  Animalia  Arthropoda  Insecta      Diptera       Hybotidae  None   \n",
       "6459  Animalia  Arthropoda  Insecta  Hymenoptera      Bethylidae  None   \n",
       "\n",
       "     species                                                 id  \\\n",
       "8283    None  AACTA6487-20|COI-5P|Australia|Animalia,Arthrop...   \n",
       "6151    None  AACTA4384-20|COI-5P|Australia|Animalia,Arthrop...   \n",
       "8288    None  AACTA6591-20|COI-5P|Australia|Animalia,Arthrop...   \n",
       "5337    None  AACTA3416-20|COI-5P|Australia|Animalia,Arthrop...   \n",
       "6459    None  AACTA4703-20|COI-5P|Australia|Animalia,Arthrop...   \n",
       "\n",
       "                                               sequence  \n",
       "8283  CATTATATTTTATTTTTGGAGCATGAGCAGGTATAGTTGGTACATC...  \n",
       "6151  CTTTATTTTATTTTCGCTATTTGAGCTGGATTAATAGGAACATCAA...  \n",
       "8288  ATTATATTTTATTTTTGGTGCTTGAGCAGGAATAGTAGGAACTTCA...  \n",
       "5337  TCTTTATTTTATATTTGGTGCCTGAGCCGGAATAGTAGGAACATCA...  \n",
       "6459  ATATTATATTTTGTATTTAGAATATGATCAGGTATAATTGGATCAT...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 2 (REVISED): PARSE TAXONOMY FROM BOLD SAMPLE DATA\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To use a new, more robust parser to correctly handle the complex and\n",
    "#   inconsistent BOLD database FASTA header format.\n",
    "#\n",
    "# RATIONALE:\n",
    "#   Initial analysis revealed the taxonomy is often a comma-separated string\n",
    "#   within a larger pipe-separated header. This new v2 parser is designed\n",
    "#   to specifically find and process this nested format, ensuring accurate\n",
    "#   extraction of taxonomic ranks.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- A list to hold our structured data ---\n",
    "parsed_data = []\n",
    "\n",
    "# --- Define the new, more robust BOLD-specific parsing function ---\n",
    "def parse_bold_taxonomy_v2(description):\n",
    "    \"\"\"\n",
    "    Parses the complex, pipe-and-comma-separated BOLD FASTA header.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary with all ranks set to None\n",
    "    parsed_ranks = {\n",
    "        'kingdom': None, 'phylum': None, 'class': None, 'order': None,\n",
    "        'family': None, 'genus': None, 'species': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # First, split the entire description by the pipe character\n",
    "        parts = description.split('|')\n",
    "        \n",
    "        # Now, find the part that actually contains the taxonomy.\n",
    "        # We'll assume it's the part with commas and \"Animalia\".\n",
    "        taxonomy_str = \"\"\n",
    "        for part in parts:\n",
    "            if ',' in part and 'Animalia' in part:\n",
    "                taxonomy_str = part\n",
    "                break\n",
    "        \n",
    "        # If we found a valid taxonomy string, process it\n",
    "        if taxonomy_str:\n",
    "            ranks = taxonomy_str.split(',')\n",
    "            \n",
    "            # Assign ranks based on their position in the comma-separated list\n",
    "            if len(ranks) > 0: parsed_ranks['kingdom'] = ranks[0]\n",
    "            if len(ranks) > 1: parsed_ranks['phylum'] = ranks[1]\n",
    "            if len(ranks) > 2: parsed_ranks['class'] = ranks[2]\n",
    "            if len(ranks) > 3: parsed_ranks['order'] = ranks[3]\n",
    "            if len(ranks) > 4: parsed_ranks['family'] = ranks[4]\n",
    "            if len(ranks) > 5: parsed_ranks['genus'] = ranks[5]\n",
    "            if len(ranks) > 6: parsed_ranks['species'] = ranks[6]\n",
    "\n",
    "    except Exception:\n",
    "        # Pass silently if any unexpected error occurs\n",
    "        pass\n",
    "        \n",
    "    return parsed_ranks\n",
    "\n",
    "# --- Apply the new parser to our sample data ---\n",
    "print(\"Applying revised BOLD taxonomy parser (v2) to sample data...\")\n",
    "\n",
    "# Loop through the records in our sample file\n",
    "with open(SAMPLE_BOLD_PATH, \"r\") as handle:\n",
    "    for record in tqdm(SeqIO.parse(handle, \"fasta\"), total=SAMPLE_SIZE, desc=\"Parsing BOLD headers\"):\n",
    "        # The entire description line contains the taxonomy\n",
    "        taxonomy_dict = parse_bold_taxonomy_v2(record.description)\n",
    "        \n",
    "        # Store the essential sequence information\n",
    "        taxonomy_dict['id'] = record.id\n",
    "        taxonomy_dict['sequence'] = str(record.seq)\n",
    "        \n",
    "        parsed_data.append(taxonomy_dict)\n",
    "\n",
    "# --- Create and Verify the DataFrame ---\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "\n",
    "print(f\"\\nParsing complete. Created a DataFrame with {len(df)} rows.\")\n",
    "\n",
    "# Display the first 5 rows for a preliminary check\n",
    "print(\"\\nPreview of the structured data (first 5 rows):\")\n",
    "display(df.head())\n",
    "\n",
    "# Display 5 random rows to check for consistency across the dataset\n",
    "print(\"\\nPreview of the structured data (5 random rows):\")\n",
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565b8015-1377-41a5-9c25-c636c2407221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Cleaning and filtering data ---\n",
      "Removed 0 rows with missing 'genus' labels.\n",
      "Removed 104 rows for rare genera (less than 3 members).\n",
      "Final DataFrame has 9896 sequences.\n",
      "---------------------------------------------\n",
      "--- Step 4: Engineering 8-mer features ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f8122c16064aa7b7042094bd1a7f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating k-mers:   0%|          | 0/9896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "--- Step 5: Vectorizing data ---\n",
      "Feature matrix shape: (9896, 41040)\n",
      "Label vector shape:   (9896,)\n",
      "---------------------------------------------\n",
      "--- Step 6: Splitting data into training/testing sets ---\n",
      "Training set shape: (7916, 41040)\n",
      "Testing set shape:  (1980, 41040)\n",
      "---------------------------------------------\n",
      "--- Step 7: Saving all COI artifacts to disk ---\n",
      "All artifacts saved successfully.\n",
      "\n",
      "--- COI DATA PREPARATION COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEPS 3-7: DATA CLEANING, FEATURE ENGINEERING, AND SAVING\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To apply the full, standardized data preparation workflow to the parsed\n",
    "#   BOLD (COI) data and save the final, model-ready artifacts to disk.\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   1. Clean Data: Remove rows with missing target labels ('genus') and\n",
    "#      filter out rare genera with fewer than 3 members to ensure data quality.\n",
    "#   2. Engineer Features: Calculate k-mer counts for each sequence. A k-mer\n",
    "#      size of 8 is chosen, as a larger k-mer can be more effective for\n",
    "#      distinguishing protein-coding genes like COI.\n",
    "#   3. Vectorize: Convert the k-mer counts and text labels into numerical\n",
    "#      matrices suitable for machine learning.\n",
    "#   4. Split Data: Partition the dataset into training (80%) and testing (20%) sets.\n",
    "#   5. Save Artifacts: Save all processed data and encoders to disk with\n",
    "#      unique 'coi' filenames.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports for this combined cell ---\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import save_npz\n",
    "import pickle\n",
    "\n",
    "# --- Configuration for this phase ---\n",
    "TARGET_RANK = 'genus'\n",
    "MIN_CLASS_MEMBERS = 3\n",
    "KMER_SIZE = 8 # Using a larger k-mer for the protein-coding COI gene\n",
    "TEST_SPLIT_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Step 3: Clean and Filter the DataFrame ---\n",
    "print(\"--- Step 3: Cleaning and filtering data ---\")\n",
    "initial_rows = len(df)\n",
    "df_cleaned = df.dropna(subset=[TARGET_RANK]).copy()\n",
    "print(f\"Removed {initial_rows - len(df_cleaned)} rows with missing '{TARGET_RANK}' labels.\")\n",
    "\n",
    "class_counts = df_cleaned[TARGET_RANK].value_counts()\n",
    "classes_to_keep = class_counts[class_counts >= MIN_CLASS_MEMBERS].index\n",
    "df_filtered = df_cleaned[df_cleaned[TARGET_RANK].isin(classes_to_keep)].copy()\n",
    "print(f\"Removed {len(df_cleaned) - len(df_filtered)} rows for rare genera (less than {MIN_CLASS_MEMBERS} members).\")\n",
    "print(f\"Final DataFrame has {len(df_filtered)} sequences.\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 4: Feature Engineering (K-mer Counting) ---\n",
    "print(f\"--- Step 4: Engineering {KMER_SIZE}-mer features ---\")\n",
    "def get_kmer_counts(sequence, k):\n",
    "    counts = Counter()\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        if \"N\" not in kmer.upper():\n",
    "            counts[kmer] += 1\n",
    "    return dict(counts)\n",
    "\n",
    "df_filtered['kmer_counts'] = list(tqdm((get_kmer_counts(seq, KMER_SIZE) for seq in df_filtered['sequence']), total=len(df_filtered), desc=\"Calculating k-mers\"))\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 5: Vectorize Features and Labels ---\n",
    "print(\"--- Step 5: Vectorizing data ---\")\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X = vectorizer.fit_transform(df_filtered['kmer_counts'])\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_filtered[TARGET_RANK])\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label vector shape:   {y.shape}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 6: Split Data ---\n",
    "print(\"--- Step 6: Splitting data into training/testing sets ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape:  {X_test.shape}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "\n",
    "# --- Step 7: Save All Processed Artifacts ---\n",
    "print(\"--- Step 7: Saving all COI artifacts to disk ---\")\n",
    "save_npz(PROCESSED_DATA_DIR / \"X_train_coi.npz\", X_train)\n",
    "save_npz(PROCESSED_DATA_DIR / \"X_test_coi.npz\", X_test)\n",
    "np.save(PROCESSED_DATA_DIR / \"y_train_coi.npy\", y_train)\n",
    "np.save(PROCESSED_DATA_DIR / \"y_test_coi.npy\", y_test)\n",
    "with open(MODELS_DIR / \"coi_genus_vectorizer.pkl\", 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(MODELS_DIR / \"coi_genus_label_encoder.pkl\", 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"All artifacts saved successfully.\")\n",
    "print(\"\\n--- COI DATA PREPARATION COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3bd41-b51e-48f3-bbf9-42b25ba38158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f280c0-56a6-41da-842a-c686152dae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TensorFlow Setup ---\n",
      "TensorFlow Version: 2.10.1\n",
      "GPU detected: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "--------------------------\n",
      "\n",
      "--- Loading COI Data ---\n",
      "Data loading complete.\n",
      "\n",
      "--- Loaded Data Shapes ---\n",
      "Shape of X_train: (7916, 41040)\n",
      "Shape of y_train: (7916,)\n",
      "------------------------------\n",
      "Shape of X_test:  (1980, 41040)\n",
      "Shape of y_test:  (1980,)\n",
      "Number of classes (genera): 111\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ATLAS v3 - COI PIPELINE - SETUP AND DATA LOADING\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To set up the environment for COI model training and load all\n",
    "#   pre-processed data artifacts from disk.\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   1.  Import all necessary libraries.\n",
    "#   2.  Set up the project's root path.\n",
    "#   3.  Verify that TensorFlow can detect and utilize the GPU.\n",
    "#   4.  Define the file paths for the COI-specific artifacts.\n",
    "#   5.  Load the training data, testing data, and the label encoder.\n",
    "#   6.  Print a summary of the loaded data shapes for verification.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "# --- 1. Verification Step: Check for GPU ---\n",
    "print(\"--- TensorFlow Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU detected: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "# --- 2. Define COI-specific file paths ---\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "\n",
    "X_TRAIN_PATH = PROCESSED_DATA_DIR / \"X_train_coi.npz\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_coi.npz\"\n",
    "Y_TRAIN_PATH = PROCESSED_DATA_DIR / \"y_train_coi.npy\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_coi.npy\"\n",
    "\n",
    "LABEL_ENCODER_PATH = MODELS_DIR / \"coi_genus_label_encoder.pkl\"\n",
    "\n",
    "# --- 3. Load the data and encoders ---\n",
    "print(\"\\n--- Loading COI Data ---\")\n",
    "X_train = load_npz(X_TRAIN_PATH)\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_train = np.load(Y_TRAIN_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "print(\"Data loading complete.\")\n",
    "\n",
    "# --- 4. Verification Step ---\n",
    "print(\"\\n--- Loaded Data Shapes ---\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Shape of X_test:  {X_test.shape}\")\n",
    "print(f\"Shape of y_test:  {y_test.shape}\")\n",
    "print(f\"Number of classes (genera): {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20e78e6-7ec6-4c58-bda2-8a46d25826e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining COI Model Architecture ---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              84051968  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 111)               113775    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,263,919\n",
      "Trainable params: 86,263,919\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--- Preparing Data and Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jampa\\.conda\\envs\\atlas\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 2048), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08/50 | Loss: 0.3441 | Acc: 95.79% [███████████████████·] | Val_Loss: 0.3090 | Val_Acc: 95.45% [███████████████████·]Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 09/50 | Loss: 0.4052 | Acc: 95.70% [███████████████████·] | Val_Loss: 0.3046 | Val_Acc: 96.59% [███████████████████·]Epoch 9: early stopping\n",
      "\n",
      "\n",
      "--- Training complete. ---\n",
      "\n",
      "Saving trained model to: C:\\Users\\jampa\\Music\\atlas\\models\\coi_genus_classifier.keras\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: DEFINE AND TRAIN THE COI MODEL\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To define the neural network architecture, train it on the prepared COI\n",
    "#   data, and save the resulting model artifact.\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   1.  Define the Keras Sequential model architecture, dynamically sized to\n",
    "#       the input features and output classes of the COI dataset.\n",
    "#   2.  Perform a pre-flight check on the training data to ensure there are no\n",
    "#       \"singleton\" classes, which would cause errors during validation splitting.\n",
    "#   3.  Instantiate the custom callback for clean, single-line training progress.\n",
    "#   4.  Execute the training using `model.fit()`, with an EarlyStopping\n",
    "#       callback to prevent overfitting and save time.\n",
    "#   5.  Immediately save the best version of the trained model to disk.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports for this cell ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- PART 1: Define and Compile Model ---\n",
    "print(\"--- Defining COI Model Architecture ---\")\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2048, activation='relu', input_shape=(input_shape,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- PART 2: Prepare Data and Train ---\n",
    "print(\"\\n--- Preparing Data and Starting Training ---\")\n",
    "\n",
    "# Pre-flight check for singletons in the training set\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "if np.min(counts) < 2:\n",
    "    print(\"WARNING: Singletons found in y_train. Cleaning...\")\n",
    "    non_singleton_indices = np.where(~np.isin(y_train, unique[counts < 2]))[0]\n",
    "    X_train = X_train[non_singleton_indices]\n",
    "    y_train = y_train[non_singleton_indices]\n",
    "\n",
    "# Create validation set\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Define custom callback\n",
    "class TrainingProgressCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = logs.get('accuracy', 0); val_acc = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0); val_loss = logs.get('val_loss', 0)\n",
    "        acc_bar = '█' * int(acc * 20) + '·' * (20 - int(acc * 20))\n",
    "        val_acc_bar = '█' * int(val_acc * 20) + '·' * (20 - int(val_acc * 20))\n",
    "        print(f\"\\rEpoch {epoch+1:02d}/50 | Loss: {loss:.4f} | Acc: {acc:.2%} [{acc_bar}] | Val_Loss: {val_loss:.4f} | Val_Acc: {val_acc:.2%} [{val_acc_bar}]\", end='')\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Start training\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping, TrainingProgressCallback()]\n",
    ")\n",
    "print(\"\\n\\n--- Training complete. ---\")\n",
    "\n",
    "\n",
    "# --- PART 3: Save the Model ---\n",
    "MODEL_PATH = MODELS_DIR / \"coi_genus_classifier.keras\"\n",
    "print(f\"\\nSaving trained model to: {MODEL_PATH}\")\n",
    "model.save(MODEL_PATH)\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe515a98-57e7-4daf-b641-1a43be2d813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Post-Training Workflow ---\n",
      "Clearing TensorFlow session and running garbage collection...\n",
      "Memory cleared successfully.\n",
      "\n",
      "Loading model from: C:\\Users\\jampa\\Music\\atlas\\models\\coi_genus_classifier.keras\n",
      "Model loaded successfully.\n",
      "\n",
      "Loading test data...\n",
      "Test data loaded successfully.\n",
      "\n",
      "Evaluating model on the test set...\n",
      "62/62 [==============================] - 2s 13ms/step - loss: 0.2293 - accuracy: 0.9687\n",
      "\n",
      "--- Final COI Model Evaluation ---\n",
      "Test Set Loss:     0.2293\n",
      "Test Set Accuracy: 96.87%\n",
      "--------------------------------\n",
      "\n",
      "--- Generating Training History Plots ---\n",
      "\n",
      "Could not generate plots because the 'history' object was not found in memory.\n",
      "This is expected if the kernel was restarted after training.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: FINAL MODEL EVALUATION AND VISUALIZATION\n",
    "# =============================================================================\n",
    "#\n",
    "# OBJECTIVE:\n",
    "#   To perform a definitive, unbiased evaluation of the trained COI model on\n",
    "#   the unseen test set. This provides the final \"report card\" for this model.\n",
    "#\n",
    "# WORKFLOW:\n",
    "#   1.  Proactively clear system memory to ensure a stable evaluation environment.\n",
    "#   2.  Load the saved model and the corresponding test data from their files.\n",
    "#   3.  Run `model.evaluate()` on the test set to get the final accuracy score.\n",
    "#   4.  Attempt to plot the training history for visual analysis, gracefully\n",
    "#       handling the case where the kernel may have been restarted.\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports for this cell ---\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np\n",
    "\n",
    "# --- Define all necessary file paths ---\n",
    "project_root = Path.cwd().parent\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"coi_genus_classifier.keras\"\n",
    "X_TEST_PATH = PROCESSED_DATA_DIR / \"X_test_coi.npz\"\n",
    "Y_TEST_PATH = PROCESSED_DATA_DIR / \"y_test_coi.npy\"\n",
    "\n",
    "# --- 1. Clean up memory ---\n",
    "print(\"--- Starting Post-Training Workflow ---\")\n",
    "print(\"Clearing TensorFlow session and running garbage collection...\")\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "print(\"Memory cleared successfully.\")\n",
    "\n",
    "# --- 2. Load model and test data ---\n",
    "print(f\"\\nLoading model from: {MODEL_PATH}\")\n",
    "loaded_model = load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "print(f\"\\nLoading test data...\")\n",
    "X_test = load_npz(X_TEST_PATH)\n",
    "y_test = np.load(Y_TEST_PATH)\n",
    "print(\"Test data loaded successfully.\")\n",
    "\n",
    "# --- 3. Evaluate the model ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\n--- Final COI Model Evaluation ---\")\n",
    "print(f\"Test Set Loss:     {loss:.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy:.2%}\")\n",
    "print(\"--------------------------------\\n\")\n",
    "\n",
    "# --- 4. Visualize training history (if possible) ---\n",
    "try:\n",
    "    print(\"--- Generating Training History Plots ---\")\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    fig.suptitle('COI Model Training History', fontsize=18)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1.plot(history_df.index + 1, history_df['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history_df.index + 1, history_df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "    ax1.set_title('Model Accuracy Over Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--')\n",
    "\n",
    "    # Plot Loss\n",
    "    ax2.plot(history_df.index + 1, history_df['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history_df.index + 1, history_df['val_loss'], label='Validation Loss', marker='o')\n",
    "    ax2.set_title('Model Loss Over Epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\nCould not generate plots because the 'history' object was not found in memory.\")\n",
    "    print(\"This is expected if the kernel was restarted after training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ec220-8dc7-49fc-8cdf-449b0b3f2d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


