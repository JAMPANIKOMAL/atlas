{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7882a96e-398e-432d-94a6-e78a9864ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\jampa\\Music\\atlas\n",
      "\n",
      "Environment is set up. Ready to begin Explorer pipeline development.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#             ATLAS v3: \"Explorer\" Unsupervised Pipeline Development\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To develop the \"Explorer\" pipeline, the unsupervised learning component\n",
    "#       of ATLAS. This pipeline is responsible for processing sequences that\n",
    "#       were NOT classified by the \"Filter\" models, discovering novel taxonomic\n",
    "#       groups, and providing a \"best guess\" annotation for them.\n",
    "#\n",
    "#   METHODOLOGY:\n",
    "#\n",
    "#       1.  Simulate Input: Create a sample FASTA file of \"unclassified\"\n",
    "#           sequences for development purposes.\n",
    "#       2.  Sequence Vectorization: Implement the Doc2Vec algorithm to convert\n",
    "#           raw DNA sequences into meaningful numerical vectors (embeddings).\n",
    "#           This involves creating a \"corpus\" of k-mers and training a model.\n",
    "#       3.  Clustering: Apply the HDBSCAN algorithm to the sequence vectors\n",
    "#           to group them into clusters of related organisms. HDBSCAN is\n",
    "#           chosen for its ability to handle noise and find clusters of\n",
    "#           varying shapes.\n",
    "#       4.  Interpretation: For each discovered cluster, select a representative\n",
    "#           sequence and (conceptually) outline how a BLAST search would be\n",
    "#           used to provide a taxonomic hypothesis.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# Gensim for Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# HDBSCAN for clustering\n",
    "import hdbscan\n",
    "\n",
    "# Scikit-learn for helper functions\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# --- Setup Project Path ---\n",
    "try:\n",
    "    project_root = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "# --- Define Directories ---\n",
    "# We will use the existing directory structure\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "MODELS_DIR = project_root / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nEnvironment is set up. Ready to begin Explorer pipeline development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c40883-fc4c-4cf0-97ce-d5404399e232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating input data for the Explorer pipeline...\n",
      "  - Source: SILVA_138.1_SSURef_NR99_tax_silva.fasta\n",
      "  - Destination: unclassified_sample_for_explorer.fasta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96661ff9bb14807be5f80a0582c97cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  - Sampling records:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Created simulated input file with 5000 sequences.\n",
      "\n",
      "Loading simulated sequences into memory...\n",
      "  - Successfully loaded 5000 sequences.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#                  STEP 1 (REVISED): SIMULATE THE INPUT DATA\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "#   OBJECTIVE:\n",
    "#\n",
    "#       To create a sample FASTA file that represents the \"unclassified\"\n",
    "#       sequences that would be the output of the \"Filter\" pipelines.\n",
    "#\n",
    "#   RATIONALE (UPDATED):\n",
    "#\n",
    "#       Based on a more rigorous approach, we will source our \"unclassified\"\n",
    "#       sequences from a database that was NOT used to train our most recent\n",
    "#       (ITS) model. Using the full SILVA database provides a diverse set of\n",
    "#       16S and 18S sequences that are novel from the perspective of the ITS\n",
    "#       classifier. This avoids data leakage and creates a more realistic\n",
    "#       development environment for the Explorer pipeline.\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "\n",
    "# --- Configuration ---\n",
    "SIMULATED_INPUT_PATH = RAW_DATA_DIR / \"unclassified_sample_for_explorer.fasta\"\n",
    "# --- FIX: Use the full SILVA database as the source ---\n",
    "SOURCE_FILE_PATH = RAW_DATA_DIR / \"SILVA_138.1_SSURef_NR99_tax_silva.fasta\"\n",
    "NUM_SEQUENCES_TO_SIMULATE = 5000\n",
    "\n",
    "# --- Main Logic ---\n",
    "# This check prevents us from re-creating the file on every run\n",
    "if not SIMULATED_INPUT_PATH.exists():\n",
    "    print(f\"Simulating input data for the Explorer pipeline...\")\n",
    "    print(f\"  - Source: {SOURCE_FILE_PATH.name}\")\n",
    "    print(f\"  - Destination: {SIMULATED_INPUT_PATH.name}\")\n",
    "    \n",
    "    simulated_records = []\n",
    "    try:\n",
    "        with open(SOURCE_FILE_PATH, \"r\") as handle_in:\n",
    "            # Use tqdm to show progress as reading the large file can take a moment\n",
    "            records_iterator = SeqIO.parse(handle_in, \"fasta\")\n",
    "            for i, record in tqdm(enumerate(records_iterator), total=NUM_SEQUENCES_TO_SIMULATE, desc=\"  - Sampling records\"):\n",
    "                if i >= NUM_SEQUENCES_TO_SIMULATE:\n",
    "                    break\n",
    "                simulated_records.append(record)\n",
    "        \n",
    "        # Write the collected records to the new file\n",
    "        with open(SIMULATED_INPUT_PATH, \"w\") as handle_out:\n",
    "            SeqIO.write(simulated_records, handle_out, \"fasta\")\n",
    "            \n",
    "        print(f\"\\n[SUCCESS] Created simulated input file with {len(simulated_records)} sequences.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n[ERROR] Source file not found: {SOURCE_FILE_PATH}\")\n",
    "        print(\"        Please ensure the full SILVA FASTA file exists in `data/raw`.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] An error occurred: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Simulated input file already exists. No action needed.\")\n",
    "    print(f\"  - Location: {SIMULATED_INPUT_PATH}\")\n",
    "\n",
    "# --- Load the sequences into memory for the next steps ---\n",
    "print(\"\\nLoading simulated sequences into memory...\")\n",
    "try:\n",
    "    unclassified_sequences = list(SeqIO.parse(SIMULATED_INPUT_PATH, \"fasta\"))\n",
    "    print(f\"  - Successfully loaded {len(unclassified_sequences)} sequences.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] Could not load sequences. Please check for the file at {SIMULATED_INPUT_PATH}\")\n",
    "    unclassified_sequences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d717d-35cd-48d7-8425-853b8d8b0d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
